================================================================
RepopackPy Output File
================================================================

This file was generated by RepopackPy on: 2025-04-25T09:42:51.491500

Purpose:
--------
This file contains a packed representation of the entire repository's contents.
It is designed to be easily consumable by AI systems for analysis, code review,
or other automated processes.

File Format:
------------
The content is organized as follows:
1. This header section
2. Repository structure
3. Multiple file entries, each consisting of:
  a. A separator line (================)
  b. The file path (File: path/to/file)
  c. Another separator line
  d. The full contents of the file
  e. A blank line

Usage Guidelines:
-----------------
1. This file should be treated as read-only. Any changes should be made to the
  original repository files, not this packed version.
2. When processing this file, use the separators and "File:" markers to
  distinguish between different files in the repository.
3. Be aware that this file may contain sensitive information. Handle it with
  the same level of security as you would the original repository.

Notes:
------
- Some files may have been excluded based on .gitignore rules and RepopackPy's
  configuration.
- Binary files are not included in this packed representation. Please refer to
  the Repository Structure section for a complete list of file paths, including
  binary files.

For more information about RepopackPy, visit: https://github.com/abinthomasonline/repopack-py

================================================================
Repository Structure
================================================================
paelladoc/
  adapters/
    input/
      __init__.py
    output/
      chroma/
        chroma_vector_store_adapter.py
      filesystem/
        mcp_config_repository.py
        taxonomy_provider.py
        taxonomy_repository.py
      sqlite/
        config_models.py
        configuration_adapter.py
        db_models.py
        mapper.py
        models.py
        sqlite_memory_adapter.py
        sqlite_user_management_adapter.py
      __init__.py
    persistence/
      __init__.py
    plugins/
      code/
        __init__.py
        code_generation.py
        generate_context.py
        generate_doc.py
      core/
        __init__.py
        continue_proj.py
        help.py
        paella.py
        project_crud.py
        project_utils.py
        user_setup.py
        verification.py
      memory/
        __init__.py
        project_memory.py
      product/
        __init__.py
        product_management.py
      styles/
        __init__.py
        coding_styles.py
        git_workflows.py
      templates/
        __init__.py
        templates.py
      __init__.py
    services/
      system_time_service.py
    __init__.py
  application/
    services/
      memory_service.py
      vector_store_service.py
    utils/
      behavior_enforcer.py
    __init__.py
  config/
    database.py
    mcp_config.json
  domain/
    models/
      enums.py
      fix_metadata.py
      language.py
      project.py
      user.py
    services/
      time_service.py
    __init__.py
    core_logic.py
  infrastructure/
    __init__.py
  ports/
    input/
      __init__.py
      mcp_port.py
      mcp_server_adapter.py
    output/
      __init__.py
      configuration_port.py
      mcp_config_port.py
      memory_port.py
      taxonomy_provider.py
      taxonomy_repository.py
      user_management_port.py
      vector_store_port.py
    __init__.py
  taxonomies/
    compliance/
      accessibility.json
      ccpa.json
      fedramp.json
      gdpr.json
      hipaa.json
      iso27001.json
      none.json
      pci-dss.json
      soc2.json
      sox.json
    domain/
      ai-ml.json
      cms.json
      ecommerce.json
      education.json
      enterprise-management.json
      entertainment.json
      finance-banking.json
      healthcare.json
      iot-embedded.json
      productivity.json
      social-media.json
    lifecycle/
      growth.json
      legacy.json
      mature.json
      mvp.json
      poc.json
    platform/
      android-native.json
      backend-service.json
      browser-extension.json
      cli-tool.json
      desktop-app.json
      flutter.json
      ios-native.json
      macos.json
      pwa.json
      react-native.json
      vscode-extension.json
      web-frontend.json
    size/
      enterprise.json
      micro.json
      open-source-community.json
      personal.json
      startup.json
      team.json
  __init__.py
  dependencies.py
tests/
  e2e/
    test_cursor_simulation.py
  integration/
    adapters/
      output/
        __init__.py
        test_chroma_vector_store_adapter.py
        test_sqlite_memory_adapter.py
        test_sqlite_memory_adapter_active.py
        test_sqlite_memory_adapter_config.py
      plugins/
        core/
          test_list_projects.py
          test_paella.py
          test_project_crud.py
      __init__.py
    __init__.py
    test_alembic_config.py
    test_server.py
  unit/
    adapters/
      output/
        filesystem/
          test_mcp_config_repository.py
    application/
      services/
        test_memory_service.py
        test_vector_store_service.py
      utils/
        test_behavior_enforcer.py
    config/
      test_database.py
    domain/
      models/
        test_project.py
    test_ping_tool.py
  README.md
  __init__.py
  conftest.py
update_test_references.py

================================================================
Repository Files
================================================================

================
File: update_test_references.py
================
#!/usr/bin/env python3
import os
import re


def update_references_in_file(file_path):
    with open(file_path, "r") as f:
        content = f.read()

    # 1. Actualizar referencias a ProjectMetadata por ProjectInfo
    content = re.sub(r"Metadata as ProjectMetadata", "ProjectInfo", content)
    content = re.sub(r"ProjectMetadata", "ProjectInfo", content)

    # 2. Actualizar referencias a metadata por project_info
    content = re.sub(r"\.metadata\.", ".project_info.", content)
    content = re.sub(r"memory\.metadata", "memory.project_info", content)
    content = re.sub(
        r"original_memory\.metadata", "original_memory.project_info", content
    )
    content = re.sub(r"project\.metadata", "project.project_info", content)

    with open(file_path, "w") as f:
        f.write(content)

    print(f"Actualizado: {file_path}")


def find_and_update_test_files(directory):
    for root, _, files in os.walk(directory):
        for file in files:
            if file.endswith(".py"):
                file_path = os.path.join(root, file)
                update_references_in_file(file_path)


if __name__ == "__main__":
    find_and_update_test_files("tests")
    # Tambi√©n actualizar los adaptadores
    find_and_update_test_files("paelladoc/adapters")

================
File: paelladoc/dependencies.py
================
"""Dependency Injection Container for Paelladoc."""

import logging
from functools import lru_cache
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker

from paelladoc.config.database import get_db_path
from paelladoc.ports.output.configuration_port import ConfigurationPort
from paelladoc.adapters.output.sqlite.configuration_adapter import (
    SQLiteConfigurationAdapter,
)
from paelladoc.adapters.output.filesystem.taxonomy_repository import (
    FileSystemTaxonomyRepository,
)
from paelladoc.ports.output.taxonomy_repository import TaxonomyRepository
from paelladoc.adapters.output.filesystem.mcp_config_repository import (
    FileSystemMCPConfigRepository,
)
from paelladoc.ports.output.mcp_config_port import MCPConfigPort
from paelladoc.adapters.output.sqlite.sqlite_memory_adapter import SQLiteMemoryAdapter
from paelladoc.ports.output.memory_port import MemoryPort
from paelladoc.ports.output.user_management_port import UserManagementPort
from paelladoc.adapters.output.sqlite.sqlite_user_management_adapter import (
    SQLiteUserManagementAdapter,
)

logger = logging.getLogger(__name__)


class Container:
    """Simple dependency injection container."""

    def __init__(self):
        logger.info("Initializing Dependency Container...")
        self._db_path = get_db_path()
        logger.info(f"Container using DB path: {self._db_path.resolve()}")
        self._async_engine = create_async_engine(
            f"sqlite+aiosqlite:///{self._db_path}",
            echo=False,  # Set to True for SQL query logging
        )
        self._async_session_factory = sessionmaker(
            self._async_engine, class_=AsyncSession, expire_on_commit=False
        )
        logger.info("Async engine and session factory created.")

    @lru_cache()  # Cache the adapter instance
    def get_db_session(self) -> AsyncSession:
        """Provides a new database session."""
        logger.debug("Providing new DB session from factory.")
        return self._async_session_factory()

    @lru_cache()  # Cache the adapter instance
    def get_configuration_port(self) -> ConfigurationPort:
        """Provides an instance of the ConfigurationPort implementation."""
        # We pass the session factory, adapter can create sessions as needed
        # OR pass a session directly if request-scoped sessions are desired
        # For simplicity now, let adapter manage its session needs implicitly
        # via the factory or create a new one per call.
        # Passing the factory allows more flexibility.
        # Revisit if session management becomes complex.

        # CORRECTION: Adapter expects an AsyncSession instance, not factory
        # Let's provide a session. Since this is cached, it will be reused.
        # Consider request-scoped sessions for web apps later.
        logger.info("Creating and caching SQLiteConfigurationAdapter instance.")
        session = self._async_session_factory()
        return SQLiteConfigurationAdapter(session=session)


# Global container instance
container = Container()


def get_container() -> Container:
    """Returns the global container instance."""
    return container


# Create instances of adapters
taxonomy_repo = FileSystemTaxonomyRepository()
mcp_config_repo = FileSystemMCPConfigRepository()
memory_adapter = SQLiteMemoryAdapter()  # Uses configured DB path
# Instantiate SQLiteUserManagementAdapter, passing the session factory from the container
user_management_adapter = SQLiteUserManagementAdapter(
    async_session_factory=container._async_session_factory
)

# Dependency mapping
# For Dependency Injector or manual injection points
dependencies = {
    TaxonomyRepository: taxonomy_repo,
    MCPConfigPort: mcp_config_repo,
    MemoryPort: memory_adapter,
    UserManagementPort: user_management_adapter,
    # Add the session factory itself if needed elsewhere
    sessionmaker: container._async_session_factory,
    # ... other mappings ...
}

================
File: paelladoc/config/mcp_config.json
================
{
  "version": "1.0",
  "mcp_configuration": {
    "core": {
      "enabled": true,
      "linked_taxonomies": [
        "Initiate::CoreSetup",
        "Initiate::InitialProductDocs",
        "Elaborate::DiscoveryAndResearch",
        "Elaborate::IdeationAndDesign"
      ],
      "tools": [
        "core_help",
        "core_continue",
        "paella_init",
        "paella_list",
        "paella_select"
      ]
    },
    "product": {
      "enabled": true,
      "linked_taxonomies": [
        "Elaborate::SpecificationAndPlanning",
        "Elaborate::CoreAndSupport"
      ],
      "tools": [
        "core_manage_story",
        "core_manage_task",
        "core_manage_sprint"
      ]
    },
    "templates": {
      "enabled": true,
      "linked_taxonomies": [
        "Govern::MemoryTemplates",
        "Generate::SupportingElements"
      ],
      "tools": [
        "core_manage_template",
        "core_manage_template_variable",
        "core_manage_template_instance"
      ]
    },
    "standards": {
      "enabled": true,
      "linked_taxonomies": [
        "Govern::StandardsMethodologies",
        "Deploy::Security"
      ],
      "tools": [
        "core_manage_coding_style",
        "core_manage_style_rule",
        "core_manage_style_validation"
      ]
    },
    "operations": {
      "enabled": true,
      "linked_taxonomies": [
        "Deploy::PipelinesAndAutomation",
        "Deploy::InfrastructureAndConfig",
        "Operate::RunbooksAndSOPs",
        "Operate::MonitoringAndAlerting",
        "Operate::Maintenance"
      ],
      "tools": [
        "core_manage_deployment",
        "core_manage_monitoring",
        "core_manage_maintenance"
      ]
    },
    "iteration": {
      "enabled": true,
      "linked_taxonomies": [
        "Iterate::LearningAndAnalysis",
        "Iterate::PlanningAndRetrospection"
      ],
      "tools": [
        "core_manage_feedback",
        "core_manage_experiment",
        "core_manage_metrics"
      ]
    }
  }
}

================
File: paelladoc/config/database.py
================
"""Database configuration module."""

import os
from pathlib import Path
import json
import logging

# Load environment variables from .env file
from dotenv import load_dotenv

load_dotenv()

logger = logging.getLogger(__name__)

CONFIG_FILE_NAME = "paelladoc_config.json"


def get_project_root() -> Path:
    """Get the project root directory."""
    return Path(__file__).parent.parent.parent.parent


def get_config_file() -> Path:
    """Get the path to the configuration file."""
    # Check multiple locations in order of precedence
    possible_locations = [
        Path.cwd() / CONFIG_FILE_NAME,  # Current directory (development)
        Path.home() / ".paelladoc" / CONFIG_FILE_NAME,  # User's home directory
        Path("/etc/paelladoc") / CONFIG_FILE_NAME,  # System-wide configuration
    ]

    for location in possible_locations:
        if location.exists():
            return location

    # If no config file exists, use the default in user's home
    default_location = Path.home() / ".paelladoc" / CONFIG_FILE_NAME
    default_location.parent.mkdir(parents=True, exist_ok=True)
    if not default_location.exists():
        default_config = {
            "db_path": str(Path.home() / ".paelladoc" / "memory.db"),
            "environment": "production",
        }
        with open(default_location, "w") as f:
            json.dump(default_config, f, indent=2)

    return default_location


def get_db_path() -> Path:
    """
    Get the database path based on multiple configuration sources.

    Priority:
    1. PAELLADOC_DB_PATH environment variable if set
    2. Path specified in configuration file
    3. Default path in user's home directory (~/.paelladoc/memory.db)

    The configuration can be set during package installation with:
    pip install paelladoc --install-option="--db-path=/path/to/db"

    Or by editing the config file at:
    - ./paelladoc_config.json (development)
    - ~/.paelladoc/paelladoc_config.json (user)
    - /etc/paelladoc/paelladoc_config.json (system)
    """
    # 1. Check environment variable first (highest priority)
    env_path = os.getenv("PAELLADOC_DB_PATH")
    if env_path:
        db_path = Path(env_path)
        logger.info(f"Using database path from environment variable: {db_path}")
        return db_path

    # 2. Check configuration file
    config_file = get_config_file()
    try:
        with open(config_file) as f:
            config = json.load(f)
            if "db_path" in config:
                db_path = Path(config["db_path"])
                logger.info(
                    f"Using database path from config file {config_file}: {db_path}"
                )
                return db_path
    except Exception as e:
        logger.warning(f"Error reading config file {config_file}: {e}")

    # 3. Default to production path in user's home
    db_path = Path.home() / ".paelladoc" / "memory.db"
    db_path.parent.mkdir(parents=True, exist_ok=True)
    logger.info(f"Using default database path: {db_path}")
    return db_path


def set_db_path(path: str | Path) -> None:
    """
    Set the database path in the configuration file.

    This can be used programmatically or during package installation.
    """
    config_file = get_config_file()
    try:
        if config_file.exists():
            with open(config_file) as f:
                config = json.load(f)
        else:
            config = {}

        config["db_path"] = str(Path(path).resolve())

        with open(config_file, "w") as f:
            json.dump(config, f, indent=2)

        logger.info(f"Updated database path in {config_file} to: {path}")
    except Exception as e:
        logger.error(f"Error updating database path in config file: {e}")
        raise


# Default paths for reference (These might become less relevant or just informative)
# DEVELOPMENT_DB_PATH = get_project_root() / "paelladoc_memory.db"
PRODUCTION_DB_PATH = Path.home() / ".paelladoc" / "memory.db"
DEFAULT_DB_PATH = get_db_path()

================
File: paelladoc/adapters/plugins/__init__.py
================
import pkgutil
import importlib
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

# Dynamically import all submodules (like core, code, styles, etc.)
# This ensures their __init__.py files are executed, which should in turn
# import the actual plugin files containing @mcp.tool decorators.

package_path = str(Path(__file__).parent)
package_name = __name__

logger.info(f"Dynamically loading plugins from: {package_path}")

for module_info in pkgutil.iter_modules([package_path]):
    if module_info.ispkg:  # Only import potential packages (directories)
        sub_package_name = f"{package_name}.{module_info.name}"
        try:
            importlib.import_module(sub_package_name)
            logger.debug(f"Successfully imported plugin package: {sub_package_name}")
        except Exception as e:
            logger.warning(f"Could not import plugin package {sub_package_name}: {e}")

logger.info("Finished dynamic plugin package loading.")

================
File: paelladoc/adapters/plugins/core/verification.py
================
from paelladoc.domain.core_logic import mcp, logger
from typing import Dict, Any, Set
from paelladoc.domain.models.project import ProjectMemory

# Domain models
from paelladoc.domain.models.project import (
    DocumentStatus,
    Bucket,
)

# Adapter for persistence
from paelladoc.adapters.output.sqlite.sqlite_memory_adapter import SQLiteMemoryAdapter

# New repository for taxonomy loading
from paelladoc.adapters.output.filesystem.taxonomy_repository import (
    FileSystemTaxonomyRepository,
)

# Dependency Injection
from paelladoc.dependencies import get_container
from paelladoc.ports.output.configuration_port import ConfigurationPort

# Instantiate the taxonomy repository - TODO: Inject this dependency
TAXONOMY_REPOSITORY = FileSystemTaxonomyRepository()

# Get configuration port from container
container = get_container()
config_port: ConfigurationPort = container.get_configuration_port()


async def validate_mece_structure(memory: ProjectMemory) -> dict:
    """Validates the MECE taxonomy structure of a project against available taxonomies."""
    validation = {
        "is_valid": True,
        "missing_dimensions": [],
        "invalid_combinations": [],
        "invalid_dimensions": [],
        "warnings": [],
    }

    try:
        # Get MECE config from database via port
        mece_config = await config_port.get_mece_dimensions()
        allowed_dimensions = mece_config.get("allowed_dimensions", [])
        required_dimensions = mece_config.get("required_dimensions", [])
        validation_rules = mece_config.get("validation_rules", {})
        dimension_details = mece_config.get("dimensions", {})

        if not allowed_dimensions or not required_dimensions:
            validation["warnings"].append("MECE dimensions not configured in database.")
            validation["is_valid"] = False
            # Return early if essential config is missing
            return validation

    except Exception as e:
        logger.error(f"Failed to load MECE configuration from DB: {e}", exc_info=True)
        validation["is_valid"] = False
        validation["warnings"].append("Failed to load MECE configuration.")
        return validation

    # Check that all required dimensions exist in system taxonomy files
    try:
        available_dimensions_files = TAXONOMY_REPOSITORY.get_available_dimensions()
    except Exception as e:
        validation["warnings"].append(
            f"Failed to load taxonomy dimensions from filesystem: {str(e)}"
        )
        # Continue validation based on DB config, but warn about filesystem issues

    for dim in required_dimensions:
        # Check if dimension exists in the config derived from DB
        if dim not in dimension_details:
            validation["warnings"].append(
                f"Required dimension '{dim}' exists in DB config but lacks details."
            )
        # Check if dimension exists in file system (if loaded)
        # We might want to decide if a mismatch between DB config and filesystem is a failure
        if (
            "available_dimensions_files" in locals()
            and dim not in available_dimensions_files
        ):
            validation["warnings"].append(
                f"Required dimension '{dim}' (from DB config) not found in filesystem taxonomies (taxonomies/{dim}/)."
            )

    # 1. Verify required dimensions are present with valid values
    for dimension in required_dimensions:
        attr_name = f"{dimension}_taxonomy"
        memory_value = getattr(memory, attr_name, None)
        info_value = getattr(memory.project_info, attr_name, None)

        if not memory_value or not info_value:
            validation["missing_dimensions"].append(dimension)
            continue

        if memory_value != info_value:
            validation["invalid_combinations"].append(
                f"Mismatched {dimension} taxonomy between ProjectInfo ({info_value}) and ProjectMemory ({memory_value})"
            )
            continue

        try:
            valid_values = TAXONOMY_REPOSITORY.get_dimension_values(dimension)
            if memory_value not in valid_values:
                validation["invalid_combinations"].append(
                    f"Invalid {dimension} taxonomy: {memory_value}. Must be one of: {', '.join(valid_values)}"
                )
        except Exception as e:
            validation["warnings"].append(
                f"Failed to validate {dimension} taxonomy values from filesystem: {str(e)}"
            )
            validation["invalid_combinations"].append(
                f"Could not validate {dimension} taxonomy value due to filesystem repository error"
            )

    # 2. Check compliance (optional dimension, assuming 'compliance' is in allowed_dimensions)
    if (
        "compliance" in allowed_dimensions
        and hasattr(memory, "compliance_taxonomy")
        and memory.compliance_taxonomy
    ):
        try:
            valid_values = TAXONOMY_REPOSITORY.get_dimension_values("compliance")
            if memory.compliance_taxonomy not in valid_values:
                validation["invalid_combinations"].append(
                    f"Invalid compliance taxonomy: {memory.compliance_taxonomy}. Must be one of: {', '.join(valid_values)}"
                )
        except Exception as e:
            validation["warnings"].append(
                f"Failed to validate compliance taxonomy values from filesystem: {str(e)}"
            )

    # 3. Reject any non-standard dimensions (based on allowed list from DB)
    for attr in dir(memory):
        if attr.endswith("_taxonomy") and not attr.startswith("_"):
            dimension = attr[:-9]
            if (
                dimension not in allowed_dimensions
                and dimension != "custom"  # Assuming custom is always allowed
            ):
                validation["invalid_dimensions"].append(dimension)
                validation["warnings"].append(
                    f"Unauthorized dimension: '{dimension}'. Only {', '.join(allowed_dimensions)} are configured."
                )

    # 4. Validate specific combinations (using rules from DB)
    if hasattr(memory, "platform_taxonomy") and hasattr(memory, "domain_taxonomy"):
        platform = memory.platform_taxonomy
        domain = memory.domain_taxonomy

        # Iterate through validation rules from DB config
        for rule_platform, rule_details in validation_rules.items():
            if platform == rule_platform and domain == rule_details.get("domain"):
                validation["warnings"].append(
                    rule_details.get("warning", "Undefined validation warning")
                )
                # Potentially use severity from rule_details.get("severity") later

    # Update overall validity
    validation["is_valid"] = (
        not validation["missing_dimensions"]
        and not validation["invalid_combinations"]
        and not validation["invalid_dimensions"]
    )

    return validation


def get_relevant_buckets_for_project(memory: ProjectMemory) -> Set[str]:
    """Get the relevant buckets for a project based on its MECE dimensions."""
    # This still relies on the filesystem taxonomy repository
    # TODO: Consider if bucket relevance logic should also use ConfigurationPort
    return TAXONOMY_REPOSITORY.get_buckets_for_project(
        platform=memory.platform_taxonomy,
        domain=memory.domain_taxonomy,
        size=memory.size_taxonomy,
        lifecycle=memory.lifecycle_taxonomy,
        compliance=memory.compliance_taxonomy,
    )


@mcp.tool(
    name="core_verification",
    description="Verifies documentation coverage against the MECE taxonomy",
)
async def core_verification(project_name: str) -> dict:
    """Checks documentation against templates and project memory.

    Calculates an overall quality/completion score based on MECE taxonomy coverage.
    Returns an error if documentation is incomplete based on defined criteria.

    Args:
        project_name: The name of the project to verify

    Returns:
        A dictionary with verification results and coverage metrics
    """
    logger.info(f"Executing core.verification for project: {project_name}")

    # --- Get Behavior Config ---
    try:
        behavior_config = await config_port.get_behavior_config(category="verification")
        min_threshold = behavior_config.get(
            "minimum_coverage_threshold", 0.7
        )  # Default if not found
        block_code_gen = behavior_config.get(
            "block_code_generation_if_incomplete", True
        )
        logger.info(
            f"Using verification config: threshold={min_threshold}, block_gen={block_code_gen}"
        )
    except Exception as e:
        logger.error(f"Failed to load verification behavior config: {e}", exc_info=True)
        # Use defaults if config fails to load
        min_threshold = 0.7
        block_code_gen = True
        logger.warning("Using default verification behavior config due to load error.")

    # --- Initialize the memory adapter - TODO: Inject this ---
    try:
        memory_adapter = SQLiteMemoryAdapter()
        logger.info(
            f"core.verification using DB path: {memory_adapter.db_path.resolve()}"
        )
    except Exception as e:
        logger.error(f"Failed to instantiate SQLiteMemoryAdapter: {e}", exc_info=True)
        return {
            "status": "error",
            "message": "Internal server error: Could not initialize memory adapter.",
        }

    # --- Load Project Memory ---
    try:
        memory = await memory_adapter.load_memory(project_name)
        if not memory:
            logger.warning(
                f"Project '{project_name}' not found for VERIFICATION command."
            )
            return {
                "status": "error",
                "message": f"Project '{project_name}' not found. Use PAELLA command to start it.",
            }
        logger.info(f"Successfully loaded memory for project: {project_name}")

    except Exception as e:
        logger.error(f"Error loading memory for '{project_name}': {e}", exc_info=True)
        return {
            "status": "error",
            "message": f"Failed to load project memory: {e}",
        }

    # Add MECE validation (now uses config port)
    mece_validation = await validate_mece_structure(memory)

    # Calculate coverage only if MECE structure is valid
    if not mece_validation["is_valid"]:
        return {
            "status": "error",
            "message": "Invalid MECE taxonomy structure",
            "validation": mece_validation,
        }

    # --- Check for custom taxonomy ---
    custom_taxonomy = None
    relevant_buckets = set()
    # min_threshold already loaded from config

    if hasattr(memory, "custom_taxonomy") and memory.custom_taxonomy:
        logger.info(f"Using custom taxonomy for project '{project_name}'")
        custom_taxonomy = memory.custom_taxonomy

        # Load relevant buckets from custom taxonomy
        relevant_buckets = set(custom_taxonomy.get("buckets", []))
        logger.info(f"Custom taxonomy has {len(relevant_buckets)} relevant buckets")

        # Use custom threshold if specified
        if "minimum_coverage_threshold" in custom_taxonomy:
            min_threshold = custom_taxonomy["minimum_coverage_threshold"]
            logger.info(f"Using custom threshold: {min_threshold}")
    else:
        logger.info("No custom taxonomy found, using buckets based on MECE dimensions")
        # Get buckets based on MECE dimensions of the project
        relevant_buckets = await get_relevant_buckets_for_project(memory)
        logger.info(f"MECE dimensions suggest {len(relevant_buckets)} relevant buckets")

    # If we don't have any relevant buckets, fall back to all buckets except system ones
    if not relevant_buckets:
        logger.info("Falling back to all regular buckets (no MECE buckets found)")
        relevant_buckets = {
            bucket.value for bucket in Bucket if bucket != Bucket.UNKNOWN
        }

    # --- Calculate MECE Coverage ---
    # Get completion stats for each bucket
    bucket_stats: Dict[str, Dict[str, Any]] = {}
    total_artifacts = 0
    total_completed = 0
    total_in_progress = 0
    total_pending = 0

    # Skip these buckets as they're more system-oriented, not documentation
    system_buckets = {
        Bucket.UNKNOWN,
        Bucket.MAINTAIN_CORE_FUNCTIONALITY,
        Bucket.GOVERN_TOOLING_SCRIPTS,
    }
    system_bucket_values = {b.value for b in system_buckets}

    # Custom bucket weights (either from custom taxonomy or defaults)
    bucket_weights = {}

    # If we have custom taxonomy with bucket details and weights
    if custom_taxonomy and "bucket_details" in custom_taxonomy:
        for bucket_name, details in custom_taxonomy["bucket_details"].items():
            if "weight" in details:
                bucket_weights[bucket_name] = details["weight"]

    # Default weights for important buckets if not specified in custom taxonomy
    if not bucket_weights:
        bucket_weights = {
            Bucket.INITIATE_INITIAL_PRODUCT_DOCS.value: 1.5,  # High importance
            Bucket.ELABORATE_SPECIFICATION_AND_PLANNING.value: 1.3,  # High importance
            Bucket.GOVERN_STANDARDS_METHODOLOGIES.value: 1.2,  # Medium-high importance
            Bucket.GENERATE_CORE_FUNCTIONALITY.value: 1.1,  # Medium-high importance
        }

    # Calculate stats for each bucket
    for bucket in Bucket:
        bucket_value = bucket.value

        # Skip system buckets and buckets not in the relevant set
        if (
            bucket in system_buckets
            or bucket_value in system_bucket_values
            or (relevant_buckets and bucket_value not in relevant_buckets)
        ):
            continue

        artifacts = memory.artifacts.get(bucket, [])
        if not artifacts:
            # If no artifacts but bucket is relevant, track as empty bucket
            if bucket_value in relevant_buckets:
                bucket_stats[bucket_value] = {
                    "total": 0,
                    "completed": 0,
                    "in_progress": 0,
                    "pending": 0,
                    "completion_percentage": 0.0,
                }
            continue

        bucket_total = len(artifacts)
        bucket_completed = sum(
            1 for a in artifacts if a.status == DocumentStatus.COMPLETED
        )
        bucket_in_progress = sum(
            1 for a in artifacts if a.status == DocumentStatus.IN_PROGRESS
        )
        bucket_pending = bucket_total - bucket_completed - bucket_in_progress

        # Calculate completion percentage
        completion_pct = (
            (bucket_completed + (bucket_in_progress * 0.5)) / bucket_total
            if bucket_total > 0
            else 0
        )

        # Store statistics
        bucket_stats[bucket_value] = {
            "total": bucket_total,
            "completed": bucket_completed,
            "in_progress": bucket_in_progress,
            "pending": bucket_pending,
            "completion_percentage": completion_pct,
        }

        # Update global counters
        total_artifacts += bucket_total
        total_completed += bucket_completed
        total_in_progress += bucket_in_progress
        total_pending += bucket_pending

    # Add custom buckets from taxonomy that aren't standard Bucket enums
    if custom_taxonomy and "buckets" in custom_taxonomy:
        for bucket_name in custom_taxonomy["buckets"]:
            # Skip if already processed above
            if bucket_name in bucket_stats:
                continue

            # This is a custom bucket not in the standard Bucket enum
            # For now, treat it as empty/pending
            bucket_stats[bucket_name] = {
                "total": 0,
                "completed": 0,
                "in_progress": 0,
                "pending": 0,
                "completion_percentage": 0.0,
                "custom": True,
            }

    # Add relevant MECE buckets not in the standard enum and not processed yet
    for bucket_name in relevant_buckets:
        if bucket_name not in bucket_stats:
            bucket_description = TAXONOMY_REPOSITORY.get_bucket_description(bucket_name)
            bucket_stats[bucket_name] = {
                "total": 0,
                "completed": 0,
                "in_progress": 0,
                "pending": 0,
                "completion_percentage": 0.0,
                "mece": True,
                "description": bucket_description,
            }

    # Calculate overall weighted completion percentage
    if total_artifacts > 0:
        # Simple (unweighted) calculation
        simple_completion_pct = (
            total_completed + (total_in_progress * 0.5)
        ) / total_artifacts

        # Weighted calculation
        weighted_sum = 0
        weight_sum = 0

        for bucket_name, stats in bucket_stats.items():
            if stats.get("total", 0) == 0:
                continue

            # Get weight for this bucket (default to 1.0)
            bucket_weight = bucket_weights.get(bucket_name, 1.0)
            weight_sum += bucket_weight
            weighted_sum += stats["completion_percentage"] * bucket_weight

        weighted_completion_pct = weighted_sum / weight_sum if weight_sum > 0 else 0
    else:
        simple_completion_pct = 0
        weighted_completion_pct = 0

    # Determine overall status
    is_complete = weighted_completion_pct >= min_threshold

    # Identify buckets that need attention (< 50% complete)
    needs_attention = []
    for bucket, stats in bucket_stats.items():
        if stats["completion_percentage"] < 0.5:
            needs_attention.append(
                {
                    "bucket": bucket,
                    "completion": stats["completion_percentage"],
                    "missing_docs": stats["pending"],
                }
            )

    # Sort by completion percentage (lowest first)
    needs_attention.sort(key=lambda x: x["completion"])

    # Create verification result
    result = {
        "status": "ok",
        "project_name": project_name,
        "overall_status": "complete" if is_complete else "incomplete",
        "completion_percentage": weighted_completion_pct,
        "simple_completion_percentage": simple_completion_pct,
        "meets_threshold": is_complete,
        "threshold": min_threshold,
        "total_artifacts": total_artifacts,
        "total_completed": total_completed,
        "total_in_progress": total_in_progress,
        "total_pending": total_pending,
        "bucket_stats": bucket_stats,
        "needs_attention": needs_attention,
        "taxonomy_version": memory.taxonomy_version,
        "custom_taxonomy": bool(custom_taxonomy),
        "message": (
            f"Documentation is {weighted_completion_pct:.1%} complete "
            f"({'meets' if is_complete else 'does not meet'} {min_threshold:.1%} threshold)."
        ),
        "allow_code_generation": is_complete or not block_code_gen,
        "mece_validation": mece_validation,
        "taxonomy_structure": {
            "platform": memory.platform_taxonomy,
            "domain": memory.domain_taxonomy,
            "size": memory.size_taxonomy,
            "compliance": memory.compliance_taxonomy,
            "lifecycle": memory.lifecycle_taxonomy,
        },
        "relevant_buckets": sorted(list(relevant_buckets)),
    }

    return result

================
File: paelladoc/adapters/plugins/core/user_setup.py
================
"""MCP Tool for registering the single OSS user."""

import logging
from typing import Dict, Any

from sqlmodel import select
from sqlalchemy.exc import IntegrityError
from sqlalchemy.orm import sessionmaker  # Import sessionmaker

# Dependency Injection & Core Logic
from paelladoc.dependencies import dependencies  # Assuming dict-based DI
from paelladoc.domain.core_logic import mcp
from paelladoc.adapters.output.sqlite.db_models import UserDB  # Import UserDB model

logger = logging.getLogger(__name__)


@mcp.tool(
    name="core_register_oss_user",
    description="Registers the single administrative user for this OSS instance.",
)
async def register_oss_user(
    user_identifier: str,
    # --- REMOVE Injected Dependencies --- #
    # session_factory: Optional[sessionmaker] = None
) -> Dict[str, Any]:
    """
    Registers the single user allowed in the OSS version of PAELLADOC.

    ACTION: Attempts to create the first and only user entry in the database.
            This user will be associated with all subsequent actions (created_by, modified_by).

    INPUT:
    - user_identifier: A unique string to identify the user (e.g., username, email). Required.

    OUTPUT: MUST return ONLY the raw JSON response from the execution.
    - On success: { "status": "ok", "message": "User 'identifier' registered successfully." }
    - On error: { "status": "error", "message": "Error description (e.g., User already registered)" }

    EXECUTION RULES:
    1. Check if any user already exists in the UserDB table.
    2. If a user exists, return an error message (only one user allowed).
    3. If no user exists, create a new UserDB entry with the provided identifier.
    4. Handle potential database errors (e.g., uniqueness constraint if somehow called twice).
    5. DO NOT add any introductory or explanatory text.

    Args:
        user_identifier: The identifier for the single OSS user.
        # REMOVE session_factory from Args

    Returns:
        Dict[str, Any]: Dictionary containing operation result or error message.
    """
    logger.info(f"Attempting to register OSS user: {user_identifier}")

    # --- Get dependencies INSIDE function (WITHOUT type hint) --- #
    session_factory = dependencies.get(sessionmaker)
    if not session_factory:
        logger.error("Session factory not found in dependencies.")
        return {
            "status": "error",
            "message": "Internal configuration error: Session factory missing.",
        }
    # --- End Dependency Resolution --- #

    async with session_factory() as session:
        async with session.begin():  # Use transaction
            try:
                # 1. Check if any user already exists
                statement_check = select(UserDB)
                results_check = await session.execute(statement_check)
                existing_user = results_check.scalars().first()

                if existing_user:
                    logger.warning(
                        f"Attempted to register user '{user_identifier}' but user '{existing_user.user_identifier}' already exists."
                    )
                    return {
                        "status": "error",
                        "message": f"OSS user already registered as '{existing_user.user_identifier}'. Only one user is allowed.",
                    }

                # 3. If no user exists, create a new one
                new_user = UserDB(user_identifier=user_identifier)
                session.add(new_user)
                await (
                    session.flush()
                )  # Flush to ensure commit happens within transaction

                logger.info(f"Successfully registered OSS user: {user_identifier}")
                return {
                    "status": "ok",
                    "message": f"User '{user_identifier}' registered successfully.",
                }

            except IntegrityError as e:
                logger.error(
                    f"Integrity error registering user '{user_identifier}': {e}",
                    exc_info=True,
                )
                # This might happen if the check somehow missed an existing user due to concurrency,
                # although less likely with a single OSS user setup.
                return {
                    "status": "error",
                    "message": f"Database integrity error: Could not register user '{user_identifier}'. It might already exist.",
                }
            except Exception as e:
                logger.error(
                    f"Error registering OSS user '{user_identifier}': {e}",
                    exc_info=True,
                )
                return {
                    "status": "error",
                    "message": f"An unexpected error occurred: {str(e)}",
                }

================
File: paelladoc/adapters/plugins/core/project_crud.py
================
"""
Core plugin for project CRUD operations.
"""

import logging
from pathlib import Path
from typing import Dict, Any, Optional
import shutil  # Import shutil for directory deletion

from paelladoc.domain.core_logic import mcp
from paelladoc.domain.models.project import ProjectInfo
from .project_utils import (
    validate_project_updates,
    create_project_backup,
    format_project_info,
)

# Dependency Injection for User Management Port
from paelladoc.dependencies import dependencies  # Assuming dict-based DI
from paelladoc.ports.output.user_management_port import UserManagementPort
from paelladoc.ports.output.memory_port import MemoryPort

logger = logging.getLogger(__name__)


@mcp.tool(
    name="core_get_project",
    description="Get detailed information about a specific project.",
)
async def get_project(
    project_name: str,
    # --- REMOVE Injected Dependencies --- #
    # memory_adapter: Optional[MemoryPort] = None
) -> Dict[str, Any]:
    """Get detailed information about a specific project.

    ACTION: Retrieves complete project information for the specified project name.

    INPUT:
    - project_name: Name of the project to retrieve (required)

    OUTPUT: MUST return ONLY the raw JSON response from the execution.
    - On success: { "status": "ok", "project": ProjectInfo }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. This tool performs ONLY the project retrieval action
    2. DO NOT add any introductory or explanatory text
    3. DO NOT interpret the project data or suggest actions
    4. DO NOT ask any questions

    Args:
        project_name: Name of the project to retrieve
        # REMOVE memory_adapter from Args

    Returns:
        Dict[str, Any]: Dictionary containing project info or error message
    """
    # --- Get dependencies INSIDE function (WITHOUT type hints) --- #
    memory_adapter = dependencies.get(MemoryPort)
    if not memory_adapter:
        logger.error("MemoryPort not found in dependencies.")
        return {
            "status": "error",
            "message": "Internal configuration error: MemoryPort missing.",
        }
    # --- End Dependency Resolution --- #

    try:
        project_memory = await memory_adapter.load_memory(project_name)

        if not project_memory:
            return {
                "status": "error",
                "message": f"Project '{project_name}' not found.",
            }

        # Format project info for response
        project_info = project_memory.project_info.model_dump()
        project_info = format_project_info(project_info)

        return {"status": "ok", "project": project_info}

    except Exception as e:
        logger.error(f"Error retrieving project '{project_name}': {e}", exc_info=True)
        return {"status": "error", "message": f"Error retrieving project: {str(e)}"}


@mcp.tool(
    name="core_update_project", description="Update specific fields of a project."
)
async def update_project(
    project_name: str,
    updates: Dict[str, Any],
    create_backup: bool = True,
    # --- REMOVE Injected Dependencies --- #
    # memory_adapter: Optional[MemoryPort] = None,
    # user_management_port: Optional[UserManagementPort] = None
) -> Dict[str, Any]:
    """Update specific fields of a project.

    ACTION: Updates specified fields of an existing project and optionally creates a backup.

    INPUT:
    - project_name: Name of the project to update (required)
    - updates: Dictionary of field names and their new values (required)
    - create_backup: Whether to create a backup before updating (optional, default: True)

    OUTPUT: MUST return ONLY the raw JSON response from the execution.
    - On success: { "status": "ok", "project": ProjectInfo, "backup_path": str (if backup created) }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Check user permissions before proceeding.
    2. Validate all field updates before applying changes
    3. Create backup if requested before making changes
    4. Apply all updates atomically - all succeed or none
    5. DO NOT add any explanatory text or suggestions

    Args:
        project_name: Name of the project to update
        updates: Dictionary of field names and their new values
        create_backup: Whether to create a backup before updating
        # REMOVE injected dependencies from Args

    Returns:
        Dict[str, Any]: Dictionary containing updated project info or error message
    """
    # --- Get dependencies INSIDE function (WITHOUT type hints) --- #
    memory_adapter = dependencies.get(MemoryPort)
    user_management_port = dependencies.get(UserManagementPort)
    if not memory_adapter:
        logger.error("MemoryPort not found in dependencies.")
        return {
            "status": "error",
            "message": "Internal configuration error: MemoryPort missing.",
        }
    if not user_management_port:
        logger.warning(
            "UserManagementPort not found in dependencies. Skipping permission check."
        )
    # --- End Dependency Resolution --- #

    try:
        # --- Permission Check ---
        if user_management_port:
            user_id = await user_management_port.get_current_user_id()
            if not await user_management_port.check_permission(
                user_id, "core_update_project", project_name
            ):
                return {
                    "status": "error",
                    "message": "Permission denied to update this project.",
                }
        # --- End Permission Check ---

        project_memory = await memory_adapter.load_memory(project_name)

        if not project_memory:
            return {
                "status": "error",
                "message": f"Project '{project_name}' not found.",
            }

        # Validate updates
        validation_errors = validate_project_updates(updates)
        if validation_errors:
            return {
                "status": "error",
                "message": "Validation failed: " + "; ".join(validation_errors),
            }

        # Create backup if requested
        backup_path = None
        if create_backup:
            backup_path, error = create_project_backup(project_memory.project_info)
            if error:
                return {"status": "error", "message": error}

        # Apply updates
        project_info_dict = project_memory.project_info.model_dump()
        project_info_dict.update(updates)
        project_memory.project_info = ProjectInfo(**project_info_dict)

        # --- ALSO update top-level taxonomy fields in ProjectMemory if they were in updates ---
        # This ensures consistency between ProjectInfo and ProjectMemory before saving
        taxonomy_keys = [
            "platform_taxonomy",
            "domain_taxonomy",
            "size_taxonomy",
            "compliance_taxonomy",
            "lifecycle_taxonomy",
            "custom_taxonomy",  # Assuming custom_taxonomy can also be updated
        ]
        for key in taxonomy_keys:
            if key in updates:
                setattr(project_memory, key, updates[key])
                logger.debug(f"Updated ProjectMemory.{key} directly from updates.")
        # --- End direct ProjectMemory update ---

        # Save changes (this will also set modified_by via the adapter)
        await memory_adapter.save_memory(project_memory)

        # Format response
        # Fetch the project again AFTER saving to ensure the response reflects the persisted state
        # project_info = project_memory.project_info.model_dump()
        refreshed_memory = await memory_adapter.load_memory(project_name)
        if not refreshed_memory:
            # Should not happen, but handle defensively
            logger.error(f"Could not reload project '{project_name}' after update.")
            return {
                "status": "error",
                "message": "Failed to confirm update persistence.",
            }

        project_info = refreshed_memory.project_info.model_dump()
        project_info = format_project_info(project_info)

        result = {"status": "ok", "project": project_info}
        if backup_path:
            result["backup_path"] = str(backup_path)

        return result

    except Exception as e:
        logger.error(f"Error updating project '{project_name}': {e}", exc_info=True)
        return {"status": "error", "message": f"Error updating project: {str(e)}"}


@mcp.tool(
    name="core_delete_project", description="Delete a project and optionally its files."
)
async def delete_project(
    project_name: str,
    confirm: bool = False,
    create_backup: bool = True,
    # --- REMOVE Injected Dependencies --- #
    # memory_adapter: Optional[MemoryPort] = None,
    # user_management_port: Optional[UserManagementPort] = None
) -> Dict[str, Any]:
    """Delete a project and optionally its files.

    ACTION: Deletes a project from the database and its associated files, with optional backup.

    INPUT:
    - project_name: Name of the project to delete (required)
    - confirm: Explicit confirmation required for deletion (optional, default: False)
    - create_backup: Whether to create a backup before deletion (optional, default: True)

    OUTPUT: MUST return ONLY the raw JSON response from the execution.
    - On success: { "status": "ok", "message": "Project deleted.", "backup_path": str (if backup created) }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Check user permissions before proceeding.
    2. Require explicit confirmation (`confirm=True`) before deletion.
    3. Create backup if requested (`create_backup=True`) before deletion.
    4. Remove both database entry and associated project directory.
    5. DO NOT add any explanatory text or suggestions.

    Args:
        project_name: Name of the project to delete
        confirm: Explicit confirmation required for deletion
        create_backup: Whether to create a backup before deletion
        # REMOVE injected dependencies from Args

    Returns:
        Dict[str, Any]: Dictionary containing operation result or error message
    """
    # --- Get dependencies INSIDE function (WITHOUT type hints) --- #
    memory_adapter = dependencies.get(MemoryPort)
    user_management_port = dependencies.get(UserManagementPort)
    if not memory_adapter:
        logger.error("MemoryPort not found in dependencies.")
        return {
            "status": "error",
            "message": "Internal configuration error: MemoryPort missing.",
        }
    if not user_management_port:
        logger.warning(
            "UserManagementPort not found in dependencies. Skipping permission check."
        )
    # --- End Dependency Resolution --- #

    # --- Permission Check ---
    if user_management_port:
        try:
            user_id = await user_management_port.get_current_user_id()
            # Use the correct permission name: 'core_delete_project'
            if not await user_management_port.check_permission(
                user_id, "core_delete_project", project_name
            ):
                return {
                    "status": "error",
                    "message": "Permission denied to delete this project.",
                }
        except Exception as perm_e:
            logger.error(
                f"Permission check failed for deleting project '{project_name}': {perm_e}",
                exc_info=True,
            )
            return {"status": "error", "message": "Permission check failed."}
    # --- End Permission Check ---

    if not confirm:
        logger.warning(
            f"Deletion attempt for project '{project_name}' without confirmation."
        )
        return {
            "status": "error",
            "message": "Deletion requires explicit confirmation. Set 'confirm=True'.",
        }

    try:
        logger.info(f"Attempting to delete project '{project_name}' with confirmation.")
        project_memory = await memory_adapter.load_memory(project_name)

        if not project_memory:
            logger.warning(f"Project '{project_name}' not found for deletion.")
            return {
                "status": "error",
                "message": f"Project '{project_name}' not found.",
            }

        # --- Backup ---
        backup_path_str: Optional[str] = None
        if create_backup:
            logger.info(
                f"Creating backup for project '{project_name}' before deletion."
            )
            backup_path, backup_error = create_project_backup(
                project_memory.project_info
            )
            if backup_error:
                logger.error(
                    f"Backup creation failed for project '{project_name}': {backup_error}"
                )
                # Decide if failure to backup should prevent deletion
                return {
                    "status": "error",
                    "message": f"Backup failed: {backup_error}. Deletion aborted.",
                }
            backup_path_str = str(backup_path) if backup_path else None
            logger.info(
                f"Backup created for project '{project_name}' at {backup_path_str}"
            )

        # --- File System Deletion ---
        base_path_str = project_memory.project_info.base_path
        if base_path_str:
            base_path = Path(base_path_str)
            if base_path.exists():
                logger.info(f"Deleting project directory: {base_path}")
                try:
                    if base_path.is_dir():
                        shutil.rmtree(base_path)
                        logger.info(f"Successfully deleted directory: {base_path}")
                    elif (
                        base_path.is_file()
                    ):  # Handle case where base_path is unexpectedly a file
                        base_path.unlink()
                        logger.warning(
                            f"Expected directory but found file at {base_path}. Deleted file."
                        )
                    else:
                        logger.warning(
                            f"Path {base_path} exists but is neither a file nor a directory."
                        )
                except OSError as os_err:  # More specific exception handling
                    logger.error(
                        f"OS error deleting project directory {base_path}: {os_err}",
                        exc_info=True,
                    )
                    # Return error as file deletion is critical
                    return {
                        "status": "error",
                        "message": f"Error deleting project files: {str(os_err)}",
                    }
                except Exception as path_e:
                    logger.error(
                        f"Unexpected error deleting project directory {base_path}: {path_e}",
                        exc_info=True,
                    )
                    return {
                        "status": "error",
                        "message": f"Unexpected error deleting project files: {str(path_e)}",
                    }
            else:
                logger.warning(
                    f"Project directory {base_path} not found, skipping file deletion."
                )
        else:
            logger.warning(
                f"Project '{project_name}' has no base_path defined, skipping file deletion."
            )

        # --- Database Deletion ---
        logger.info(f"Deleting project '{project_name}' from database.")
        await memory_adapter.delete_memory(project_name)
        logger.info(f"Successfully deleted project '{project_name}' from database.")

        # --- Success Response ---
        result = {
            "status": "ok",
            "message": f"Project '{project_name}' deleted successfully.",
        }
        if backup_path_str:
            result["backup_path"] = backup_path_str

        return result

    except Exception as e:
        logger.error(f"Error deleting project '{project_name}': {e}", exc_info=True)
        return {
            "status": "error",
            "message": f"An unexpected error occurred during project deletion: {str(e)}",
        }

================
File: paelladoc/adapters/plugins/core/project_utils.py
================
"""Utilities for project management."""

import logging
import shutil
from datetime import datetime
from pathlib import Path
from typing import Dict, Any, Optional, List, Tuple

from paelladoc.domain.models.language import SupportedLanguage
from paelladoc.domain.models.project import ProjectInfo

logger = logging.getLogger(__name__)


class ValidationError(Exception):
    """Raised when project validation fails."""

    pass


def validate_project_updates(updates: Dict[str, Any]) -> List[str]:
    """
    Validate project field updates.

    Args:
        updates: Dictionary of field names and their new values.

    Returns:
        List of validation error messages. Empty if validation passes.
    """
    errors = []

    # Prevent updating base_path directly
    if "base_path" in updates:
        errors.append("base_path cannot be updated directly through this function.")
        # We can potentially remove it from updates to prevent further processing
        # or just return errors immediately if this is critical.
        # For now, just record the error.

    # Validate documentation_language
    if "documentation_language" in updates:
        value = updates["documentation_language"]
        if value not in [lang.value for lang in SupportedLanguage]:
            errors.append(f"Invalid documentation_language: {value}")

    # Validate interaction_language
    if "interaction_language" in updates:
        value = updates["interaction_language"]
        if value not in [lang.value for lang in SupportedLanguage]:
            errors.append(f"Invalid interaction_language: {value}")

    # Validate name (if present)
    if "name" in updates:
        if not updates["name"] or not isinstance(updates["name"], str):
            errors.append("Project name cannot be empty and must be a string")

    return errors


def create_project_backup(
    project_info: ProjectInfo, backup_dir: Optional[Path] = None
) -> Tuple[Optional[Path], Optional[str]]:
    """
    Create a backup of project files.

    Args:
        project_info: Project information.
        backup_dir: Optional directory to store backups. Defaults to project's parent dir.

    Returns:
        Tuple of (backup_path, error_message). If backup fails, path will be None.
    """
    try:
        base_path = Path(project_info.base_path)
        if not base_path.exists():
            return None, "Project directory does not exist"

        # Use provided backup dir or project's parent
        backup_dir = backup_dir or base_path.parent
        backup_dir.mkdir(parents=True, exist_ok=True)

        # Create timestamped backup name
        timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
        backup_name = f"{project_info.name}_backup_{timestamp}"
        backup_path = backup_dir / f"{backup_name}.zip"

        # Create zip backup
        shutil.make_archive(str(backup_path.with_suffix("")), "zip", base_path)

        logger.info(f"Created backup at {backup_path}")
        return backup_path, None

    except Exception as e:
        error_msg = f"Error creating backup: {e}"
        logger.error(error_msg)
        return None, error_msg


def format_project_info(project_info: Dict[str, Any]) -> Dict[str, Any]:
    """
    Format project information for API response.

    Args:
        project_info: Raw project info dictionary.

    Returns:
        Formatted project info with consistent types.
    """
    # Ensure base_path is string
    if "base_path" in project_info:
        project_info["base_path"] = str(project_info["base_path"])

    # Convert any Path objects to strings
    for key, value in project_info.items():
        if isinstance(value, Path):
            project_info[key] = str(value)

    return project_info

================
File: paelladoc/adapters/plugins/core/__init__.py
================
import pkgutil
import importlib
import logging
from pathlib import Path

logger = logging.getLogger(__name__)

# Dynamically import all modules within this 'core' package
# to ensure @mcp.tool decorators are executed.

package_path = str(Path(__file__).parent)
package_name = __name__

logger.info(f"Dynamically loading core plugins from: {package_path}")

for module_info in pkgutil.iter_modules([package_path]):
    # Import all .py files (except __init__.py itself)
    if module_info.name != "__init__" and not module_info.ispkg:
        module_name = f"{package_name}.{module_info.name}"
        try:
            importlib.import_module(module_name)
            logger.debug(f"Successfully loaded core plugin module: {module_name}")
        except Exception as e:
            logger.warning(f"Could not load core plugin module {module_name}: {e}")

logger.info("Finished dynamic core plugin loading.")

"""
Core plugins for PAELLADOC command handling.

Imports:
    - help: Provides the HELP command functionality.
    - paella: Initiates new documentation projects.
    - continue_proj: Continues existing documentation projects.
    - verification: Verifies documentation integrity.
    - list_projects: Lists existing projects.
"""

# Removed explicit imports and __all__, relying on dynamic loading above
# from .help import core_help
# from .paella import core_paella # This was causing issues
# from .continue_proj import core_continue
# from .verification import core_verification
# from .list_projects import list_projects
#
# __all__ = [
#     "core_help",
#     "core_paella",
#     "core_continue",
#     "core_verification",
#     "list_projects",
# ]

================
File: paelladoc/adapters/plugins/core/paella.py
================
"""PAELLADOC project initialization module."""

from pathlib import Path
from typing import Dict, Optional

# Import the shared FastMCP instance from core_logic
from paelladoc.domain.core_logic import mcp, logger

# Domain models and services
from paelladoc.domain.models.project import (
    ProjectMemory,
    ProjectInfo,
    Bucket,
    DocumentStatus,
    ArtifactMeta,
    set_time_service,
)
from paelladoc.adapters.services.system_time_service import SystemTimeService

# Adapter for persistence
from paelladoc.ports.output.memory_port import MemoryPort
from paelladoc.dependencies import dependencies

# Initialize logger for this module
# logger is already imported from core_logic

# Create FastMCP instance - REMOVED, using imported instance
# mcp = FastMCP("PAELLADOC")


@mcp.tool(
    name="paella_init",
    description="Initiates the conversational workflow to define and document a new PAELLADOC project",
)
async def paella_init(
    base_path: str,
    documentation_language: str,
    interaction_language: str,
    new_project_name: str,
    platform_taxonomy: str,  # e.g., "pwa", "web-frontend", "vscode-extension"
    domain_taxonomy: str,
    size_taxonomy: str,
    compliance_taxonomy: str,
    lifecycle_taxonomy: str,
    custom_taxonomy: Optional[Dict] = None,  # Still optional
    # --- REMOVE Injected Dependencies --- #
    # memory_adapter: Optional[MemoryPort] = None,
) -> Dict:
    """
    Initiates the conversational workflow to define and document a new PAELLADOC project.

    This tool gathers essential project details, including the core taxonomies (platform,
    domain, size, compliance) which are mandatory for project setup and analysis.

    It creates the project structure and persists the initial memory state with all
    provided information.

    Once executed successfully, the project is initialized with its defined taxonomies and ready
    for the next conversational steps.

    Args:
        base_path: Base path where the project documentation will be stored.
        documentation_language: Primary language for the generated documentation (e.g., 'en', 'es').
        interaction_language: Language used during conversational interactions (e.g., 'en', 'es').
        new_project_name: Unique name for the new PAELLADOC project.
        platform_taxonomy: Identifier for the target platform (e.g., "pwa", "web-frontend").
        domain_taxonomy: Identifier for the project's domain (e.g., "ecommerce", "healthcare").
        size_taxonomy: Identifier for the estimated project size (e.g., "mvp", "enterprise").
        compliance_taxonomy: Identifier for any compliance requirements (e.g., "gdpr", "none").
        lifecycle_taxonomy: Identifier for the project's lifecycle (e.g., "startup", "growth").
        custom_taxonomy: (Optional) A dictionary for any user-defined taxonomy.
        # REMOVE memory_adapter from Args

    Returns:
        A dictionary confirming the project's creation ('status': 'ok') or detailing an error ('status': 'error').
        On success, includes the 'project_name' and resolved 'base_path'.
    """
    logger.info(
        f"Initializing new project: {new_project_name} with taxonomies: Platform={platform_taxonomy}, Domain={domain_taxonomy}, Size={size_taxonomy}, Compliance={compliance_taxonomy}"
    )

    # --- Get dependencies INSIDE function (WITHOUT abstract type hint) --- #
    memory_adapter = dependencies.get(MemoryPort)
    if not memory_adapter:
        logger.error("MemoryPort not found in dependencies.")
        return {
            "status": "error",
            "message": "Internal configuration error: MemoryPort missing.",
        }
    # --- End Dependency Resolution --- #

    try:
        # Initialize TimeService with SystemTimeService implementation
        set_time_service(SystemTimeService())

        # Create absolute path
        abs_base_path = Path(base_path).expanduser().resolve()

        # Ensure the base directory exists
        abs_base_path.mkdir(parents=True, exist_ok=True)

        # Create project memory - project_info now includes audit fields
        project_info = ProjectInfo(
            name=new_project_name,
            interaction_language=interaction_language,
            documentation_language=documentation_language,
            base_path=abs_base_path,
            platform_taxonomy=platform_taxonomy,
            domain_taxonomy=domain_taxonomy,
            size_taxonomy=size_taxonomy,
            compliance_taxonomy=compliance_taxonomy,
            lifecycle_taxonomy=lifecycle_taxonomy,
            custom_taxonomy=custom_taxonomy if custom_taxonomy else {},
            # created_by/at etc will be set by save_memory
        )

        # Create project memory - directly using fields
        project_memory = ProjectMemory(
            project_info=project_info,
            artifacts={
                Bucket.INITIATE_INITIAL_PRODUCT_DOCS: [
                    ArtifactMeta(
                        name="Project Charter",
                        status=DocumentStatus.PENDING,
                        bucket=Bucket.INITIATE_INITIAL_PRODUCT_DOCS,
                        path=Path("Project_Charter.md"),
                    )
                ]
            },
            platform_taxonomy=platform_taxonomy,
            domain_taxonomy=domain_taxonomy,
            size_taxonomy=size_taxonomy,
            compliance_taxonomy=compliance_taxonomy,
            lifecycle_taxonomy=lifecycle_taxonomy,
            custom_taxonomy=custom_taxonomy if custom_taxonomy else {},
            # created_by/at etc will be set by save_memory
        )

        # Save to memory using the adapter obtained from dependencies
        await memory_adapter.save_memory(project_memory)

        return {
            "status": "ok",
            "message": f"Project '{new_project_name}' created successfully at {abs_base_path}",
            "project_name": new_project_name,
            "base_path": str(abs_base_path),
        }

    except Exception as e:
        logger.error(
            f"Error creating project: {str(e)}", exc_info=True
        )  # Log traceback
        return {"status": "error", "message": f"Failed to create project: {str(e)}"}


@mcp.tool(
    name="paella_list",
    description="Retrieves detailed information for all PAELLADOC projects stored in the system memory",
)
async def paella_list() -> Dict:
    """
    Retrieves detailed information for all PAELLADOC projects stored in the system memory.

    This tool provides comprehensive information about each project, including:
    - Project name and languages
    - Base path and purpose
    - Target audience and objectives
    - All taxonomy configurations
    - Validation status

    This is useful for:
    - Getting an overview of all projects
    - Selecting a project to work on with 'paella_select'
    - Verifying project configurations

    Returns:
        A dictionary containing:
        - status: 'ok' or 'error'
        - projects: List[ProjectInfo] - Complete information for each project
        - message: Description of the operation result
    """
    # --- Get dependencies INSIDE function (WITHOUT abstract type hint) --- #
    memory_adapter = dependencies.get(MemoryPort)
    if not memory_adapter:
        logger.error("MemoryPort not found in dependencies.")
        return {
            "status": "error",
            "message": "Internal configuration error: MemoryPort missing.",
        }
    # --- End Dependency Resolution --- #

    try:
        projects = await memory_adapter.list_projects()

        return {
            "status": "ok",
            "projects": projects,
            "message": "Projects retrieved successfully",
        }
    except Exception as e:
        logger.error(
            f"Error listing projects: {str(e)}", exc_info=True
        )  # Log traceback
        return {"status": "error", "message": f"Failed to list projects: {str(e)}"}


@mcp.tool(
    name="paella_select",
    description="Loads the memory of an existing PAELLADOC project and sets it as the active context",
)
async def paella_select(project_name: str) -> Dict:
    """
    Loads the memory of an existing PAELLADOC project and sets it as the active context.

    This tool makes the specified project the current focus for subsequent conversational
    commands and actions within the Paelladoc session. It retrieves the project's state
    from the persistent memory.

    Args:
        project_name: The name of the existing PAELLADOC project to activate.

    Returns:
        A dictionary containing the operation status ('ok' or 'error'), a confirmation message,
        and key details of the selected project (name, base path). Returns an error status
        if the project is not found.
    """
    logger.info(f"Attempting to select and activate project: '{project_name}'")

    # --- Get dependencies INSIDE function (WITHOUT abstract type hint) --- #
    memory_adapter = dependencies.get(MemoryPort)
    if not memory_adapter:
        logger.error("MemoryPort not found in dependencies.")
        return {
            "status": "error",
            "message": "Internal configuration error: MemoryPort missing.",
        }
    # --- End Dependency Resolution --- #

    try:
        # No longer need to instantiate: memory_adapter = SQLiteMemoryAdapter()
        # First, check if the project exists
        exists = await memory_adapter.project_exists(project_name)

        if not exists:
            logger.warning(f"Project '{project_name}' not found during select.")
            return {"status": "error", "message": f"Project '{project_name}' not found"}

        # If it exists, set it as active
        success = await memory_adapter.set_active_project(project_name)

        if success:
            logger.info(f"Successfully activated project '{project_name}'.")
            return {
                "status": "ok",
                "message": f"Project '{project_name}' selected and activated",
                "project_name": project_name,
            }
        else:
            logger.error(
                f"Failed to set project '{project_name}' as active, despite existence."
            )
            return {
                "status": "error",
                "message": f"Failed to activate project '{project_name}'",
            }
    except Exception as e:
        logger.error(
            f"Error selecting/activating project '{project_name}': {str(e)}",
            exc_info=True,
        )
        return {"status": "error", "message": f"Failed to select project: {str(e)}"}


# Remove the main execution block as this module is not meant to be run directly
# if __name__ == "__main__":
#     mcp.run()

================
File: paelladoc/adapters/plugins/core/continue_proj.py
================
from paelladoc.domain.core_logic import mcp
import logging

# Initialize logger for this module
logger = logging.getLogger(__name__)

# Domain models
from paelladoc.domain.models.project import (
    DocumentStatus,
    Bucket,
)

# Adapter for persistence
from paelladoc.adapters.output.sqlite.sqlite_memory_adapter import SQLiteMemoryAdapter

# Dependency Injection
from paelladoc.dependencies import get_container
from paelladoc.ports.output.configuration_port import ConfigurationPort

# Get configuration port from container
container = get_container()
config_port: ConfigurationPort = container.get_configuration_port()


@mcp.tool(
    name="core_continue", description="Continues work on an existing PAELLADOC project."
)
async def core_continue(
    project_name: str,
) -> dict:
    """Loads an existing project's memory and suggests the next steps.

    Args:
        project_name (str): The name of the project to continue.

    Behavior Config: Behavior configuration is now loaded dynamically from the database.
    """
    logging.info(
        f"Executing initial implementation for core.continue for project: {project_name}..."
    )

    # --- Get Behavior Config & Bucket Order ---
    try:
        # TODO: Implement behavior config usage in next iteration
        # Keeping this to maintain API compatibility
        _ = await config_port.get_behavior_config(category="continue")
        # Example: Get a specific config value (add more as needed)
        # documentation_first = behavior_config.get("documentation_first", True)

        # Get bucket order from DB
        bucket_order_names = await config_port.get_bucket_order(category="default")
        if not bucket_order_names:
            logger.warning(
                "No default bucket order found in config DB. Using fallback enum order."
            )
            # Fallback to basic Enum order if DB config is missing
            bucket_order = list(Bucket)
        else:
            # Convert bucket names back to Enum members if possible
            bucket_order = []
            for name in bucket_order_names:
                try:
                    bucket_order.append(Bucket(name))
                except ValueError:
                    logger.warning(
                        f"Bucket name '{name}' from DB config is not a valid Bucket enum member. Skipping."
                    )
            # Add any missing standard buckets at the end (optional, depends on desired behavior)
            missing_buckets = [b for b in list(Bucket) if b not in bucket_order]
            bucket_order.extend(missing_buckets)

        logger.info(f"Using bucket order: {[b.value for b in bucket_order]}")

    except Exception as e:
        logger.error(
            f"Failed to load configuration for core.continue: {e}", exc_info=True
        )
        # Decide on fallback behavior - maybe use hardcoded defaults here? For now, fail hard.
        return {"status": "error", "message": "Failed to load necessary configuration."}

    # --- Dependency Injection (Temporary Direct Instantiation) ---
    # TODO: Replace with proper dependency injection
    try:
        # Use the default path defined in the adapter (project root)
        memory_adapter = SQLiteMemoryAdapter()
        logger.info(f"core.continue using DB path: {memory_adapter.db_path.resolve()}")

    except Exception as e:
        logging.error(f"Failed to instantiate SQLiteMemoryAdapter: {e}", exc_info=True)
        return {
            "status": "error",
            "message": "Internal server error: Could not initialize memory adapter.",
        }

    # --- Load Project Memory ---
    try:
        memory = await memory_adapter.load_memory(project_name)
        if not memory:
            logging.warning(f"Project '{project_name}' not found for CONTINUE command.")
            return {
                "status": "error",
                "message": f"Project '{project_name}' not found. Use PAELLA command to start it.",
            }
        logging.info(f"Successfully loaded memory for project: {project_name}")

    except Exception as e:
        logging.error(f"Error loading memory for '{project_name}': {e}", exc_info=True)
        return {
            "status": "error",
            "message": f"Failed to load project memory: {e}",
        }

    # --- Calculate Next Step (Simplified) ---
    # Uses bucket_order loaded from config_port
    # TODO: Implement sophisticated logic based on behavior_config loaded from config_port

    next_step_suggestion = (
        "No pending artifacts found. Project might be complete or need verification."
    )
    found_pending = False

    for bucket in bucket_order:  # Use the dynamically loaded order
        artifacts_in_bucket = memory.artifacts.get(bucket, [])
        for artifact in artifacts_in_bucket:
            if artifact.status == DocumentStatus.PENDING:
                next_step_suggestion = f"Next suggested step: Work on artifact '{artifact.name}' ({artifact.path}) in bucket '{bucket.value}'."
                found_pending = True
                break
        if found_pending:
            break

    # Get overall phase completion for context
    phase_completion_summary = "Phase completion: "
    phases = sorted(
        list(set(b.value.split("::")[0] for b in Bucket if "::" in b.value))
    )
    phase_summaries = []
    try:
        for phase in phases:
            stats = memory.get_phase_completion(phase)
            if stats["total"] > 0:
                phase_summaries.append(
                    f"{phase}({stats['completion_percentage']:.0f}%)"
                )
        if not phase_summaries:
            phase_completion_summary += "(No artifacts tracked yet)"
        else:
            phase_completion_summary += ", ".join(phase_summaries)

    except Exception as e:
        logging.warning(f"Could not calculate phase completion: {e}")
        phase_completion_summary += "(Calculation error)"

    # --- Return Status and Suggestion ---
    return {
        "status": "ok",
        "message": f"Project '{project_name}' loaded. {phase_completion_summary}",
        "next_step": next_step_suggestion,
    }

================
File: paelladoc/adapters/plugins/core/help.py
================
from paelladoc.domain.core_logic import mcp
import logging

# Adapter for taxonomy loading
from paelladoc.adapters.output.filesystem.taxonomy_provider import (
    FileSystemTaxonomyProvider,
)

# Dependency Injection
from paelladoc.dependencies import get_container
from paelladoc.ports.output.configuration_port import ConfigurationPort

# Instantiate the taxonomy provider
# TODO: Replace direct instantiation with Dependency Injection
TAXONOMY_PROVIDER = FileSystemTaxonomyProvider()

# Get configuration port from container
container = get_container()
config_port: ConfigurationPort = container.get_configuration_port()

# Insert behavior config here

# TODO: Review imports and add any other necessary modules


@mcp.tool(
    name="core_help",
    description="Shows help information about available commands",
)
async def core_help(command: str = None, format: str = "detailed") -> dict:
    """Provides help information about available PAELLADOC commands.

    Args:
        command: Optional specific command to get help for
        format: Output format (detailed, summary, examples)

    Returns:
        Dictionary with help information
    """
    logging.info(f"Executing core.help with command={command}, format={format}")

    # Load commands from DB via ConfigurationPort
    try:
        commands_config = await config_port.get_commands_metadata()
        if not commands_config:
            logging.warning(
                "No commands found in config DB. Using fallback empty dict."
            )
            commands_config = {}

        logging.info(f"Loaded {len(commands_config)} commands from configuration DB")
    except Exception as e:
        logging.error(f"Failed to load commands from DB: {e}", exc_info=True)
        # Return error if commands can't be loaded (critical)
        return {
            "status": "error",
            "message": "Failed to load commands information from database.",
        }

    # If a specific command is requested
    if command and command in commands_config:
        return {"status": "ok", "command": command, "help": commands_config[command]}

    # Otherwise return all commands
    result = {
        "status": "ok",
        "available_commands": list(commands_config.keys()),
        "format": format,
    }

    # Add command information based on format
    if format == "detailed":
        result["commands"] = commands_config
        try:
            available_taxonomies = TAXONOMY_PROVIDER.get_available_taxonomies()
            if "select_taxonomy" in commands_config:
                commands_config["select_taxonomy"]["available_options"] = (
                    available_taxonomies
                )
            if "taxonomy_info" in commands_config:
                commands_config["taxonomy_info"]["available_taxonomies"] = (
                    available_taxonomies
                )
        except Exception as e:
            logging.error(f"Failed to load taxonomies for help: {e}", exc_info=True)
            # Continue without taxonomy info if loading fails
    elif format == "summary":
        result["commands"] = {
            cmd: info["description"] for cmd, info in commands_config.items()
        }
    elif format == "examples":
        result["commands"] = {
            cmd: info.get("example", "") for cmd, info in commands_config.items()
        }

    return result

================
File: paelladoc/adapters/plugins/memory/project_memory.py
================
from paelladoc.domain.core_logic import mcp
import logging

# Insert behavior config here

# TODO: Review imports and add any other necessary modules


@mcp.tool(
    name="project_memory",
    description="Manages the project's memory file (.memory.json)",
)
def memory_project_memory() -> dict:
    """Handles operations related to the project memory.

    Likely used internally by other commands (PAELLA, CONTINUE, VERIFY)
    to load, save, and update project state, progress, and metadata.
    Provides the HELP CONTEXT (though this might be deprecated).
    """

    # TODO: Implement the actual logic of the command here
    # Access parameters using their variable names (e.g., param)
    # Access behavior config using BEHAVIOR_CONFIG dict (if present)
    logging.info("Executing stub for memory.project_memory...")

    # Example: Print parameters
    local_vars = locals()
    param_values = {}
    logging.info(f"Parameters received: {param_values}")

    # Replace with actual return value based on command logic
    return {
        "status": "ok",
        "message": "Successfully executed stub for memory.project_memory",
    }

================
File: paelladoc/adapters/plugins/code/generate_doc.py
================
from paelladoc.domain.core_logic import mcp
import logging

# Insert behavior config here

# TODO: Review imports and add any other necessary modules


@mcp.tool(name="generate_doc", description="3. Wait for user selection")
def code_generate_doc() -> dict:
    """3. Wait for user selection"""

    # TODO: Implement the actual logic of the command here
    # Access parameters using their variable names (e.g., param)
    # Access behavior config using BEHAVIOR_CONFIG dict (if present)
    logging.info("Executing stub for code.generate_doc...")

    # Example: Print parameters
    local_vars = locals()
    param_values = {}
    logging.info(f"Parameters received: {param_values}")

    # Replace with actual return value based on command logic
    return {
        "status": "ok",
        "message": "Successfully executed stub for code.generate_doc",
    }

================
File: paelladoc/adapters/plugins/code/generate_context.py
================
from paelladoc.domain.core_logic import mcp
import logging

# Insert behavior config here

# TODO: Review imports and add any other necessary modules


@mcp.tool(
    name="generate_context",
    description="This automatically creates the context file that will be used by GENERATE-DOC for interactive documentation generation.",
)
def code_generate_context() -> dict:
    """This automatically creates the context file that will be used by GENERATE-DOC for interactive documentation generation."""

    # TODO: Implement the actual logic of the command here
    # Access parameters using their variable names (e.g., param)
    # Access behavior config using BEHAVIOR_CONFIG dict (if present)
    logging.info("Executing stub for code.generate_context...")

    # Example: Print parameters
    local_vars = locals()
    param_values = {}
    logging.info(f"Parameters received: {param_values}")

    # Replace with actual return value based on command logic
    return {
        "status": "ok",
        "message": "Successfully executed stub for code.generate_context",
    }

================
File: paelladoc/adapters/plugins/code/code_generation.py
================
from paelladoc.domain.core_logic import mcp
import logging


# Extracted behavior configuration from the original MDC file
BEHAVIOR_CONFIG = {
    "abort_if_documentation_incomplete": True,
    "code_after_documentation": True,
    "confirm_each_parameter": True,
    "conversation_required": True,
    "documentation_first": True,
    "documentation_verification_path": "/docs/{project_name}/.memory.json",
    "enforce_one_question_rule": True,
    "extract_from_complete_documentation": True,
    "force_single_question_mode": True,
    "guide_to_continue_command": True,
    "interactive": True,
    "max_questions_per_message": 1,
    "one_parameter_at_a_time": True,
    "prevent_web_search": True,
    "prohibit_multiple_questions": True,
    "require_complete_documentation": True,
    "require_step_confirmation": True,
    "required_documentation_sections": [
        "project_definition",
        "market_research",
        "user_research",
        "problem_definition",
        "product_definition",
        "architecture_decisions",
        "product_roadmap",
        "user_stories",
        "technical_architecture",
        "technical_specifications",
        "api_specification",
        "database_design",
    ],
    "sequential_questions": True,
    "single_question_mode": True,
    "strict_parameter_sequence": True,
    "strict_question_sequence": True,
    "verify_documentation_completeness": True,
    "wait_for_response": True,
    "wait_for_user_response": True,
}

# TODO: Review imports and add any other necessary modules


@mcp.tool(
    name="code_generation",
    description="The command uses the script at `.cursor/rules/scripts/extract_repo_content.py` to perform the repository extraction, which leverages repopack-py to convert the codebase to text.",
)
def code_code_generation() -> dict:
    """The command uses the script at `.cursor/rules/scripts/extract_repo_content.py` to perform the repository extraction, which leverages repopack-py to convert the codebase to text.

    Behavior Config: this tool has associated behavior configuration extracted
    from the MDC file. See the `BEHAVIOR_CONFIG` variable in the source code.
    """

    # TODO: Implement the actual logic of the command here
    # Access parameters using their variable names (e.g., param)
    # Access behavior config using BEHAVIOR_CONFIG dict (if present)
    logging.info("Executing stub for code.code_generation...")

    # Example: Print parameters
    local_vars = locals()
    param_values = {}
    logging.info(f"Parameters received: {param_values}")

    # Replace with actual return value based on command logic
    return {
        "status": "ok",
        "message": "Successfully executed stub for code.code_generation",
    }

================
File: paelladoc/adapters/plugins/product/product_management.py
================
"""Product management module for PAELLADOC."""

from typing import Dict, Any, List, Optional
from paelladoc.domain.core_logic import mcp
import logging

logger = logging.getLogger(__name__)

# Insert behavior config here

# TODO: Review imports and add any other necessary modules


@mcp.tool(
    name="core_manage_story",
    description="Manages user stories in the project with CRUD operations",
)
async def manage_story(
    operation: str,
    id: Optional[str] = None,
    title: Optional[str] = None,
    description: Optional[str] = None,
    acceptance_criteria: Optional[List[str]] = None,
    priority: Optional[str] = None,
    points: Optional[int] = None,
    status: Optional[str] = None,
    sprint: Optional[str] = None,
) -> Dict[str, Any]:
    """Manages user stories in the project.

    ACTION: Performs CRUD operations on user stories.

    INPUT:
    - operation: Operation to perform (create/update/delete/list/show/prioritize)
    - id: Story identifier (required for update/delete/show)
    - title: Story title (required for create)
    - description: Story description in user story format
    - acceptance_criteria: List of acceptance criteria
    - priority: Priority level (critical/high/medium/low)
    - points: Story points estimate
    - status: Current status
    - sprint: Sprint identifier

    OUTPUT: MUST return ONLY the raw JSON response:
    - On success: { "status": "ok", "story": StoryInfo }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Validate all required fields based on operation
    2. Ensure atomic operations
    3. Return standardized response format
    """
    try:
        # Implementation here
        return {
            "status": "ok",
            "message": "Story operation completed successfully",
            "operation": operation,
        }
    except Exception as e:
        logger.error(f"Error in story management: {e}")
        return {"status": "error", "message": str(e)}


@mcp.tool(
    name="core_manage_task",
    description="Manages tasks in the project with CRUD operations",
)
async def manage_task(
    operation: str,
    id: Optional[str] = None,
    title: Optional[str] = None,
    description: Optional[str] = None,
    story_id: Optional[str] = None,
    assignee: Optional[str] = None,
    status: Optional[str] = None,
    estimate: Optional[float] = None,
    due_date: Optional[str] = None,
    dependencies: Optional[List[str]] = None,
    blockers: Optional[str] = None,
) -> Dict[str, Any]:
    """Manages tasks in the project.

    ACTION: Performs CRUD operations on tasks.

    INPUT:
    - operation: Operation to perform (create/update/delete/list/show/assign)
    - id: Task identifier (required for update/delete/show)
    - title: Task title (required for create)
    - description: Task description
    - story_id: Parent user story identifier
    - assignee: Person assigned to the task
    - status: Current status
    - estimate: Estimated hours
    - due_date: Due date (YYYY-MM-DD)
    - dependencies: List of dependent task IDs
    - blockers: Description of blockers

    OUTPUT: MUST return ONLY the raw JSON response:
    - On success: { "status": "ok", "task": TaskInfo }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Validate all required fields based on operation
    2. Ensure atomic operations
    3. Return standardized response format
    """
    try:
        # Implementation here
        return {
            "status": "ok",
            "message": "Task operation completed successfully",
            "operation": operation,
        }
    except Exception as e:
        logger.error(f"Error in task management: {e}")
        return {"status": "error", "message": str(e)}


@mcp.tool(
    name="core_manage_sprint",
    description="Manages sprint planning and execution with CRUD operations",
)
async def manage_sprint(
    operation: str,
    id: Optional[str] = None,
    name: Optional[str] = None,
    goal: Optional[str] = None,
    start_date: Optional[str] = None,
    end_date: Optional[str] = None,
    capacity: Optional[int] = None,
    stories: Optional[List[str]] = None,
    status: Optional[str] = None,
) -> Dict[str, Any]:
    """Manages sprint planning and execution.

    ACTION: Performs CRUD operations on sprints.

    INPUT:
    - operation: Operation to perform (create/update/delete/start/end/plan/list/show)
    - id: Sprint identifier (required for update/delete/show)
    - name: Sprint name (required for create)
    - goal: Sprint goal
    - start_date: Start date (YYYY-MM-DD)
    - end_date: End date (YYYY-MM-DD)
    - capacity: Team capacity in story points
    - stories: List of story IDs
    - status: Sprint status

    OUTPUT: MUST return ONLY the raw JSON response:
    - On success: { "status": "ok", "sprint": SprintInfo }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Validate all required fields based on operation
    2. Ensure atomic operations
    3. Return standardized response format
    """
    try:
        # Implementation here
        return {
            "status": "ok",
            "message": "Sprint operation completed successfully",
            "operation": operation,
        }
    except Exception as e:
        logger.error(f"Error in sprint management: {e}")
        return {"status": "error", "message": str(e)}

================
File: paelladoc/adapters/plugins/styles/coding_styles.py
================
"""Coding styles management module for PAELLADOC."""

from typing import Dict, Any, List, Optional
from paelladoc.domain.core_logic import mcp
import logging

logger = logging.getLogger(__name__)

# Insert behavior config here

# TODO: Review imports and add any other necessary modules


@mcp.tool(
    name="core_manage_coding_style",
    description="Manages coding style definitions with CRUD operations",
)
async def manage_coding_style(
    operation: str,
    id: Optional[str] = None,
    name: Optional[str] = None,
    language: Optional[str] = None,
    rules: Optional[Dict[str, Any]] = None,
    formatters: Optional[List[Dict[str, Any]]] = None,
    linters: Optional[List[Dict[str, Any]]] = None,
    metadata: Optional[Dict[str, Any]] = None,
    version: Optional[str] = None,
) -> Dict[str, Any]:
    """Manages coding style definitions.

    ACTION: Performs CRUD operations on coding style definitions.

    INPUT:
    - operation: Operation to perform (create/update/delete/list/show/export/import)
    - id: Style identifier (required for update/delete/show)
    - name: Style name (required for create)
    - language: Programming language
    - rules: Dictionary of style rules and their configurations
    - formatters: List of code formatter configurations
    - linters: List of linter configurations
    - metadata: Additional style metadata
    - version: Style version

    OUTPUT: MUST return ONLY the raw JSON response:
    - On success: { "status": "ok", "style": StyleInfo }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Validate all required fields based on operation
    2. Ensure atomic operations
    3. Return standardized response format
    4. Validate rule syntax and compatibility
    """
    try:
        # Implementation here
        return {
            "status": "ok",
            "message": "Coding style operation completed successfully",
            "operation": operation,
        }
    except Exception as e:
        logger.error(f"Error in coding style management: {e}")
        return {"status": "error", "message": str(e)}


@mcp.tool(
    name="core_manage_style_rule",
    description="Manages individual coding style rules with CRUD operations",
)
async def manage_style_rule(
    operation: str,
    style_id: str,
    id: Optional[str] = None,
    name: Optional[str] = None,
    category: Optional[str] = None,
    pattern: Optional[str] = None,
    severity: Optional[str] = None,
    fix: Optional[Dict[str, Any]] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """Manages coding style rules.

    ACTION: Performs CRUD operations on coding style rules.

    INPUT:
    - operation: Operation to perform (create/update/delete/list/show)
    - style_id: Parent style identifier
    - id: Rule identifier (required for update/delete/show)
    - name: Rule name (required for create)
    - category: Rule category (e.g., "formatting", "naming", "complexity")
    - pattern: Rule pattern or definition
    - severity: Rule severity level
    - fix: Automated fix configuration
    - metadata: Additional rule metadata

    OUTPUT: MUST return ONLY the raw JSON response:
    - On success: { "status": "ok", "rule": RuleInfo }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Validate all required fields based on operation
    2. Ensure atomic operations
    3. Return standardized response format
    4. Validate rule pattern syntax
    """
    try:
        # Implementation here
        return {
            "status": "ok",
            "message": "Style rule operation completed successfully",
            "operation": operation,
        }
    except Exception as e:
        logger.error(f"Error in style rule management: {e}")
        return {"status": "error", "message": str(e)}


@mcp.tool(
    name="core_manage_style_validation",
    description="Manages code validation against coding styles",
)
async def manage_style_validation(
    operation: str,
    style_id: str,
    code: Optional[str] = None,
    file_path: Optional[str] = None,
    fix: Optional[bool] = False,
    ignore_rules: Optional[List[str]] = None,
    custom_config: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """Manages code validation against coding styles.

    ACTION: Performs validation operations on code using defined styles.

    INPUT:
    - operation: Operation to perform (validate/fix/analyze)
    - style_id: Style identifier to validate against
    - code: Code string to validate (mutually exclusive with file_path)
    - file_path: Path to file to validate (mutually exclusive with code)
    - fix: Whether to attempt automatic fixes
    - ignore_rules: List of rule IDs to ignore
    - custom_config: Custom validation configuration

    OUTPUT: MUST return ONLY the raw JSON response:
    - On success: { "status": "ok", "validation": ValidationResults }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Validate input parameters
    2. Ensure atomic operations
    3. Return standardized response format
    4. Handle large files efficiently
    """
    try:
        # Implementation here
        return {
            "status": "ok",
            "message": "Style validation operation completed successfully",
            "operation": operation,
        }
    except Exception as e:
        logger.error(f"Error in style validation: {e}")
        return {"status": "error", "message": str(e)}

================
File: paelladoc/adapters/plugins/styles/git_workflows.py
================
from paelladoc.domain.core_logic import mcp
import logging

# Insert behavior config here

# TODO: Review imports and add any other necessary modules


@mcp.tool(
    name="git_workflows",
    description="Manages Git workflow methodologies for the project.",
)
def styles_git_workflows() -> dict:
    """Applies or customizes Git workflows.

    Supports workflows like GitHub Flow, GitFlow, Trunk-Based.
    Provides guidance based on project complexity.
    Simple projects → GitHub Flow
    Complex projects → GitFlow or Trunk-Based
    """

    # TODO: Implement the actual logic of the command here
    # Access parameters using their variable names (e.g., param)
    # Access behavior config using BEHAVIOR_CONFIG dict (if present)
    logging.info("Executing stub for styles.git_workflows...")

    # Example: Print parameters
    local_vars = locals()
    param_values = {}
    logging.info(f"Parameters received: {param_values}")

    # Replace with actual return value based on command logic
    return {
        "status": "ok",
        "message": "Successfully executed stub for styles.git_workflows",
    }

================
File: paelladoc/adapters/plugins/templates/templates.py
================
"""Templates management module for PAELLADOC."""

from typing import Dict, Any, List, Optional
from paelladoc.domain.core_logic import mcp
import logging

logger = logging.getLogger(__name__)

# Insert behavior config here

# TODO: Review imports and add any other necessary modules


@mcp.tool(
    name="core_manage_template",
    description="Manages documentation templates with CRUD operations",
)
async def manage_template(
    operation: str,
    id: Optional[str] = None,
    name: Optional[str] = None,
    category: Optional[str] = None,
    content: Optional[str] = None,
    variables: Optional[Dict[str, str]] = None,
    metadata: Optional[Dict[str, Any]] = None,
    tags: Optional[List[str]] = None,
    version: Optional[str] = None,
) -> Dict[str, Any]:
    """Manages documentation templates.

    ACTION: Performs CRUD operations on documentation templates.

    INPUT:
    - operation: Operation to perform (create/update/delete/list/show/export/import)
    - id: Template identifier (required for update/delete/show)
    - name: Template name (required for create)
    - category: Template category (e.g., "technical", "user", "api")
    - content: Template content with variable placeholders
    - variables: Dictionary of variable names and descriptions
    - metadata: Additional template metadata
    - tags: List of searchable tags
    - version: Template version

    OUTPUT: MUST return ONLY the raw JSON response:
    - On success: { "status": "ok", "template": TemplateInfo }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Validate all required fields based on operation
    2. Ensure atomic operations
    3. Return standardized response format
    4. Validate template syntax before save
    """
    try:
        # Implementation here
        return {
            "status": "ok",
            "message": "Template operation completed successfully",
            "operation": operation,
        }
    except Exception as e:
        logger.error(f"Error in template management: {e}")
        return {"status": "error", "message": str(e)}


@mcp.tool(
    name="core_manage_template_variable",
    description="Manages template variables with CRUD operations",
)
async def manage_template_variable(
    operation: str,
    template_id: str,
    name: Optional[str] = None,
    description: Optional[str] = None,
    default_value: Optional[str] = None,
    validation_rules: Optional[Dict[str, Any]] = None,
    required: Optional[bool] = None,
) -> Dict[str, Any]:
    """Manages template variables.

    ACTION: Performs CRUD operations on template variables.

    INPUT:
    - operation: Operation to perform (create/update/delete/list/show)
    - template_id: Parent template identifier
    - name: Variable name (required for create)
    - description: Variable description
    - default_value: Default value if not provided
    - validation_rules: Rules for validating variable values
    - required: Whether the variable is required

    OUTPUT: MUST return ONLY the raw JSON response:
    - On success: { "status": "ok", "variable": VariableInfo }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Validate all required fields based on operation
    2. Ensure atomic operations
    3. Return standardized response format
    4. Validate variable name format
    """
    try:
        # Implementation here
        return {
            "status": "ok",
            "message": "Template variable operation completed successfully",
            "operation": operation,
        }
    except Exception as e:
        logger.error(f"Error in template variable management: {e}")
        return {"status": "error", "message": str(e)}


@mcp.tool(
    name="core_manage_template_instance",
    description="Manages template instances with CRUD operations",
)
async def manage_template_instance(
    operation: str,
    template_id: str,
    id: Optional[str] = None,
    name: Optional[str] = None,
    variables: Optional[Dict[str, str]] = None,
    output_format: Optional[str] = None,
    metadata: Optional[Dict[str, Any]] = None,
) -> Dict[str, Any]:
    """Manages template instances.

    ACTION: Performs CRUD operations on template instances.

    INPUT:
    - operation: Operation to perform (create/update/delete/list/show/render)
    - template_id: Parent template identifier
    - id: Instance identifier (required for update/delete/show)
    - name: Instance name (required for create)
    - variables: Dictionary of variable values
    - output_format: Desired output format (e.g., "md", "html", "pdf")
    - metadata: Additional instance metadata

    OUTPUT: MUST return ONLY the raw JSON response:
    - On success: { "status": "ok", "instance": InstanceInfo }
    - On error: { "status": "error", "message": "Error description" }

    EXECUTION RULES:
    1. Validate all required fields based on operation
    2. Ensure atomic operations
    3. Return standardized response format
    4. Validate all required variables are provided
    """
    try:
        # Implementation here
        return {
            "status": "ok",
            "message": "Template instance operation completed successfully",
            "operation": operation,
        }
    except Exception as e:
        logger.error(f"Error in template instance management: {e}")
        return {"status": "error", "message": str(e)}

================
File: paelladoc/adapters/output/chroma/chroma_vector_store_adapter.py
================
import logging
import uuid
from typing import List, Dict, Any, Optional
from pathlib import Path

import chromadb
from chromadb.api.models.Collection import Collection

# Import NotFoundError from the appropriate module depending on chromadb version
try:
    from chromadb.errors import NotFoundError
except ImportError:
    try:
        from chromadb.api.errors import NotFoundError
    except ImportError:

        class NotFoundError(ValueError):
            """Fallback NotFoundError inheriting from ValueError for broader compatibility."""

            pass


# Ports and Domain Models/Helpers
from paelladoc.ports.output.vector_store_port import VectorStorePort, SearchResult

logger = logging.getLogger(__name__)

# Default path for persistent ChromaDB data
DEFAULT_CHROMA_PATH = Path.home() / ".paelladoc" / "chroma_data"


class ChromaSearchResult(SearchResult):
    """Concrete implementation of SearchResult for Chroma results."""

    def __init__(
        self,
        id: str,
        distance: Optional[float],
        metadata: Optional[Dict[str, Any]],
        document: Optional[str],
    ):
        self.id = id
        self.distance = distance
        self.metadata = metadata
        self.document = document


class ChromaVectorStoreAdapter(VectorStorePort):
    """ChromaDB implementation of the VectorStorePort."""

    def __init__(
        self,
        persist_path: Optional[Path] = DEFAULT_CHROMA_PATH,
        in_memory: bool = False,
    ):
        """Initializes the ChromaDB client.

        Args:
            persist_path: Path to store persistent Chroma data. Ignored if in_memory is True.
            in_memory: If True, runs ChromaDB entirely in memory (data is lost on exit).
        """
        if in_memory:
            logger.info("Initializing ChromaDB client in-memory.")
            self.client = chromadb.Client()
        else:
            self.persist_path = persist_path or DEFAULT_CHROMA_PATH
            self.persist_path.mkdir(parents=True, exist_ok=True)
            logger.info(
                f"Initializing persistent ChromaDB client at: {self.persist_path}"
            )
            self.client = chromadb.PersistentClient(path=str(self.persist_path))

        # TODO: Consider configuration for embedding function, distance function, etc.
        # Using Chroma's defaults for now (all-MiniLM-L6-v2 and cosine distance)

    async def get_or_create_collection(self, collection_name: str) -> Collection:
        """Gets or creates a Chroma collection."""
        try:
            collection = self.client.get_collection(name=collection_name)
            logger.debug(f"Retrieved existing Chroma collection: {collection_name}")
            return collection
        except (NotFoundError, ValueError) as e:
            # Handle case where collection does not exist (NotFoundError or ValueError)
            if "does not exist" in str(e):  # Check if the error indicates non-existence
                logger.debug(f"Collection '{collection_name}' not found, creating...")
                collection = self.client.create_collection(name=collection_name)
                logger.info(f"Created new Chroma collection: {collection_name}")
                return collection
            else:
                logger.error(
                    f"Unexpected error getting collection '{collection_name}': {e}",
                    exc_info=True,
                )
                raise
        except Exception as e:
            logger.error(
                f"Error getting or creating collection '{collection_name}': {e}",
                exc_info=True,
            )
            raise

    async def add_documents(
        self,
        collection_name: str,
        documents: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ) -> List[str]:
        """Adds documents to the specified Chroma collection."""
        collection = await self.get_or_create_collection(collection_name)

        # Generate IDs if not provided
        if not ids:
            ids = [str(uuid.uuid4()) for _ in documents]
        elif len(ids) != len(documents):
            raise ValueError("Number of ids must match number of documents")

        # Add documents to the collection (this handles embedding generation)
        try:
            # collection.add is synchronous in the current chromadb client API
            collection.add(documents=documents, metadatas=metadatas, ids=ids)
            logger.info(
                f"Added {len(documents)} documents to collection '{collection_name}'."
            )
            return ids
        except Exception as e:
            logger.error(
                f"Error adding documents to collection '{collection_name}': {e}",
                exc_info=True,
            )
            raise

    async def search_similar(
        self,
        collection_name: str,
        query_texts: List[str],
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None,
        where_document: Optional[Dict[str, Any]] = None,
        include: Optional[List[str]] = ["metadatas", "documents", "distances"],
    ) -> List[List[SearchResult]]:
        """Searches for similar documents in the Chroma collection."""
        try:
            collection = self.client.get_collection(name=collection_name)
        except (NotFoundError, ValueError) as e:
            # Handle case where collection does not exist
            if "does not exist" in str(e):
                logger.warning(f"Collection '{collection_name}' not found for search.")
                return [[] for _ in query_texts]
            else:
                logger.error(
                    f"Unexpected error retrieving collection '{collection_name}' for search: {e}",
                    exc_info=True,
                )
                raise
        except Exception as e:
            logger.error(
                f"Error retrieving collection '{collection_name}' for search: {e}",
                exc_info=True,
            )
            raise

        try:
            # collection.query is synchronous
            results = collection.query(
                query_texts=query_texts,
                n_results=n_results,
                where=where,
                where_document=where_document,
                include=include,
            )

            # Map Chroma's result structure to our SearchResult list of lists
            # Chroma returns a dict with keys like 'ids', 'distances', 'metadatas', 'documents'
            # Each value is a list of lists (one inner list per query)
            mapped_results: List[List[SearchResult]] = []
            num_queries = len(query_texts)
            result_ids = results.get("ids") or [[] for _ in range(num_queries)]
            result_distances = results.get("distances") or [
                [] for _ in range(num_queries)
            ]
            result_metadatas = results.get("metadatas") or [
                [] for _ in range(num_queries)
            ]
            result_documents = results.get("documents") or [
                [] for _ in range(num_queries)
            ]

            for i in range(num_queries):
                query_results = []
                # Ensure all result lists have the expected length for the i-th query
                num_docs_for_query = (
                    len(result_ids[i]) if result_ids and i < len(result_ids) else 0
                )
                for j in range(num_docs_for_query):
                    query_results.append(
                        ChromaSearchResult(
                            id=result_ids[i][j]
                            if result_ids
                            and i < len(result_ids)
                            and j < len(result_ids[i])
                            else "N/A",
                            distance=result_distances[i][j]
                            if result_distances
                            and i < len(result_distances)
                            and j < len(result_distances[i])
                            else None,
                            metadata=result_metadatas[i][j]
                            if result_metadatas
                            and i < len(result_metadatas)
                            and j < len(result_metadatas[i])
                            else None,
                            document=result_documents[i][j]
                            if result_documents
                            and i < len(result_documents)
                            and j < len(result_documents[i])
                            else None,
                        )
                    )
                mapped_results.append(query_results)

            return mapped_results

        except Exception as e:
            logger.error(
                f"Error querying collection '{collection_name}': {e}", exc_info=True
            )
            raise

    async def delete_collection(self, collection_name: str) -> None:
        """Deletes a Chroma collection."""
        try:
            self.client.delete_collection(name=collection_name)
            logger.info(f"Deleted Chroma collection: {collection_name}")
        except (NotFoundError, ValueError) as e:
            # Handle case where collection does not exist
            if "does not exist" in str(e):
                logger.warning(
                    f"Attempted to delete non-existent collection: {collection_name}"
                )
            else:
                logger.error(
                    f"Unexpected error deleting collection '{collection_name}': {e}",
                    exc_info=True,
                )
                raise
        except Exception as e:
            logger.error(
                f"Error deleting collection '{collection_name}': {e}", exc_info=True
            )
            raise

================
File: paelladoc/adapters/output/filesystem/taxonomy_provider.py
================
import logging
from typing import Dict, List
import importlib.resources

from paelladoc.ports.output.taxonomy_provider import TaxonomyProvider

logger = logging.getLogger(__name__)

# Removed old path calculation based on __file__
# # Determine the base path relative to this file's location
# # Assumes this structure: src/paelladoc/adapters/output/filesystem/taxonomy_provider.py
# # And taxonomies are at: project_root/taxonomies/
# ADAPTER_DIR = Path(__file__).parent
# SRC_DIR = ADAPTER_DIR.parent.parent.parent
# PROJECT_ROOT = SRC_DIR.parent
# TAXONOMY_BASE_PATH = PROJECT_ROOT / "taxonomies"


class FileSystemTaxonomyProvider(TaxonomyProvider):
    """Provides available taxonomy information by scanning package resources."""

    def __init__(self):
        """Initializes the provider."""
        # Base path is now determined dynamically using importlib.resources
        # self.base_path = base_path # Removed base_path argument
        # if not self.base_path.is_dir(): # Check happens within get_available_taxonomies
        #     logger.error(
        #         f"Taxonomy base path not found or not a directory: {self.base_path.resolve()}"
        #     )
        self._cached_taxonomies: Dict[str, List[str]] | None = None

    def get_available_taxonomies(self) -> Dict[str, List[str]]:
        """Scans the package resources for taxonomy directories and loads available taxonomy names.

        Uses a simple cache to avoid repeated resource scanning.
        """
        if self._cached_taxonomies is not None:
            logger.debug("Returning cached taxonomies")
            return self._cached_taxonomies

        available_taxonomies = {}
        categories = ["platform", "domain", "size", "compliance"]

        try:
            # Get the base path to the 'taxonomies' directory within the 'paelladoc' package
            base_taxonomy_path_resource = importlib.resources.files(
                "paelladoc"
            ).joinpath("taxonomies")
            logger.debug(
                f"Scanning for taxonomies in package resource: {base_taxonomy_path_resource}"
            )

            if not base_taxonomy_path_resource.is_dir():
                logger.error(
                    f"Cannot scan taxonomies, package resource directory not found: {base_taxonomy_path_resource}"
                )
                self._cached_taxonomies = {cat: [] for cat in categories}
                return self._cached_taxonomies

            for category in categories:
                category_path_resource = base_taxonomy_path_resource.joinpath(category)
                if category_path_resource.is_dir():
                    try:
                        tax_files = sorted(
                            f.name.removesuffix(
                                ".json"
                            )  # Get filename without .json extension
                            for f in category_path_resource.iterdir()
                            if f.is_file() and f.name.endswith(".json")
                        )
                        available_taxonomies[category] = tax_files
                        logger.debug(
                            f"Found {len(tax_files)} taxonomies in '{category}': {tax_files}"
                        )
                    except OSError as e:
                        logger.error(
                            f"Error reading taxonomy package resource directory {category_path_resource}: {e}"
                        )
                        available_taxonomies[category] = []
                else:
                    available_taxonomies[category] = []
                    logger.warning(
                        f"Taxonomy package resource directory not found: {category_path_resource}"
                    )

        except ModuleNotFoundError:
            logger.error(
                "Could not find package 'paelladoc' via importlib.resources. Is the package installed correctly?"
            )
            self._cached_taxonomies = {cat: [] for cat in categories}
            return self._cached_taxonomies
        except Exception as e:
            logger.error(
                f"Unexpected error scanning taxonomy resources: {e}", exc_info=True
            )
            self._cached_taxonomies = {cat: [] for cat in categories}
            return self._cached_taxonomies

        self._cached_taxonomies = available_taxonomies
        logger.info(
            f"Loaded {sum(len(v) for v in available_taxonomies.values())} taxonomies across {len(categories)} categories from package resources."
        )
        return available_taxonomies

================
File: paelladoc/adapters/output/filesystem/mcp_config_repository.py
================
"""Filesystem implementation of MCP configuration repository."""

import json
import logging
from pathlib import Path
from typing import Dict, List, Optional, Set

from paelladoc.ports.output.mcp_config_port import MCPConfigPort

logger = logging.getLogger(__name__)


class FileSystemMCPConfigRepository(MCPConfigPort):
    """Implementation of MCPConfigPort that reads from filesystem."""

    def __init__(self):
        """Initialize the repository."""
        self._cached_config: Optional[Dict] = None
        self.config_path = (
            Path(__file__).parent.parent.parent.parent / "config" / "mcp_config.json"
        )
        if not self.config_path.exists():
            logger.warning(f"MCP config file not found at: {self.config_path}")

    async def _load_config(self) -> Dict:
        """Load the MCP configuration from file."""
        if self._cached_config is not None:
            return self._cached_config

        try:
            with open(self.config_path, "r") as f:
                data = json.load(f)
                self._cached_config = data.get("mcp_configuration", {})
                return self._cached_config
        except Exception as e:
            logger.error(f"Error reading MCP config file {self.config_path}: {e}")
            return {}

    async def get_available_mcps(self) -> List[str]:
        """Returns a list of all available MCP names."""
        config = await self._load_config()
        return [
            name
            for name, mcp_config in config.items()
            if mcp_config.get("enabled", False)
        ]

    async def get_mcp_config(self, mcp_name: str) -> Optional[Dict]:
        """Returns the configuration for a specific MCP."""
        config = await self._load_config()
        return config.get(mcp_name)

    async def get_tools_for_taxonomy(self, taxonomy_bucket: str) -> List[str]:
        """Returns a list of MCP tools available for a given taxonomy bucket."""
        config = await self._load_config()
        tools: Set[str] = set()

        # For each enabled MCP
        for mcp_config in config.values():
            if not mcp_config.get("enabled", False):
                continue

            # If this taxonomy bucket is linked to this MCP
            if taxonomy_bucket in mcp_config.get("linked_taxonomies", []):
                # Add its tools to our set
                tools.update(mcp_config.get("tools", []))

        return sorted(list(tools))

    async def get_enabled_mcps_for_project(
        self, project_buckets: List[str]
    ) -> List[str]:
        """Returns a list of MCPs that should be enabled based on project buckets."""
        config = await self._load_config()
        enabled_mcps: Set[str] = set()

        # For each project bucket
        for bucket in project_buckets:
            # Check each MCP
            for mcp_name, mcp_config in config.items():
                if not mcp_config.get("enabled", False):
                    continue

                # If this bucket is linked to this MCP
                if bucket in mcp_config.get("linked_taxonomies", []):
                    enabled_mcps.add(mcp_name)

        return sorted(list(enabled_mcps))

================
File: paelladoc/adapters/output/filesystem/taxonomy_repository.py
================
import json
import logging
from pathlib import Path
from typing import Dict, List, Set, Optional
import importlib.resources

from paelladoc.ports.output.taxonomy_repository import TaxonomyRepository

logger = logging.getLogger(__name__)


class FileSystemTaxonomyRepository(TaxonomyRepository):
    """Implementation of TaxonomyRepository that reads from filesystem."""

    def __init__(self):
        """Initializes the repository."""
        self._cached_dimensions: Optional[List[str]] = None
        self._cached_dimension_values: Dict[str, List[str]] = {}
        self._cached_dimension_buckets: Dict[str, Dict[str, List[str]]] = {}
        self._cached_all_buckets: Optional[List[str]] = None
        self._cached_bucket_descriptions: Optional[Dict[str, str]] = None

        # Base taxonomy.json path for all buckets
        self.taxonomy_path = (
            Path(__file__).parent.parent.parent.parent.parent / "taxonomy.json"
        )
        if not self.taxonomy_path.exists():
            logger.warning(
                f"Main taxonomy.json file not found at: {self.taxonomy_path}"
            )

    def _get_base_taxonomy_path(self) -> Path:
        """Get the base path to the taxonomy directories within the package."""
        try:
            return Path(importlib.resources.files("paelladoc").joinpath("taxonomies"))
        except Exception as e:
            logger.error(f"Error locating taxonomy path: {e}")
            return Path(__file__).parent.parent.parent.parent / "taxonomies"

    def get_available_dimensions(self) -> List[str]:
        """Returns a list of available dimensions."""
        if self._cached_dimensions is not None:
            return self._cached_dimensions

        base_path = self._get_base_taxonomy_path()
        if not base_path.is_dir():
            logger.error(f"Taxonomy base path not found: {base_path}")
            return []

        dimensions = []
        for item in base_path.iterdir():
            if item.is_dir():
                dimensions.append(item.name)

        self._cached_dimensions = sorted(dimensions)
        return self._cached_dimensions

    def get_dimension_values(self, dimension: str) -> List[str]:
        """Returns a list of available values for a specific dimension."""
        if dimension in self._cached_dimension_values:
            return self._cached_dimension_values[dimension]

        base_path = self._get_base_taxonomy_path() / dimension
        if not base_path.is_dir():
            logger.error(f"Dimension directory not found: {base_path}")
            return []

        values = []
        for item in base_path.iterdir():
            if item.is_file() and item.suffix == ".json":
                values.append(item.stem)

        self._cached_dimension_values[dimension] = sorted(values)
        return self._cached_dimension_values[dimension]

    def get_buckets_for_dimension_value(self, dimension: str, value: str) -> List[str]:
        """Returns the list of taxonomy buckets associated with a specific dimension value."""
        # Check cache first
        if (
            dimension in self._cached_dimension_buckets
            and value in self._cached_dimension_buckets[dimension]
        ):
            return self._cached_dimension_buckets[dimension][value]

        # Initialize the dimension cache if needed
        if dimension not in self._cached_dimension_buckets:
            self._cached_dimension_buckets[dimension] = {}

        # Read from file
        file_path = self._get_base_taxonomy_path() / dimension / f"{value}.json"
        if not file_path.exists():
            logger.error(f"Dimension value file not found: {file_path}")
            return []

        try:
            with open(file_path, "r") as f:
                data = json.load(f)
                buckets = data.get("buckets", [])
                self._cached_dimension_buckets[dimension][value] = buckets
                return buckets
        except Exception as e:
            logger.error(f"Error reading dimension value file {file_path}: {e}")
            return []

    def get_buckets_for_project(
        self,
        platform: str,
        domain: str,
        size: str,
        lifecycle: str,
        compliance: Optional[str] = None,
    ) -> Set[str]:
        """Returns the combined set of taxonomy buckets for a project with the given dimensions."""
        all_buckets = set()

        # Add buckets from required dimensions
        all_buckets.update(self.get_buckets_for_dimension_value("platform", platform))
        all_buckets.update(self.get_buckets_for_dimension_value("domain", domain))
        all_buckets.update(self.get_buckets_for_dimension_value("size", size))
        all_buckets.update(self.get_buckets_for_dimension_value("lifecycle", lifecycle))

        # Add buckets from optional dimensions if provided
        if compliance:
            all_buckets.update(
                self.get_buckets_for_dimension_value("compliance", compliance)
            )

        return all_buckets

    def get_all_available_buckets(self) -> List[str]:
        """Returns a list of all available bucket IDs from the taxonomy."""
        if self._cached_all_buckets is not None:
            return self._cached_all_buckets

        try:
            with open(self.taxonomy_path, "r") as f:
                data = json.load(f)
                buckets = list(data.get("mece_taxonomy", {}).keys())
                self._cached_all_buckets = buckets
                return buckets
        except Exception as e:
            logger.error(f"Error reading main taxonomy file {self.taxonomy_path}: {e}")
            return []

    def get_bucket_description(self, bucket_id: str) -> Optional[str]:
        """Returns the description for a specific bucket ID, or None if not found."""
        if self._cached_bucket_descriptions is None:
            try:
                with open(self.taxonomy_path, "r") as f:
                    data = json.load(f)
                    self._cached_bucket_descriptions = {
                        bucket: info.get("description", "")
                        for bucket, info in data.get("mece_taxonomy", {}).items()
                    }
            except Exception as e:
                logger.error(
                    f"Error reading main taxonomy file {self.taxonomy_path}: {e}"
                )
                self._cached_bucket_descriptions = {}

        return self._cached_bucket_descriptions.get(bucket_id)

================
File: paelladoc/adapters/output/sqlite/configuration_adapter.py
================
"""SQLite adapter implementation for the ConfigurationPort."""

import logging
from datetime import datetime
from typing import Dict, Any, List, Optional
from sqlmodel import select
from sqlalchemy.ext.asyncio import AsyncSession

from paelladoc.ports.output.configuration_port import ConfigurationPort
from .config_models import (
    BehaviorConfigDB,
    MECEDimensionDB,
    TaxonomyValidationDB,
    BucketOrderDB,
    CommandDB,
)

logger = logging.getLogger(__name__)


class SQLiteConfigurationAdapter(ConfigurationPort):
    """SQLite implementation of the ConfigurationPort."""

    def __init__(self, session: AsyncSession):
        """Initialize the adapter with a database session.

        Args:
            session: SQLAlchemy async session for database operations.
        """
        self.session = session

    async def get_behavior_config(
        self, category: Optional[str] = None
    ) -> Dict[str, Any]:
        """Retrieves behavior configuration settings from the database.

        Args:
            category: Optional category to filter configurations.

        Returns:
            Dictionary of configuration key-value pairs.
        """
        try:
            statement = select(BehaviorConfigDB)
            if category:
                statement = statement.where(BehaviorConfigDB.category == category)

            results = await self.session.execute(statement)
            configs = results.scalars().all()

            return {config.key: config.value for config in configs}
        except Exception as e:
            logger.error(f"Error retrieving behavior config: {e}", exc_info=True)
            return {}

    async def get_mece_dimensions(self) -> Dict[str, Dict[str, Any]]:
        """Retrieves MECE dimension configurations from the database.

        Returns:
            Dictionary containing dimension configurations and validations.
        """
        try:
            # Get dimensions
            dimensions_stmt = select(MECEDimensionDB)
            dimensions_result = await self.session.execute(dimensions_stmt)
            dimensions = dimensions_result.scalars().all()

            # Get validations
            validations_stmt = select(TaxonomyValidationDB)
            validations_result = await self.session.execute(validations_stmt)
            validations = validations_result.scalars().all()

            # Structure the response
            allowed_dimensions = [dim.name for dim in dimensions]
            required_dimensions = [dim.name for dim in dimensions if dim.is_required]

            validation_rules = {
                val.platform: {
                    "domain": val.domain,
                    "warning": val.warning,
                    "severity": val.severity,
                }
                for val in validations
            }

            return {
                "allowed_dimensions": allowed_dimensions,
                "required_dimensions": required_dimensions,
                "validation_rules": validation_rules,
                "dimensions": {
                    dim.name: {
                        "required": dim.is_required,
                        "description": dim.description,
                        "rules": dim.validation_rules,
                    }
                    for dim in dimensions
                },
            }
        except Exception as e:
            logger.error(f"Error retrieving MECE dimensions: {e}", exc_info=True)
            return {
                "allowed_dimensions": [],
                "required_dimensions": [],
                "validation_rules": {},
                "dimensions": {},
            }

    async def get_bucket_order(self, category: Optional[str] = None) -> List[str]:
        """Retrieves the ordered list of buckets from the database.

        Args:
            category: Optional category to filter bucket types.

        Returns:
            List of bucket names in their defined order.
        """
        try:
            statement = select(BucketOrderDB).order_by(BucketOrderDB.order_index)
            if category:
                statement = statement.where(BucketOrderDB.category == category)

            results = await self.session.execute(statement)
            buckets = results.scalars().all()

            return [bucket.bucket_name for bucket in buckets]
        except Exception as e:
            logger.error(f"Error retrieving bucket order: {e}", exc_info=True)
            return []

    async def get_commands_metadata(self) -> Dict[str, Dict[str, Any]]:
        """Retrieves metadata about available commands from the database.

        Returns:
            Dictionary of command definitions including descriptions,
            parameters, and examples.
        """
        try:
            statement = select(CommandDB)
            results = await self.session.execute(statement)
            commands = results.scalars().all()

            return {
                cmd.name: {
                    "description": cmd.description,
                    "parameters": cmd.parameters,
                    "example": cmd.example,
                }
                for cmd in commands
            }
        except Exception as e:
            logger.error(f"Error retrieving commands metadata: {e}", exc_info=True)
            return {}

    async def save_behavior_config(
        self, config: Dict[str, Any], category: Optional[str] = None
    ) -> None:
        """Saves behavior configuration settings to the database.

        Args:
            config: Dictionary of configuration key-value pairs.
            category: Optional category for the configuration.
        """
        try:
            for key, value in config.items():
                # Check if config already exists
                statement = select(BehaviorConfigDB).where(
                    BehaviorConfigDB.key == key, BehaviorConfigDB.category == category
                )
                result = await self.session.execute(statement)
                existing_config = result.scalars().first()

                if existing_config:
                    # Update existing config
                    existing_config.value = value
                    existing_config.updated_at = datetime.utcnow()
                else:
                    # Create new config
                    new_config = BehaviorConfigDB(
                        key=key, value=value, category=category
                    )
                    self.session.add(new_config)

            await self.session.commit()
        except Exception as e:
            logger.error(f"Error saving behavior config: {e}", exc_info=True)
            await self.session.rollback()
            raise

================
File: paelladoc/adapters/output/sqlite/models.py
================
from typing import List, Optional
from sqlmodel import Field, Relationship, SQLModel, Column, JSON
import datetime

# Note: Domain Enums like DocumentStatus are not directly used here,
# we store their string representation (e.g., 'pending').
# The adapter layer will handle the conversion.

# --- Database Models ---

# Forward references are needed for relationships defined before the target model


class ProjectInfoDB(SQLModel, table=True):
    # Represents the metadata associated with a project memory entry
    id: Optional[int] = Field(default=None, primary_key=True)
    # name field is stored in ProjectMemoryDB as it's the primary identifier
    language: Optional[str] = None
    purpose: Optional[str] = None
    target_audience: Optional[str] = None
    objectives: Optional[List[str]] = Field(default=None, sa_column=Column(JSON))

    # Define the one-to-one relationship back to ProjectMemoryDB
    # Use Optional because a metadata row might briefly exist before being linked
    project_memory: Optional["ProjectMemoryDB"] = Relationship(
        back_populates="project_meta"
    )


class ProjectDocumentDB(SQLModel, table=True):
    # Represents a single document tracked within a project memory
    id: Optional[int] = Field(default=None, primary_key=True)
    name: str = Field(index=True)  # Name of the document file (e.g., "README.md")
    template_origin: Optional[str] = None
    status: str = Field(default="pending", index=True)  # Store enum string value

    # Foreign key to link back to the main project memory entry
    project_memory_id: Optional[int] = Field(
        default=None, foreign_key="projectmemorydb.id"
    )
    # Define the many-to-one relationship back to ProjectMemoryDB
    project_memory: Optional["ProjectMemoryDB"] = Relationship(
        back_populates="documents"
    )


class ProjectMemoryDB(SQLModel, table=True):
    # Represents the main project memory entry in the database
    id: Optional[int] = Field(default=None, primary_key=True)
    # Use project_name from metadata as the main unique identifier for lookups
    name: str = Field(
        index=True, unique=True
    )  # Changed from project_name to match domain model

    # New fields to match domain model
    base_path: str = Field(default="")  # Store as string, convert to Path in adapter
    interaction_language: str = Field(default="en-US")
    documentation_language: str = Field(default="en-US")
    taxonomy_version: str = Field(default="0.5")

    created_at: datetime.datetime = Field(default_factory=datetime.datetime.now)
    last_updated_at: datetime.datetime = Field(default_factory=datetime.datetime.now)

    # Foreign key to link to the associated metadata entry
    project_meta_id: Optional[int] = Field(
        default=None, foreign_key="projectmetadatadb.id", unique=True
    )
    # Define the one-to-one relationship to ProjectInfoDB
    project_meta: Optional[ProjectInfoDB] = Relationship(
        back_populates="project_memory"
    )

    # Define the one-to-many relationship to ProjectDocumentDB
    documents: List[ProjectDocumentDB] = Relationship(back_populates="project_memory")

================
File: paelladoc/adapters/output/sqlite/mapper.py
================
"""
Mapping functions between domain models and SQLite DB models.
"""

import logging
from typing import Dict, List, Optional
from pathlib import Path
import datetime
import uuid

# Domain Models
from paelladoc.domain.models.project import (
    ProjectMemory,
    ProjectInfo,
    ArtifactMeta,
    Bucket,  # Import if needed for mapping logic (e.g., default status)
)

# Database Models
from .db_models import ProjectMemoryDB, ArtifactMetaDB

logger = logging.getLogger(__name__)


def _ensure_utc(dt: Optional[datetime.datetime]) -> Optional[datetime.datetime]:
    """Ensures a datetime object is UTC, converting naive datetimes."""
    if dt is None:
        return None
    if dt.tzinfo is None:
        # Assume naive datetimes from DB are UTC, or handle conversion if needed
        return dt.replace(tzinfo=datetime.timezone.utc)
    return dt.astimezone(datetime.timezone.utc)


def map_db_to_domain(db_memory: ProjectMemoryDB) -> ProjectMemory:
    """Maps a ProjectMemoryDB instance to a ProjectMemory domain model."""
    # Map ProjectInfo
    domain_project_info = ProjectInfo(
        name=db_memory.name,
        language=db_memory.language,
        purpose=db_memory.purpose,
        target_audience=db_memory.target_audience,
        objectives=db_memory.objectives or [],
        base_path=Path(db_memory.base_path) if db_memory.base_path else None,
        interaction_language=db_memory.interaction_language,
        documentation_language=db_memory.documentation_language,
        taxonomy_version=db_memory.taxonomy_version,
        # Map taxonomy fields
        platform_taxonomy=db_memory.platform_taxonomy,
        domain_taxonomy=db_memory.domain_taxonomy,
        size_taxonomy=db_memory.size_taxonomy,
        compliance_taxonomy=db_memory.compliance_taxonomy,
        lifecycle_taxonomy=db_memory.lifecycle_taxonomy,
        custom_taxonomy=db_memory.custom_taxonomy or {},
        taxonomy_validation=db_memory.taxonomy_validation or {},
    )

    # Map Artifacts
    domain_artifacts: Dict[Bucket, List[ArtifactMeta]] = {}
    if db_memory.artifacts:
        for db_artifact in db_memory.artifacts:
            bucket = db_artifact.bucket
            if bucket not in domain_artifacts:
                domain_artifacts[bucket] = []
            domain_artifacts[bucket].append(
                ArtifactMeta(
                    id=db_artifact.id,
                    name=db_artifact.name,
                    bucket=bucket,
                    path=Path(db_artifact.path) if db_artifact.path else None,
                    created_at=_ensure_utc(db_artifact.created_at),
                    updated_at=_ensure_utc(db_artifact.updated_at),
                    created_by=db_artifact.created_by,
                    modified_by=db_artifact.modified_by,
                    status=db_artifact.status,
                )
            )

    # Create the domain model
    return ProjectMemory(
        project_info=domain_project_info,
        # Assuming the old 'documents' field is not needed or handled separately
        # documents={}, # If needed, map from a potential documents JSON field
        artifacts=domain_artifacts,
        taxonomy_version=db_memory.taxonomy_version,
        created_at=_ensure_utc(db_memory.created_at),
        last_updated_at=_ensure_utc(db_memory.last_updated_at),
        created_by=db_memory.created_by,
        modified_by=db_memory.modified_by,
        # Map taxonomy fields directly to ProjectMemory as well
        platform_taxonomy=db_memory.platform_taxonomy,
        domain_taxonomy=db_memory.domain_taxonomy,
        size_taxonomy=db_memory.size_taxonomy,
        compliance_taxonomy=db_memory.compliance_taxonomy,
        lifecycle_taxonomy=db_memory.lifecycle_taxonomy,
        custom_taxonomy=db_memory.custom_taxonomy or {},
        taxonomy_validation=db_memory.taxonomy_validation or {},
    )


def map_domain_artifact_to_db(
    domain_artifact: ArtifactMeta, project_memory_id: Optional[uuid.UUID] = None
) -> ArtifactMetaDB:
    """Maps a domain ArtifactMeta to a DB ArtifactMetaDB."""
    return ArtifactMetaDB(
        id=domain_artifact.id,
        project_memory_id=project_memory_id,
        name=domain_artifact.name,
        bucket=domain_artifact.bucket,
        path=str(domain_artifact.path),
        created_at=_ensure_utc(domain_artifact.created_at)
        if domain_artifact.created_at
        else datetime.datetime.now(datetime.timezone.utc),
        updated_at=_ensure_utc(domain_artifact.updated_at)
        if domain_artifact.updated_at
        else datetime.datetime.now(datetime.timezone.utc),
        created_by=domain_artifact.created_by,
        modified_by=domain_artifact.modified_by,
        status=domain_artifact.status,
    )


def map_domain_to_db(
    domain_memory: ProjectMemory, existing_db_model: Optional[ProjectMemoryDB] = None
) -> ProjectMemoryDB:
    """Maps a ProjectMemory domain model to a ProjectMemoryDB instance, excluding artifacts."""
    if existing_db_model:
        db_memory = existing_db_model
    else:
        db_memory = ProjectMemoryDB()
        # Set ID for new object if domain object has one (e.g., from deserialization)
        # Otherwise, rely on DB default_factory
        # db_memory.id = domain_memory.id # Assuming ProjectMemory doesn't have an ID field directly

    # Map fields from ProjectInfo
    if domain_memory.project_info:
        db_memory.name = domain_memory.project_info.name
        db_memory.language = domain_memory.project_info.language
        db_memory.purpose = domain_memory.project_info.purpose
        db_memory.target_audience = domain_memory.project_info.target_audience
        db_memory.objectives = domain_memory.project_info.objectives
        db_memory.base_path = (
            str(domain_memory.project_info.base_path)
            if domain_memory.project_info.base_path
            else None
        )
        db_memory.interaction_language = domain_memory.project_info.interaction_language
        db_memory.documentation_language = (
            domain_memory.project_info.documentation_language
        )
        db_memory.taxonomy_version = (
            domain_memory.taxonomy_version
        )  # Use ProjectMemory version
        db_memory.platform_taxonomy = domain_memory.project_info.platform_taxonomy
        db_memory.domain_taxonomy = domain_memory.project_info.domain_taxonomy
        db_memory.size_taxonomy = domain_memory.project_info.size_taxonomy
        db_memory.compliance_taxonomy = domain_memory.project_info.compliance_taxonomy
        db_memory.lifecycle_taxonomy = domain_memory.project_info.lifecycle_taxonomy
        db_memory.custom_taxonomy = domain_memory.project_info.custom_taxonomy
        db_memory.taxonomy_validation = domain_memory.project_info.taxonomy_validation

    # Map top-level fields from ProjectMemory (overwriting if needed)
    db_memory.taxonomy_version = domain_memory.taxonomy_version
    db_memory.created_at = (
        _ensure_utc(domain_memory.created_at)
        if domain_memory.created_at and not existing_db_model  # Only set on creation
        else (
            existing_db_model.created_at
            if existing_db_model
            else datetime.datetime.now(datetime.timezone.utc)
        )
    )
    db_memory.last_updated_at = datetime.datetime.now(datetime.timezone.utc)
    db_memory.created_by = domain_memory.created_by
    db_memory.modified_by = domain_memory.modified_by

    # Map taxonomy fields directly from ProjectMemory (ensure these overwrite ProjectInfo ones)
    db_memory.platform_taxonomy = domain_memory.platform_taxonomy
    db_memory.domain_taxonomy = domain_memory.domain_taxonomy
    db_memory.size_taxonomy = domain_memory.size_taxonomy
    db_memory.compliance_taxonomy = domain_memory.compliance_taxonomy
    db_memory.lifecycle_taxonomy = domain_memory.lifecycle_taxonomy
    db_memory.custom_taxonomy = domain_memory.custom_taxonomy
    db_memory.taxonomy_validation = domain_memory.taxonomy_validation

    # DO NOT map artifacts here, handled by sync_artifacts_db

    return db_memory


def sync_artifacts_db(
    session,  # Pass the SQLAlchemy session
    domain_memory: ProjectMemory,
    db_memory: ProjectMemoryDB,  # The DB object being saved (might be new or existing)
    existing_db_memory: Optional[
        ProjectMemoryDB
    ],  # The state loaded from DB (if exists)
) -> List[ArtifactMetaDB]:  # Return list of artifacts to delete
    """
    Synchronizes the ArtifactMetaDB entries based on the domain model's artifacts.
    Should be called within the adapter's session context after the ProjectMemoryDB
    object exists and has an ID.
    Returns a list of ArtifactMetaDB objects that should be deleted.
    """

    if not db_memory.id:
        logger.error("Cannot sync artifacts: ProjectMemoryDB object has no ID.")
        raise ValueError("ProjectMemoryDB must have an ID before syncing artifacts.")

    # Base the current DB state on the eager-loaded existing_db_memory if available
    if existing_db_memory and existing_db_memory.artifacts is not None:
        db_artifacts_map: Dict[uuid.UUID, ArtifactMetaDB] = {
            a.id: a for a in existing_db_memory.artifacts
        }
    else:
        db_artifacts_map: Dict[uuid.UUID, ArtifactMetaDB] = {}  # No existing artifacts

    domain_artifact_ids = set()
    artifacts_to_add = []
    artifacts_to_delete = []

    for bucket, domain_artifact_list in domain_memory.artifacts.items():
        for domain_artifact in domain_artifact_list:
            if not isinstance(domain_artifact, ArtifactMeta):
                logger.warning(
                    f"Skipping non-ArtifactMeta item found in domain artifacts: {domain_artifact}"
                )
                continue

            domain_artifact_ids.add(domain_artifact.id)
            db_artifact = db_artifacts_map.get(domain_artifact.id)

            if db_artifact:
                # Update existing artifact fields found in the map
                db_artifact.name = domain_artifact.name
                db_artifact.bucket = domain_artifact.bucket
                db_artifact.path = str(domain_artifact.path)
                db_artifact.status = domain_artifact.status
                db_artifact.updated_at = _ensure_utc(
                    domain_artifact.updated_at
                ) or datetime.datetime.now(datetime.timezone.utc)
                db_artifact.modified_by = domain_artifact.modified_by
                # IMPORTANT: Add the updated artifact to the session if its state changed
                # This might be implicitly handled by modifying the object while attached?
                # session.add(db_artifact) # Usually not needed for updates on attached objects
            else:
                # Artifact exists in domain but not in the loaded DB state -> Add it
                new_db_artifact = map_domain_artifact_to_db(
                    domain_artifact, project_memory_id=db_memory.id
                )
                artifacts_to_add.append(new_db_artifact)

    # Identify artifacts to delete (present in DB map but not in domain IDs)
    for db_artifact_id, db_artifact in db_artifacts_map.items():
        if db_artifact_id not in domain_artifact_ids:
            artifacts_to_delete.append(db_artifact)

    # Add new artifacts to the session
    if artifacts_to_add:
        session.add_all(artifacts_to_add)
        logger.debug(
            f"Adding {len(artifacts_to_add)} new artifacts to session for project {db_memory.name}."
        )

    # Return artifacts to be deleted by the caller (adapter)
    return artifacts_to_delete

================
File: paelladoc/adapters/output/sqlite/config_models.py
================
"""SQLModel models for configuration tables."""

from datetime import datetime
from typing import Optional, Dict, Any, List
from sqlmodel import SQLModel, Field
from sqlalchemy import Column, JSON


class BehaviorConfigDB(SQLModel, table=True):
    """Database model for behavior configurations."""

    __tablename__ = "behavior_configs"

    id: Optional[int] = Field(default=None, primary_key=True)
    key: str = Field(index=True)
    value: Dict[str, Any] = Field(sa_column=Column(JSON))
    description: Optional[str] = None
    category: Optional[str] = Field(default=None, index=True)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)


class MECEDimensionDB(SQLModel, table=True):
    """Database model for MECE dimensions."""

    __tablename__ = "mece_dimensions"

    id: Optional[int] = Field(default=None, primary_key=True)
    name: str = Field(index=True)
    is_required: bool = Field(default=False)
    description: Optional[str] = None
    validation_rules: Optional[Dict[str, Any]] = Field(
        default=None, sa_column=Column(JSON)
    )
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)


class TaxonomyValidationDB(SQLModel, table=True):
    """Database model for taxonomy validations."""

    __tablename__ = "taxonomy_validations"

    id: Optional[int] = Field(default=None, primary_key=True)
    platform: Optional[str] = Field(default=None, index=True)
    domain: Optional[str] = Field(default=None, index=True)
    warning: str
    severity: str = Field(index=True)
    created_at: datetime = Field(default_factory=datetime.utcnow)


class BucketOrderDB(SQLModel, table=True):
    """Database model for bucket ordering."""

    __tablename__ = "bucket_order"

    id: Optional[int] = Field(default=None, primary_key=True)
    bucket_name: str = Field(index=True)
    order_index: int
    category: Optional[str] = Field(default=None, index=True)
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)


class CommandDB(SQLModel, table=True):
    """Database model for command metadata."""

    __tablename__ = "commands"

    id: Optional[int] = Field(default=None, primary_key=True)
    name: str = Field(index=True)
    description: str
    parameters: Optional[List[Dict[str, Any]]] = Field(
        default=None, sa_column=Column(JSON)
    )
    example: Optional[str] = None
    created_at: datetime = Field(default_factory=datetime.utcnow)
    updated_at: datetime = Field(default_factory=datetime.utcnow)

================
File: paelladoc/adapters/output/sqlite/sqlite_memory_adapter.py
================
"""SQLite adapter for project memory persistence."""

import logging
from typing import Optional, List, Dict, Any
from pathlib import Path
import subprocess
import os
from sqlmodel import SQLModel, select
from sqlalchemy.ext.asyncio import create_async_engine, AsyncSession
from sqlalchemy.orm import sessionmaker, selectinload
from sqlalchemy.exc import IntegrityError
from sqlalchemy import update, Row

# Ports and Domain Models
from paelladoc.ports.output.memory_port import MemoryPort
from paelladoc.domain.models.project import (
    ProjectMemory,
    ProjectInfo,
)

# Database Models for this adapter
from .db_models import ProjectMemoryDB, UserDB

# Import UserDB model from the correct location

# Import the new mapper functions
from .mapper import map_db_to_domain, map_domain_to_db, sync_artifacts_db

# Configuration
from paelladoc.config.database import get_db_path

# Dependency Injection for User Management Port
# from paelladoc.dependencies import dependencies # Assuming dict-based DI
from paelladoc.ports.output.user_management_port import UserManagementPort

# Default database path (obtained via config logic)
# DEFAULT_DB_PATH = get_db_path() # No longer needed as constant? __init__ uses get_db_path()

logger = logging.getLogger(__name__)

# Remove redundant/fragile PROJECT_ROOT calculation
# PROJECT_ROOT = Path(__file__).parent.parent.parent.parent.parent
# logger.info(f"Project root calculated as: {PROJECT_ROOT.resolve()}")
# DEFAULT_DB_PATH = PROJECT_ROOT / "paelladoc_memory.db"
# logger.info(f"Default database path set to: {DEFAULT_DB_PATH.resolve()}")


class SQLiteMemoryAdapter(MemoryPort):
    """SQLite implementation of the MemoryPort using new MECE/Artifact models."""

    # Keep __init__ from HEAD (using get_db_path)
    def __init__(self, db_path: str | Path | None = None):
        """
        Initialize the SQLite adapter.

        Args:
            db_path: Optional custom database path. If not provided, uses the configured default.
        """
        self.db_path = Path(db_path) if db_path else get_db_path()
        logger.info(
            f"Initializing SQLite adapter with database path: {self.db_path.resolve()}"
        )

        # Ensure the parent directory exists
        self.db_path.parent.mkdir(parents=True, exist_ok=True)

        # Create async engine
        self.async_engine = create_async_engine(
            f"sqlite+aiosqlite:///{self.db_path}",
            echo=False,  # Set to True for SQL query logging
            connect_args={"check_same_thread": False},  # Necessary for SQLite async
        )

        # Create async session factory (named async_session)
        self.async_session = sessionmaker(
            self.async_engine, class_=AsyncSession, expire_on_commit=False
        )
        logger.info("SQLiteMemoryAdapter initialized.")

    async def _run_migrations(self):
        """Run database migrations using alembic."""
        try:
            logger.info("Running database migrations...")
            # Get the project root directory where alembic.ini is located
            project_root = Path(__file__).parent.parent.parent.parent.parent.absolute()

            # Set PAELLADOC_DB_PATH environment variable for alembic
            os.environ["PAELLADOC_DB_PATH"] = str(self.db_path)

            # Run alembic upgrade
            result = subprocess.run(
                ["alembic", "upgrade", "head"],
                capture_output=True,
                text=True,
                check=False,
                cwd=project_root,  # Run from project root where alembic.ini is
            )

            if result.returncode == 0:
                logger.info("Database migrations completed successfully")
                if result.stdout:
                    logger.debug(f"Migration output:\n{result.stdout}")
            else:
                logger.error(f"Database migration failed with error:\n{result.stderr}")
                # Instead of raising an error, just create tables directly in test environments
                if "test" in str(self.db_path):
                    logger.warning(
                        "Test environment detected, creating tables directly..."
                    )
                    async with self.async_engine.begin() as conn:
                        await conn.run_sync(SQLModel.metadata.create_all)
                    return
                raise RuntimeError("Database migration failed")

        except Exception as e:
            logger.error(f"Error running migrations: {e}")
            raise

    async def _create_db_and_tables(self):
        """Creates the database and tables if they don't exist and ensures initial admin user."""
        # Wrap the entire process to catch and log any unexpected errors
        try:
            # First run migrations - TEMPORARILY COMMENTED OUT due to subprocess issues
            # logger.info("Attempting to run migrations via adapter...") # Add log
            # await self._run_migrations()
            # logger.info("Migrations finished (or skipped if failed/not needed).") # Add log

            # Ensure all tables exist using SQLModel metadata
            # This is safe even if migrations didn't run, but relies on external migration for schema updates.
            logger.info(
                "Ensuring tables exist via SQLModel.metadata.create_all..."
            )  # Add log
            async with self.async_engine.begin() as conn:
                await conn.run_sync(SQLModel.metadata.create_all)
            logger.info("Database tables checked/created via SQLModel.")

            # --- Add initial OSS admin user ---
            # Get the potentially patched dependency HERE
            from paelladoc.dependencies import dependencies
            user_port_instance = dependencies.get(UserManagementPort)
            await self._ensure_initial_admin_user(user_port_instance) # Pasar la instancia
            # --- End add initial OSS admin user ---

        except Exception as e:
            # Log the full traceback if anything goes wrong here
            logger.error(
                f"Critical error during database/table creation or initial user setup: {e}",
                exc_info=True,
            )
            # Re-raise the exception to allow higher-level handlers to catch it if needed
            raise

    async def _ensure_initial_admin_user(self, user_management_port: UserManagementPort):
        """Checks if any user exists and creates the initial admin user if none are found."""
        # Wrap the core logic to log detailed errors
        try:
            logger.debug("Attempting to use provided UserManagementPort...")
            if not user_management_port:
                logger.error(
                    "UserManagementPort instance was not provided. Cannot create initial admin user."
                )
                return
            logger.debug("UserManagementPort obtained via argument.")

            # Check if any user exists directly via the port
            logger.debug("Checking if any user exists via port...")
            user_exists = await user_management_port.check_if_any_user_exists()
            logger.debug(f"Result from check_if_any_user_exists: {user_exists}")

            if not user_exists:
                admin_user_identifier = os.getenv(
                    "PAELLADOC_OSS_ADMIN_USER", "admin@paelladoc.default"
                )
                logger.info(
                    f"No users found. Attempting to create initial admin user: {admin_user_identifier}"
                )
                try:
                    # The create_user method should handle hashing and saving
                    await user_management_port.create_user(admin_user_identifier)
                    logger.info(
                        f"Successfully created initial admin user: {admin_user_identifier}"
                    )
                except Exception as create_err:
                    # Log the specific error during user creation
                    logger.error(
                        f"Failed to create initial admin user '{admin_user_identifier}': {create_err}",
                        exc_info=True,
                    )
                    # Decide if this should raise an error or just log - re-raising for now
                    raise create_err
            else:
                logger.debug(
                    "Users already exist in the database. Skipping initial admin user creation."
                )

        except Exception as e:
            # Log the full traceback if any error occurs during the process
            logger.error(f"Error ensuring initial admin user: {e}", exc_info=True)
            # Re-raise the exception
            raise

    # --- MemoryPort Implementation --- #

    async def save_memory(self, memory: ProjectMemory) -> None:
        """Saves the project memory state (including artifacts) to SQLite using the mapper."""
        project_name = memory.project_info.name
        logger.debug(f"Attempting to save memory for project: {project_name}")
        await self._create_db_and_tables()

        # Import dependencies dictionary *inside* the method to break circular import (already done)
        from paelladoc.dependencies import dependencies

        # Get User Management Port from dependencies
        user_management_port: UserManagementPort = dependencies.get(UserManagementPort)
        if not user_management_port:
            # Fallback or error handling if port is not registered
            logger.warning(
                "UserManagementPort not found in dependencies. Cannot set created_by/modified_by."
            )
            current_user_id = None
        else:
            current_user_id = await user_management_port.get_current_user_id()
            logger.debug(f"Retrieved current user ID: {current_user_id}")

        async with self.async_session() as session:
            async with (
                session.begin()
            ):  # Use session.begin() for transaction management
                try:
                    # Try to load existing DB object WITH artifacts
                    statement = (
                        select(ProjectMemoryDB)
                        .where(ProjectMemoryDB.name == project_name)
                        .options(selectinload(ProjectMemoryDB.artifacts))
                    )
                    results = await session.execute(statement)
                    existing_db_memory = results.scalars().first()

                    # Set created_by only if it's a new project
                    if not existing_db_memory and current_user_id:
                        memory.project_info.created_by = current_user_id

                    # Always set modified_by
                    if current_user_id:
                        memory.project_info.modified_by = current_user_id

                    # Use mapper to map domain object to DB object (create or update fields)
                    db_memory = map_domain_to_db(memory, existing_db_memory)

                    # Ensure DB model also gets the updated user IDs
                    if not existing_db_memory and current_user_id:
                        db_memory.created_by = current_user_id
                    if current_user_id:
                        db_memory.modified_by = current_user_id

                    # Add the main object to the session (SQLModel handles INSERT or UPDATE)
                    # If existing_db_memory is None, this adds a new object.
                    # If existing_db_memory is not None, this adds the *updated* existing object back.
                    session.add(db_memory)

                    # Flush to get the ID if it's a new project before syncing artifacts
                    if not existing_db_memory:
                        await session.flush()
                        logger.debug(
                            f"Flushed new project {db_memory.name} with ID {db_memory.id}"
                        )
                    elif db_memory.id is None:
                        # Should not happen if existing_db_memory was found and mapped correctly
                        await session.flush()
                        logger.warning(
                            f"Flushed existing project {db_memory.name} which unexpectedly had no ID before flush."
                        )
                    # We need db_memory.id for syncing artifacts below
                    if db_memory.id is None:
                        # If ID is still None after potential flush, something is wrong
                        raise RuntimeError(
                            f"Could not obtain ID for project {project_name} before syncing artifacts."
                        )

                    # Sync artifacts using the dedicated function, passing the loaded state
                    artifacts_to_delete = sync_artifacts_db(
                        session, memory, db_memory, existing_db_memory
                    )

                    # Perform deletions
                    if artifacts_to_delete:
                        logger.debug(
                            f"Deleting {len(artifacts_to_delete)} artifacts from session for project {project_name}"
                        )
                        for artifact_to_del in artifacts_to_delete:
                            await session.delete(artifact_to_del)

                    # Commit is handled by session.begin() context manager
                    logger.info(
                        f"Successfully saved memory for project: {project_name}"
                    )

                except IntegrityError as e:
                    logger.error(
                        f"Integrity error saving project '{project_name}': {e}",
                        exc_info=True,
                    )
                    raise ValueError(
                        f"Project '{project_name}' might already exist or another integrity issue occurred."
                    ) from e
                except Exception as e:
                    logger.error(
                        f"Unexpected error saving project '{project_name}': {e}",
                        exc_info=True,
                    )
                    raise

    async def load_memory(self, project_name: str) -> Optional[ProjectMemory]:
        """Loads project memory (including artifacts) from SQLite using the mapper."""
        logger.debug(f"Attempting to load memory for project: {project_name}")
        await self._create_db_and_tables()

        async with self.async_session() as session:
            try:
                statement = (
                    select(ProjectMemoryDB)
                    .where(ProjectMemoryDB.name == project_name)
                    .options(
                        selectinload(ProjectMemoryDB.artifacts)
                    )  # Eager load artifacts
                )
                results = await session.execute(statement)
                db_memory = results.scalars().first()

                if db_memory:
                    logger.debug(
                        f"Found project '{project_name}' in DB, mapping to domain model."
                    )
                    # Use the mapper function
                    return map_db_to_domain(db_memory)
                else:
                    logger.debug(f"Project '{project_name}' not found in DB.")
                    return None
            except Exception as e:
                logger.error(
                    f"Error loading project '{project_name}': {e}", exc_info=True
                )
                # Optional: Re-raise a custom domain exception?
                return None  # Return None on error for now

    async def project_exists(self, project_name: str) -> bool:
        """Checks if a project memory exists in the SQLite database."""
        logger.debug(f"Checking existence for project: {project_name}")
        await self._create_db_and_tables()

        async with self.async_session() as session:
            try:
                statement = select(ProjectMemoryDB.id).where(
                    ProjectMemoryDB.name == project_name
                )
                results = await session.execute(statement)
                exists = results.scalars().first() is not None
                logger.debug(f"Project '{project_name}' exists: {exists}")
                return exists
            except Exception as e:
                logger.error(
                    f"Error checking project existence for '{project_name}': {e}",
                    exc_info=True,
                )
                return False

    async def list_projects(self) -> List[Dict[str, Any]]:
        """Lists info for all projects stored in the database, including active status."""
        logger.debug("Listing all projects info from database.")
        await self._create_db_and_tables()

        projects_data: List[Dict[str, Any]] = []
        async with self.async_session() as session:
            try:
                # Select necessary columns INCLUDING is_active
                statement = select(
                    ProjectMemoryDB.name,
                    ProjectMemoryDB.is_active,
                    ProjectMemoryDB.language,
                    ProjectMemoryDB.purpose,
                    ProjectMemoryDB.target_audience,
                    ProjectMemoryDB.objectives,
                    ProjectMemoryDB.base_path,
                    ProjectMemoryDB.interaction_language,
                    ProjectMemoryDB.documentation_language,
                    ProjectMemoryDB.taxonomy_version,
                    ProjectMemoryDB.platform_taxonomy,
                    ProjectMemoryDB.domain_taxonomy,
                    ProjectMemoryDB.size_taxonomy,
                    ProjectMemoryDB.compliance_taxonomy,
                    ProjectMemoryDB.lifecycle_taxonomy,
                    ProjectMemoryDB.custom_taxonomy,
                    ProjectMemoryDB.taxonomy_validation,
                    # Add audit fields if needed by the list view
                    ProjectMemoryDB.created_at,
                    ProjectMemoryDB.created_by,
                    ProjectMemoryDB.last_updated_at,
                    ProjectMemoryDB.modified_by,
                )
                results = await session.execute(statement)
                rows: List[Row] = results.all()

                for row in rows:
                    # Map row to dictionary
                    try:
                        # Use ._mapping to access row data as a dictionary
                        row_dict = row._mapping
                        # Create the dictionary, converting Path if necessary
                        project_dict = {
                            key: (str(value) if isinstance(value, Path) else value)
                            for key, value in row_dict.items()
                        }
                        # Ensure base_path is string even if None initially (though should be str from DB)
                        if (
                            "base_path" in project_dict
                            and project_dict["base_path"] is not None
                        ):
                            project_dict["base_path"] = str(
                                Path(project_dict["base_path"])
                            )  # Ensure it's string path
                        elif "base_path" in project_dict:
                            project_dict["base_path"] = (
                                None  # Handle potential None explicitly
                            )

                        # Clean up None objectives to empty list if needed by consumers
                        if (
                            "objectives" in project_dict
                            and project_dict["objectives"] is None
                        ):
                            project_dict["objectives"] = []

                        # Ensure taxonomies are dicts if None
                        if (
                            "custom_taxonomy" in project_dict
                            and project_dict["custom_taxonomy"] is None
                        ):
                            project_dict["custom_taxonomy"] = {}
                        if (
                            "taxonomy_validation" in project_dict
                            and project_dict["taxonomy_validation"] is None
                        ):
                            project_dict["taxonomy_validation"] = {}

                        projects_data.append(project_dict)

                    except Exception as map_error:  # Catch mapping errors
                        logger.error(
                            f"Error mapping project dict for '{getattr(row, 'name', 'UNKNOWN')}': {map_error}",
                            exc_info=True,
                        )
                        continue  # Skip projects that fail mapping

                logger.debug(f"Found {len(projects_data)} projects.")
                return projects_data
            except Exception as e:
                logger.error(f"Error listing projects: {e}", exc_info=True)
                return []  # Return empty list on error

    # list_projects_names removed as list_projects now returns ProjectInfo

    async def delete_memory(self, project_name: str) -> None:
        """Delete a project memory and its artifacts from the database.

        Args:
            project_name: Name of the project to delete.

        Raises:
            ValueError: If project doesn't exist.
        """
        logger.debug(f"Attempting to delete project: {project_name}")
        await self._create_db_and_tables()

        async with self.async_session() as session:
            async with session.begin():
                try:
                    # Find the project
                    statement = (
                        select(ProjectMemoryDB)
                        .where(ProjectMemoryDB.name == project_name)
                        .options(selectinload(ProjectMemoryDB.artifacts))
                    )
                    results = await session.execute(statement)
                    project_db = results.scalars().first()

                    if not project_db:
                        raise ValueError(f"Project '{project_name}' not found.")

                    # Delete all artifacts first (due to foreign key constraint)
                    if project_db.artifacts:
                        for artifact in project_db.artifacts:
                            await session.delete(artifact)

                    # Delete the project
                    await session.delete(project_db)

                    logger.info(f"Successfully deleted project: {project_name}")

                except Exception as e:
                    logger.error(
                        f"Error deleting project '{project_name}': {e}", exc_info=True
                    )
                    raise

    async def get_active_project(self) -> Optional[ProjectInfo]:
        """Gets the currently active project, if any."""
        await self._create_db_and_tables()  # Ensure DB exists
        async with self.async_session() as session:
            statement = select(ProjectMemoryDB).where(ProjectMemoryDB.is_active)
            results = await session.execute(statement)
            active_db_project = results.scalars().first()
            if active_db_project:
                # Convert only the necessary fields to ProjectInfo
                # This avoids loading the full ProjectMemory
                return ProjectInfo(
                    name=active_db_project.name,
                    language=active_db_project.language,
                    purpose=active_db_project.purpose,
                    target_audience=active_db_project.target_audience,
                    base_path=active_db_project.base_path,
                    interaction_language=active_db_project.interaction_language,
                    documentation_language=active_db_project.documentation_language,
                    platform_taxonomy=active_db_project.platform_taxonomy,
                    domain_taxonomy=active_db_project.domain_taxonomy,
                    size_taxonomy=active_db_project.size_taxonomy,
                    compliance_taxonomy=active_db_project.compliance_taxonomy,
                    lifecycle_taxonomy=active_db_project.lifecycle_taxonomy,
                )
            return None

    async def set_active_project(self, project_name: str) -> bool:
        """Sets the specified project as active, deactivating others. Returns True on success."""
        logger.info(f"Attempting to set project '{project_name}' as active.")
        await self._create_db_and_tables()  # Ensure DB exists
        async with self.async_session() as session:
            async with session.begin():  # Start transaction
                try:
                    # Step 1: Deactivate all currently active projects
                    update_stmt_deactivate = (
                        update(ProjectMemoryDB)
                        .where(ProjectMemoryDB.is_active)
                        .values(is_active=False)
                    )
                    await session.execute(update_stmt_deactivate)
                    logger.debug("Deactivated previously active projects.")

                    # Step 2: Activate the target project
                    update_stmt_activate = (
                        update(ProjectMemoryDB)
                        .where(ProjectMemoryDB.name == project_name)
                        .values(is_active=True)
                        .returning(ProjectMemoryDB.id)  # Check if update happened
                    )
                    result = await session.execute(update_stmt_activate)

                    if result.scalar_one_or_none() is None:
                        logger.warning(
                            f"Project '{project_name}' not found to activate. Rolling back."
                        )
                        # Transaction will be rolled back automatically by context manager
                        return False

                    logger.info(f"Successfully set project '{project_name}' as active.")
                    # Transaction commits automatically here
                    return True

                except Exception as e:
                    logger.error(
                        f"Error setting active project '{project_name}': {e}",
                        exc_info=True,
                    )
                    # Transaction will be rolled back automatically
                    return False

    # Remove ensure_utc helper method from the adapter (should be in mapper)
    # def ensure_utc(self, dt: datetime.datetime) -> datetime.datetime:
    #     ...

================
File: paelladoc/adapters/output/sqlite/db_models.py
================
from typing import List, Optional
from uuid import UUID, uuid4
from datetime import datetime, timezone
from pathlib import Path

from sqlmodel import Field, Relationship, SQLModel, Column
from sqlalchemy.sql.sqltypes import JSON
from sqlalchemy.schema import Index  # Import Index
from sqlalchemy.sql import text  # Import text for WHERE clause

from paelladoc.domain.models.project import (
    Bucket,
    DocumentStatus,
)  # Import enums from domain

# --- Knowledge Graph Documentation ---
"""
Knowledge Graph (KG) Ready Model Design

This file defines SQLModel models with relationships specifically designed to be
KG-compatible. Each relationship defined here (through foreign keys) represents a 
potential edge in a knowledge graph.

Primary Nodes:
- ProjectMemoryDB: Represents a project (central node)
- ArtifactMetaDB: Represents documentation artifacts
- TaxonomyDB: Represents MECE taxonomy selections

Edge Types (Relationships):
1. HAS_ARTIFACT: ProjectMemoryDB -> ArtifactMetaDB
   - Direction: Project contains artifacts
   - Properties: None (simple containment)
   - FK: ArtifactMetaDB.project_memory_id -> ProjectMemoryDB.id

2. HAS_TAXONOMY: ProjectMemoryDB -> TaxonomyDB
   - Direction: Project uses taxonomy combinations
   - Properties: Selected categories
   - Validates MECE structure

3. IMPLEMENTS: ArtifactMetaDB -> TaxonomyDB
   - Direction: Artifact implements taxonomy requirements
   - Properties: Coverage metrics

Future Potential Edges:
1. DEPENDS_ON: ArtifactMetaDB -> ArtifactMetaDB
   - Would represent dependencies between artifacts
   - Need to add a dependencies table or attribute

2. CREATED_BY: ArtifactMetaDB -> User
   - Connects artifacts to creators
   - Already tracking created_by/modified_by fields

Query Patterns:
- Find all artifacts for a project: ProjectMemoryDB -[HAS_ARTIFACT]-> ArtifactMetaDB
- Find taxonomy coverage: ProjectMemoryDB -[HAS_TAXONOMY]-> TaxonomyDB
- Validate MECE structure: TaxonomyDB -[IMPLEMENTS]-> ArtifactMetaDB

MECE Structure Support:
- Platform taxonomies (web, mobile, desktop, extensions)
- Domain taxonomies (infra, tools, data/AI, business)
- Size taxonomies (personal to enterprise)
- Compliance taxonomies (GDPR, HIPAA, PCI)
"""

# --- Artifact Model ---


class ArtifactMetaDB(SQLModel, table=True):
    """Database model for ArtifactMeta"""

    __table_args__ = {"extend_existing": True}

    # Use the domain UUID as the primary key
    id: UUID = Field(default_factory=uuid4, primary_key=True, index=True)

    # KG Edge: HAS_ARTIFACT (ProjectMemoryDB -> ArtifactMetaDB)
    # This foreign key creates a directional relationship from Project to Artifact
    project_memory_id: UUID = Field(foreign_key="projectmemorydb.id", index=True)

    name: str = Field(index=True)
    bucket: Bucket = Field(index=True)  # Store enum value directly
    path: str = Field(index=True)  # Store Path as string
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    updated_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

    # KG Node Properties for actor/authorship tracking
    # These fields can be used to create CREATED_BY and MODIFIED_BY edges in a KG
    created_by: Optional[str] = Field(default=None, index=True)
    modified_by: Optional[str] = Field(default=None, index=True)

    status: DocumentStatus = Field(index=True)  # Store enum value directly

    # Define the relationship back to ProjectMemoryDB
    # This defines the reverse navigation for the HAS_ARTIFACT relationship
    project_memory: "ProjectMemoryDB" = Relationship(back_populates="artifacts")

    # KG-Ready: Store Path as string for easier querying/linking
    def __init__(self, *, path: Path, **kwargs):
        super().__init__(path=str(path), **kwargs)

    @property
    def path_obj(self) -> Path:
        return Path(self.path)


# --- Project Memory Model ---


class ProjectMemoryDB(SQLModel, table=True):
    """Project memory database model."""

    # Use a separate UUID for the DB primary key, keep metadata name unique?
    # Or use metadata.name as PK? For now, using UUID.
    id: UUID = Field(default_factory=uuid4, primary_key=True, index=True)
    name: str = Field(unique=True, index=True)  # From metadata.name
    is_active: bool = Field(default=False, index=True)  # Track active project
    language: Optional[str] = Field(default=None)
    purpose: Optional[str] = Field(default=None)
    target_audience: Optional[str] = Field(default=None)
    objectives: Optional[List[str]] = Field(
        sa_column=Column(JSON), default=None
    )  # Store list as JSON
    base_path: Optional[str] = Field(
        default=None
    )  # Store as string representation of Path
    interaction_language: Optional[str] = Field(default=None)
    documentation_language: Optional[str] = Field(default=None)
    taxonomy_version: str
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))
    last_updated_at: datetime = Field(
        default_factory=lambda: datetime.now(timezone.utc)
    )

    # KG Node Properties for actor/authorship tracking
    created_by: Optional[str] = Field(default=None, index=True)
    modified_by: Optional[str] = Field(default=None, index=True)

    # MECE Taxonomy Configuration
    platform_taxonomy: Optional[str] = Field(index=True)  # Selected platform taxonomy
    domain_taxonomy: Optional[str] = Field(index=True)  # Selected domain taxonomy
    size_taxonomy: Optional[str] = Field(index=True)  # Selected size taxonomy
    compliance_taxonomy: Optional[str] = Field(
        index=True
    )  # Selected compliance taxonomy
    lifecycle_taxonomy: Optional[str] = Field(index=True)  # Added lifecycle taxonomy

    # Custom taxonomy configuration for this project
    custom_taxonomy: Optional[dict] = Field(
        sa_column=Column(JSON), default=None
    )  # Store as JSON object

    # MECE validation status
    taxonomy_validation: Optional[dict] = Field(
        sa_column=Column(JSON), default=None
    )  # Store validation results

    # Define the one-to-many relationship to ArtifactMetaDB
    # KG Edge: HAS_ARTIFACT (ProjectMemoryDB -> ArtifactMetaDB)
    # artifacts will be loaded automatically by SQLModel/SQLAlchemy when accessed
    artifacts: List["ArtifactMetaDB"] = Relationship(back_populates="project_memory")

    # TODO: Decide how to handle the old 'documents' field if migration is needed.
    # Could be another JSON field temporarily or migrated into ArtifactMetaDB.
    # For now, omitting it, assuming new structure only or migration handles it.

    # Remove the Meta class definition here if it only contained __table_args__ for the index
    # class Meta:
    #     __table_args__ = (
    #         # Add a partial unique index on is_active when it's true
    #         # This ensures only one project can be active at a time
    #         UniqueConstraint('is_active', name='uix_active_project',
    #                         sqlite_where="is_active = 1"),
    #     )
    # Define __table_args__ directly in the class body
    __table_args__ = (
        # Add a partial unique index on is_active when it's true using Index
        Index(
            "uix_active_project",
            "is_active",
            unique=True,
            sqlite_where=text("is_active = 1"),
        ),
    )


# --- User Model (Minimal for OSS) --- #


class UserDB(SQLModel, table=True):
    """Minimal user model for OSS version, primarily for tracking authorship."""

    # Allow extending the existing table definition during test collection
    __table_args__ = {"extend_existing": True}

    id: Optional[int] = Field(default=None, primary_key=True)
    user_identifier: str = Field(unique=True, index=True)
    created_at: datetime = Field(default_factory=lambda: datetime.now(timezone.utc))

    # In SaaS, this model would be significantly more complex, potentially including
    # relationships to roles, tenants, etc. For OSS, we just need a unique ID
    # to link to created_by/modified_by fields.


# --- Configuration Models --- #

================
File: paelladoc/adapters/output/sqlite/sqlite_user_management_adapter.py
================
"""SQLite implementation of the UserManagementPort."""

import logging
from typing import Optional, Any, List

from sqlmodel import select, delete, func
from sqlalchemy.ext.asyncio import AsyncSession
from sqlalchemy.orm import sessionmaker
from sqlalchemy.exc import IntegrityError, NoResultFound

from paelladoc.ports.output.user_management_port import UserManagementPort
from paelladoc.domain.models.user import User  # User for return type
from .db_models import UserDB  # UserDB for queries

logger = logging.getLogger(__name__)


class SQLiteUserManagementAdapter(UserManagementPort):
    """
    SQLite implementation of the UserManagementPort using SQLModel.
    Handles user creation, retrieval, deletion, and permission checks based on UserDB.
    """

    def __init__(self, async_session_factory: sessionmaker[AsyncSession]):
        """Initialize the adapter with an async session factory."""
        self.async_session = async_session_factory
        logger.info("SQLiteUserManagementAdapter initialized.")

    async def get_current_user_id(self) -> Optional[str]:
        """
        Retrieves the user identifier from the single entry in the UserDB table (OSS assumption).
        Returns None if the table is empty or has more than one entry (configuration error).
        """
        async with self.async_session() as session:
            try:
                statement = select(UserDB)
                results = await session.execute(statement)
                users = results.scalars().all()

                if len(users) == 1:
                    user_id = users[0].user_identifier
                    # logger.debug(f"SQLiteUserManagementAdapter: Found user ID: {user_id}") # Too verbose?
                    return user_id
                elif len(users) == 0:
                    logger.warning(
                        "SQLiteUserManagementAdapter: No user found in UserDB. OSS user not configured."
                    )
                    return None
                else:
                    # More than one user means configuration is incorrect for OSS
                    logger.error(
                        "SQLiteUserManagementAdapter: Found multiple users in UserDB for OSS mode. Configuration error."
                    )
                    # Consider returning the first one? Or None? Returning None is safer.
                    return None
            except Exception as e:
                logger.error(
                    f"SQLiteUserManagementAdapter: Error fetching user: {e}",
                    exc_info=True,
                )
                return None

    async def check_permission(
        self, user_id: Optional[str], action: str, resource_id: Optional[Any] = None
    ) -> bool:
        """
        In OSS mode, returns True only if a valid user_id (not None) is provided.
        This ensures operations are blocked until the OSS user is configured.
        A real implementation would check roles/permissions against the action/resource.
        """
        # Simplified OSS check: Any logged-in user can do anything.
        has_permission = user_id is not None
        # logger.debug(
        #     f"SQLiteUserManagementAdapter: Permission check for user '{user_id}' "
        #     f"on action '{action}' (resource: '{resource_id}'): {has_permission}"
        # )
        return has_permission

    async def create_user(self, user_identifier: str) -> User:
        """Creates a new user in the database."""
        logger.info(f"Attempting to create user: {user_identifier}")
        async with self.async_session() as session:
            async with session.begin():
                try:
                    # Check if user already exists
                    existing_user_stmt = select(UserDB).where(
                        UserDB.user_identifier == user_identifier
                    )
                    result = await session.execute(existing_user_stmt)
                    if result.scalars().first():
                        logger.warning(f"User '{user_identifier}' already exists.")
                        # Consider raising an error or returning the existing user
                        # For simplicity, let's assume we just want to ensure it exists.
                        # Re-fetch to return consistent object
                        return await self.get_user_by_identifier(user_identifier)
                        # raise ValueError(f"User '{user_identifier}' already exists.")

                    new_user_db = UserDB(user_identifier=user_identifier)
                    session.add(new_user_db)
                    await (
                        session.flush()
                    )  # Flush to get ID and handle potential immediate errors
                    await session.refresh(
                        new_user_db
                    )  # Refresh to get all fields populated
                    logger.info(
                        f"Successfully created user: {user_identifier} with ID: {new_user_db.id}"
                    )
                    # Map DB model to Domain model for return
                    return User(
                        id=new_user_db.id,
                        user_identifier=new_user_db.user_identifier,
                        created_at=new_user_db.created_at,
                    )

                except IntegrityError as e:
                    await session.rollback()
                    logger.error(
                        f"Database integrity error creating user '{user_identifier}': {e}",
                        exc_info=True,
                    )
                    raise  # Re-raise after logging
                except Exception as e:
                    await session.rollback()
                    logger.error(
                        f"Unexpected error creating user '{user_identifier}': {e}",
                        exc_info=True,
                    )
                    raise

    async def get_user_by_identifier(self, user_identifier: str) -> Optional[User]:
        """Retrieves a user by their identifier from the database."""
        async with self.async_session() as session:
            try:
                statement = select(UserDB).where(
                    UserDB.user_identifier == user_identifier
                )
                result = await session.execute(statement)
                user_db = result.scalars().one_or_none()  # Use one_or_none for clarity

                if user_db:
                    # Map DB to Domain
                    return User(
                        id=user_db.id,
                        user_identifier=user_db.user_identifier,
                        created_at=user_db.created_at,
                    )
                else:
                    logger.debug(f"User '{user_identifier}' not found.")
                    return None
            except Exception as e:
                logger.error(
                    f"Error retrieving user '{user_identifier}': {e}", exc_info=True
                )
                return None  # Return None on error

    async def get_all_users(self) -> List[User]:
        """Retrieves all users from the database."""
        async with self.async_session() as session:
            try:
                statement = select(UserDB)
                results = await session.execute(statement)
                users_db = results.scalars().all()
                # Map list of DB models to list of Domain models
                return [
                    User(
                        id=u.id,
                        user_identifier=u.user_identifier,
                        created_at=u.created_at,
                    )
                    for u in users_db
                ]
            except Exception as e:
                logger.error(f"Error retrieving all users: {e}", exc_info=True)
                return []  # Return empty list on error

    async def delete_user(self, user_identifier: str) -> bool:
        """Deletes a user by their identifier from the database."""
        logger.info(f"Attempting to delete user: {user_identifier}")
        async with self.async_session() as session:
            async with session.begin():
                try:
                    # Find the user ID first
                    select_stmt = select(UserDB.id).where(
                        UserDB.user_identifier == user_identifier
                    )
                    result = await session.execute(select_stmt)
                    user_id = result.scalar_one_or_none()

                    if user_id is None:
                        logger.warning(
                            f"User '{user_identifier}' not found for deletion."
                        )
                        return False

                    # Delete the user by ID
                    delete_stmt = delete(UserDB).where(UserDB.id == user_id)
                    delete_result = await session.execute(delete_stmt)

                    if delete_result.rowcount == 1:
                        logger.info(f"Successfully deleted user: {user_identifier}")
                        return True
                    else:
                        # This shouldn't happen if we found the ID, but good to check
                        logger.warning(
                            f"User '{user_identifier}' found but deletion failed (rowcount={delete_result.rowcount})."
                        )
                        return False

                except (
                    NoResultFound
                ):  # Should be caught by the scalar_one_or_none check above
                    logger.warning(
                        f"User '{user_identifier}' not found for deletion (NoResultFound)."
                    )
                    return False
                except Exception as e:
                    await session.rollback()
                    logger.error(
                        f"Error deleting user '{user_identifier}': {e}", exc_info=True
                    )
                    raise  # Re-raise error after rollback

    async def check_if_any_user_exists(self) -> bool:
        """Checks if at least one user exists in the UserDB table."""
        async with self.async_session() as session:
            try:
                # Efficiently check for existence using count or limit 1
                statement = select(
                    func.count(UserDB.id)
                )  # Use func.count for efficiency
                result = await session.execute(statement)
                count = result.scalar_one()
                exists = count > 0
                # logger.debug(f"Checked if any user exists: {exists} (Count: {count})")
                return exists
            except Exception as e:
                logger.error(f"Error checking if any user exists: {e}", exc_info=True)
                return False  # Assume false on error? Or raise? False seems safer for initial setup.

================
File: paelladoc/adapters/services/system_time_service.py
================
"""System implementation of the time service."""

import datetime
from ...domain.services.time_service import TimeService


class SystemTimeService(TimeService):
    """System implementation of TimeService using system clock."""

    def get_current_time(self) -> datetime.datetime:
        """Get current timestamp in UTC using system clock."""
        return datetime.datetime.now(datetime.timezone.utc)

    def ensure_utc(self, dt: datetime.datetime) -> datetime.datetime:
        """Convert a datetime to UTC.

        If the datetime has no timezone info, assumes it's in UTC.
        """
        if dt.tzinfo is None:
            return dt.replace(tzinfo=datetime.timezone.utc)
        return dt.astimezone(datetime.timezone.utc)

================
File: paelladoc/application/utils/behavior_enforcer.py
================
"""
Utility for enforcing behavior rules defined in tool configurations.
"""

import logging
from typing import Dict, Any, Set, Optional

# Assuming MCPContext structure or relevant parts are accessible
# from mcp.context import Context as MCPContext # Or use Any for now

logger = logging.getLogger(__name__)


class BehaviorViolationError(Exception):
    """Custom exception raised when a behavior rule is violated."""

    def __init__(self, message: str):
        self.message = message
        super().__init__(self.message)


class BehaviorEnforcer:
    """Enforces conversational behavior based on tool config and context."""

    @staticmethod
    def enforce(
        tool_name: str,
        behavior_config: Optional[Dict[str, Any]],
        ctx: Optional[Any],  # Replace Any with actual MCPContext type if available
        provided_args: Optional[Dict[str, Any]],
    ):
        """Checks current context and arguments against behavior rules.

        Args:
            tool_name: The name of the tool being called.
            behavior_config: The BEHAVIOR_CONFIG dictionary for the tool.
            ctx: The current MCP context object (expected to have ctx.progress).
            provided_args: The arguments passed to the tool function in the current call.

        Raises:
            BehaviorViolationError: If a rule is violated.
        """
        if not behavior_config:
            logger.debug(
                f"No behavior config for tool '{tool_name}', skipping enforcement."
            )
            return

        if not ctx or not hasattr(ctx, "progress") or not provided_args:
            logger.warning(
                f"Behavior enforcement skipped for '{tool_name}': missing context or args."
            )
            # Decide if this should be an error or just skipped
            return

        # --- Enforce fixed_question_order ---
        if "fixed_question_order" in behavior_config:
            sequence = behavior_config["fixed_question_order"]
            if not isinstance(sequence, list):
                logger.warning(
                    f"Invalid 'fixed_question_order' in config for {tool_name}. Skipping check."
                )
                return

            # Assume ctx.progress['collected_params'] holds previously gathered arguments
            collected_params: Set[str] = ctx.progress.get("collected_params", set())

            # Identify arguments provided in *this* specific call (non-None values)
            current_call_args = {k for k, v in provided_args.items() if v is not None}

            # Identify which of the currently provided args are *new* (not already collected)
            newly_provided_params = current_call_args - collected_params

            if not newly_provided_params:
                # No *new* parameters were provided in this call.
                # This might be okay if just confirming or if sequence is done.
                # Or maybe it should error if the sequence is *not* done?
                # For now, allow proceeding. Behavior could be refined.
                logger.debug(
                    f"Tool '{tool_name}': No new parameters provided, sequence check passes by default."
                )
                return

            # Find the first parameter in the defined sequence that hasn't been collected yet
            expected_next_param = None
            for param in sequence:
                if param not in collected_params:
                    expected_next_param = param
                    break

            if expected_next_param is None:
                # The defined sequence is complete.
                # Should we allow providing *other* (optional?) parameters now?
                # If strict_parameter_sequence is True, maybe disallow?
                # For now, allow extra parameters after the main sequence.
                logger.debug(
                    f"Tool '{tool_name}': Sequence complete, allowing provided args: {newly_provided_params}"
                )
                return

            # --- Enforce one_parameter_at_a_time (implicitly for sequence) ---
            # Check if exactly one *new* parameter was provided and if it's the expected one.
            if len(newly_provided_params) > 1:
                raise BehaviorViolationError(
                    f"Tool '{tool_name}' expects parameters sequentially. "
                    f"Expected next: '{expected_next_param}'. "
                    f"Provided multiple new parameters: {newly_provided_params}. "
                    f"Collected so far: {collected_params}."
                )

            provided_param = list(newly_provided_params)[0]
            if provided_param != expected_next_param:
                raise BehaviorViolationError(
                    f"Tool '{tool_name}' expects parameters sequentially. "
                    f"Expected next: '{expected_next_param}'. "
                    f"Got unexpected new parameter: '{provided_param}'. "
                    f"Collected so far: {collected_params}."
                )

            # If we reach here, exactly one new parameter was provided and it was the expected one.
            logger.debug(
                f"Tool '{tool_name}': Correct sequential parameter '{provided_param}' provided."
            )

        # --- Add other rule checks here as needed ---
        # e.g., max_questions_per_message (more complex, needs turn context)
        # e.g., documentation_first (likely better as separate middleware/check)

        # If all checks pass
        return

================
File: paelladoc/application/services/vector_store_service.py
================
import logging
from typing import List, Dict, Any, Optional

# Ports and SearchResult
from paelladoc.ports.output.vector_store_port import VectorStorePort, SearchResult

logger = logging.getLogger(__name__)


class VectorStoreService:
    """Application service for interacting with the vector store.

    Uses the VectorStorePort to abstract the underlying vector database.
    """

    def __init__(self, vector_store_port: VectorStorePort):
        """Initializes the service with a VectorStorePort implementation."""
        self.vector_store_port = vector_store_port
        logger.info(
            f"VectorStoreService initialized with port: {type(vector_store_port).__name__}"
        )

    async def add_texts_to_collection(
        self,
        collection_name: str,
        documents: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ) -> List[str]:
        """Adds text documents to a specific collection."""
        logger.debug(
            f"Service: Adding {len(documents)} documents to vector store collection '{collection_name}'"
        )
        try:
            added_ids = await self.vector_store_port.add_documents(
                collection_name=collection_name,
                documents=documents,
                metadatas=metadatas,
                ids=ids,
            )
            logger.info(
                f"Service: Successfully added documents to collection '{collection_name}' with IDs: {added_ids}"
            )
            return added_ids
        except Exception as e:
            logger.error(
                f"Service: Error adding documents to collection '{collection_name}': {e}",
                exc_info=True,
            )
            # Re-raise or handle specific exceptions as needed
            raise

    async def find_similar_texts(
        self,
        collection_name: str,
        query_texts: List[str],
        n_results: int = 5,
        filter_metadata: Optional[Dict[str, Any]] = None,
        filter_document: Optional[Dict[str, Any]] = None,
    ) -> List[List[SearchResult]]:
        """Finds documents similar to the query texts within a collection."""
        logger.debug(
            f"Service: Searching collection '{collection_name}' for texts similar to: {query_texts} (n={n_results})"
        )
        try:
            results = await self.vector_store_port.search_similar(
                collection_name=collection_name,
                query_texts=query_texts,
                n_results=n_results,
                where=filter_metadata,  # Pass filters to the port
                where_document=filter_document,
                # Include common fields by default
                include=["metadatas", "documents", "distances", "ids"],
            )
            logger.info(
                f"Service: Found {sum(len(r) for r in results)} potential results for {len(query_texts)} queries in '{collection_name}'."
            )
            return results
        except Exception as e:
            logger.error(
                f"Service: Error searching collection '{collection_name}': {e}",
                exc_info=True,
            )
            # Re-raise or handle specific exceptions as needed
            raise

    async def ensure_collection_exists(self, collection_name: str):
        """Ensures a collection exists, creating it if necessary."""
        logger.debug(f"Service: Ensuring collection '{collection_name}' exists.")
        try:
            await self.vector_store_port.get_or_create_collection(collection_name)
            logger.info(f"Service: Collection '{collection_name}' checked/created.")
        except Exception as e:
            logger.error(
                f"Service: Error ensuring collection '{collection_name}' exists: {e}",
                exc_info=True,
            )
            raise

    async def remove_collection(self, collection_name: str):
        """Removes a collection entirely."""
        logger.debug(f"Service: Attempting to remove collection '{collection_name}'.")
        try:
            await self.vector_store_port.delete_collection(collection_name)
            logger.info(f"Service: Collection '{collection_name}' removed.")
        except Exception as e:
            logger.error(
                f"Service: Error removing collection '{collection_name}': {e}",
                exc_info=True,
            )
            raise

================
File: paelladoc/application/services/memory_service.py
================
import logging
from typing import Optional, Dict, Any, List, Callable, Awaitable

# Domain Models
from paelladoc.domain.models.project import (
    ProjectMemory,
    DocumentStatus,
    ArtifactMeta,
    Bucket,
)

# Ports
from paelladoc.ports.output.memory_port import MemoryPort

logger = logging.getLogger(__name__)

# Type definition for event handlers
EventHandler = Callable[[str, Dict[str, Any]], Awaitable[None]]


class MemoryService:
    """Application service for managing project memory operations.

    Uses the MemoryPort to interact with the persistence layer.
    """

    def __init__(self, memory_port: MemoryPort):
        """Initializes the service with a MemoryPort implementation."""
        self.memory_port = memory_port
        self._event_handlers: Dict[str, List[EventHandler]] = {}
        logger.info(
            f"MemoryService initialized with port: {type(memory_port).__name__}"
        )

    # --- Event System ---

    async def _emit_event(self, event_name: str, event_data: Dict[str, Any]) -> None:
        """Emits an event to all registered handlers for that event type.

        Args:
            event_name: The name of the event (e.g., 'artifact_created')
            event_data: Dictionary with event data
        """
        if event_name not in self._event_handlers:
            logger.debug(f"No handlers registered for event: {event_name}")
            return

        handlers = self._event_handlers[event_name]
        logger.debug(f"Emitting event '{event_name}' to {len(handlers)} handlers")

        for handler in handlers:
            try:
                await handler(event_name, event_data)
            except Exception as e:
                logger.error(
                    f"Error in event handler for '{event_name}': {e}", exc_info=True
                )

    def register_event_handler(self, event_name: str, handler: EventHandler) -> None:
        """Registers a handler function for a specific event type.

        Args:
            event_name: The event name to listen for
            handler: Async function that will be called when the event occurs
        """
        if event_name not in self._event_handlers:
            self._event_handlers[event_name] = []

        self._event_handlers[event_name].append(handler)
        logger.debug(f"Registered handler for event: {event_name}")

    def unregister_event_handler(self, event_name: str, handler: EventHandler) -> bool:
        """Unregisters a handler function for a specific event type.

        Args:
            event_name: The event name
            handler: The handler function to remove

        Returns:
            True if the handler was removed, False if not found
        """
        if event_name not in self._event_handlers:
            return False

        handlers = self._event_handlers[event_name]
        if handler in handlers:
            handlers.remove(handler)
            logger.debug(f"Unregistered handler for event: {event_name}")
            return True

        return False

    # --- Memory Service Methods ---

    async def get_project_memory(self, project_name: str) -> Optional[ProjectMemory]:
        """Retrieves the memory for a specific project."""
        logger.debug(f"Service: Attempting to get memory for project '{project_name}'")
        memory = await self.memory_port.load_memory(project_name)

        if memory:
            await self._emit_event(
                "memory_loaded",
                {
                    "project_name": project_name,
                    "memory_id": str(memory.project_info.name),
                    "timestamp": memory.last_updated_at.isoformat()
                    if memory.last_updated_at
                    else None,
                },
            )

        return memory

    async def check_project_exists(self, project_name: str) -> bool:
        """Checks if a project memory already exists."""
        logger.debug(f"Service: Checking existence for project '{project_name}'")
        return await self.memory_port.project_exists(project_name)

    async def create_project_memory(self, memory: ProjectMemory) -> ProjectMemory:
        """Creates a new project memory entry.

        Raises:
            ValueError: If a project with the same name already exists.
        """
        project_name = memory.project_info.name
        logger.debug(
            f"Service: Attempting to create memory for project '{project_name}'"
        )

        exists = await self.check_project_exists(project_name)
        if exists:
            logger.error(f"Cannot create project '{project_name}': already exists.")
            raise ValueError(f"Project memory for '{project_name}' already exists.")

        await self.memory_port.save_memory(memory)
        logger.info(
            f"Service: Successfully created memory for project '{project_name}'"
        )

        # Emit project_created event
        await self._emit_event(
            "project_created",
            {
                "project_name": project_name,
                "base_path": str(memory.project_info.base_path)
                if memory.project_info.base_path
                else None,
                "timestamp": memory.created_at.isoformat()
                if memory.created_at
                else None,
                "project_info_details": {
                    k: v
                    for k, v in memory.project_info.dict().items()
                    if k not in ["name", "base_path"] and v is not None
                },
            },
        )

        # Emit taxonomy event if taxonomy fields were provided
        if (
            memory.platform_taxonomy
            or memory.domain_taxonomy
            or memory.size_taxonomy
            or memory.compliance_taxonomy
            or memory.custom_taxonomy
        ):
            await self._emit_event(
                "taxonomy_updated",
                {
                    "project_name": project_name,
                    "new_taxonomy": {
                        "platform": memory.platform_taxonomy,
                        "domain": memory.domain_taxonomy,
                        "size": memory.size_taxonomy,
                        "compliance": memory.compliance_taxonomy,
                        "custom": memory.custom_taxonomy,
                    },
                    "old_taxonomy": None,  # First time setting it
                },
            )

        # Emit artifact_created events for initial artifacts
        for bucket, artifacts in memory.artifacts.items():
            for artifact in artifacts:
                await self._emit_event(
                    "artifact_created",
                    {
                        "project_name": project_name,
                        "artifact_id": str(artifact.id),
                        "artifact_name": artifact.name,
                        "bucket": bucket.value,
                        "path": str(artifact.path),
                        "status": artifact.status.value,
                        "timestamp": artifact.created_at.isoformat()
                        if artifact.created_at
                        else None,
                        "created_by": artifact.created_by,
                    },
                )

        return memory  # Return the saved object (could also reload it)

    async def update_project_memory(self, memory: ProjectMemory) -> ProjectMemory:
        """Updates an existing project memory entry.

        Raises:
            ValueError: If the project does not exist.
        """
        project_name = memory.project_info.name
        logger.debug(
            f"Service: Attempting to update memory for project '{project_name}'"
        )

        # Ensure the project exists before attempting an update
        # Note: save_memory itself handles the create/update logic, but this check
        # makes the service layer's intent clearer and prevents accidental creation.
        exists = await self.check_project_exists(project_name)
        if not exists:
            logger.error(f"Cannot update project '{project_name}': does not exist.")
            raise ValueError(
                f"Project memory for '{project_name}' does not exist. Use create_project_memory first."
            )

        # Get the old memory to compare changes
        old_memory = await self.memory_port.load_memory(project_name)

        # Save the updated memory
        await self.memory_port.save_memory(memory)
        logger.info(
            f"Service: Successfully updated memory for project '{project_name}'"
        )

        # Emit project_updated event
        await self._emit_event(
            "project_updated",
            {
                "project_name": project_name,
                "timestamp": memory.last_updated_at.isoformat()
                if memory.last_updated_at
                else None,
            },
        )

        # Check if taxonomy fields changed
        if old_memory and (
            memory.platform_taxonomy != old_memory.platform_taxonomy
            or memory.domain_taxonomy != old_memory.domain_taxonomy
            or memory.size_taxonomy != old_memory.size_taxonomy
            or memory.compliance_taxonomy != old_memory.compliance_taxonomy
        ):
            await self._emit_event(
                "taxonomy_updated",
                {
                    "project_name": project_name,
                    "timestamp": memory.last_updated_at.isoformat()
                    if memory.last_updated_at
                    else None,
                    "new_taxonomy": {
                        "platform": memory.platform_taxonomy,
                        "domain": memory.domain_taxonomy,
                        "size": memory.size_taxonomy,
                        "compliance": memory.compliance_taxonomy,
                    },
                    "old_taxonomy": {
                        "platform": old_memory.platform_taxonomy,
                        "domain": old_memory.domain_taxonomy,
                        "size": old_memory.size_taxonomy,
                        "compliance": old_memory.compliance_taxonomy,
                    },
                },
            )

        # Check for new or updated artifacts
        if old_memory:
            # Track artifacts by ID to detect changes
            for bucket, artifacts in memory.artifacts.items():
                # Skip empty buckets
                if not artifacts:
                    continue

                old_bucket_artifacts = old_memory.artifacts.get(bucket, [])
                old_artifact_ids = {str(a.id): a for a in old_bucket_artifacts}

                # Check each artifact in the new memory
                for artifact in artifacts:
                    artifact_id = str(artifact.id)

                    # If artifact didn't exist before, it's new
                    if artifact_id not in old_artifact_ids:
                        await self._emit_event(
                            "artifact_created",
                            {
                                "project_name": project_name,
                                "artifact_id": artifact_id,
                                "artifact_name": artifact.name,
                                "bucket": bucket.value,
                                "path": str(artifact.path),
                                "status": artifact.status.value,
                                "timestamp": artifact.created_at.isoformat()
                                if artifact.created_at
                                else None,
                                "created_by": artifact.created_by,
                            },
                        )
                    else:
                        # If artifact existed, check if it was updated
                        old_artifact = old_artifact_ids[artifact_id]
                        if (
                            artifact.status != old_artifact.status
                            or artifact.updated_at != old_artifact.updated_at
                        ):
                            await self._emit_event(
                                "artifact_updated",
                                {
                                    "project_name": project_name,
                                    "artifact_id": artifact_id,
                                    "artifact_name": artifact.name,
                                    "bucket": bucket.value,
                                    "path": str(artifact.path),
                                    "old_status": old_artifact.status.value,
                                    "new_status": artifact.status.value,
                                    "timestamp": artifact.updated_at.isoformat()
                                    if artifact.updated_at
                                    else None,
                                    "modified_by": artifact.modified_by,
                                },
                            )

        return memory  # Return the updated object

    # Example of a more specific use case method:
    async def update_document_status_in_memory(
        self, project_name: str, document_name: str, new_status: DocumentStatus
    ) -> Optional[ProjectMemory]:
        """Updates the status of a specific document within a project's memory."""
        logger.debug(
            f"Service: Updating status for document '{document_name}' in project '{project_name}' to {new_status}"
        )
        memory = await self.get_project_memory(project_name)
        if not memory:
            logger.warning(
                f"Project '{project_name}' not found, cannot update document status."
            )
            return None

        if document_name not in memory.documents:
            logger.warning(
                f"Document '{document_name}' not found in project '{project_name}', cannot update status."
            )
            # Or should we raise an error?
            return memory  # Return unchanged memory?

        # Get old status for event
        old_status = memory.documents[document_name].status

        # Update status
        memory.update_document_status(
            document_name, new_status
        )  # Use domain model method

        # Save the updated memory
        await self.memory_port.save_memory(memory)
        logger.info(
            f"Service: Saved updated status for document '{document_name}' in project '{project_name}'"
        )

        # Emit document_status_changed event
        await self._emit_event(
            "document_status_changed",
            {
                "project_name": project_name,
                "document_name": document_name,
                "old_status": old_status.value,
                "new_status": new_status.value,
                "timestamp": memory.last_updated_at.isoformat()
                if memory.last_updated_at
                else None,
            },
        )

        return memory

    async def add_artifact(
        self, project_name: str, artifact: ArtifactMeta, author: Optional[str] = None
    ) -> Optional[ProjectMemory]:
        """Adds a new artifact to a project's memory.

        Args:
            project_name: The name of the project
            artifact: The artifact to add
            author: Optional name of the author creating the artifact

        Returns:
            The updated project memory, or None if project not found
        """
        logger.debug(
            f"Service: Adding artifact '{artifact.name}' to project '{project_name}'"
        )

        # Set author if provided
        if author and not artifact.created_by:
            artifact.created_by = author
            artifact.modified_by = author

        memory = await self.get_project_memory(project_name)
        if not memory:
            logger.warning(f"Project '{project_name}' not found, cannot add artifact.")
            return None

        # Add the artifact
        added = memory.add_artifact(artifact)
        if not added:
            logger.warning(
                f"Artifact with path '{artifact.path}' already exists in project '{project_name}'"
            )
            return memory

        # Save the updated memory
        await self.memory_port.save_memory(memory)
        logger.info(
            f"Service: Saved new artifact '{artifact.name}' in project '{project_name}'"
        )

        # Emit artifact_created event
        await self._emit_event(
            "artifact_created",
            {
                "project_name": project_name,
                "artifact_id": str(artifact.id),
                "artifact_name": artifact.name,
                "bucket": artifact.bucket.value,
                "path": str(artifact.path),
                "status": artifact.status.value,
                "timestamp": artifact.created_at.isoformat()
                if artifact.created_at
                else None,
                "created_by": artifact.created_by,
            },
        )

        return memory

    async def update_artifact_status(
        self,
        project_name: str,
        bucket: Bucket,
        artifact_name: str,
        new_status: DocumentStatus,
        modifier: Optional[str] = None,
    ) -> Optional[ProjectMemory]:
        """Updates the status of a specific artifact within a project's memory.

        Args:
            project_name: The name of the project
            bucket: The bucket containing the artifact
            artifact_name: The name of the artifact to update
            new_status: The new status to set
            modifier: Optional name of the person making the change

        Returns:
            The updated project memory, or None if project not found
        """
        logger.debug(
            f"Service: Updating status for artifact '{artifact_name}' in project '{project_name}' to {new_status}"
        )

        memory = await self.get_project_memory(project_name)
        if not memory:
            logger.warning(
                f"Project '{project_name}' not found, cannot update artifact status."
            )
            return None

        # Get the artifact to check its current status
        artifact = memory.get_artifact(bucket, artifact_name)
        if not artifact:
            logger.warning(
                f"Artifact '{artifact_name}' not found in bucket '{bucket.value}' for project '{project_name}'"
            )
            return memory

        old_status = artifact.status

        # Update the artifact status
        updated = memory.update_artifact_status(
            bucket, artifact_name, new_status, modifier
        )
        if not updated:
            logger.warning(
                f"Failed to update status for artifact '{artifact_name}' in project '{project_name}'"
            )
            return memory

        # Save the updated memory
        await self.memory_port.save_memory(memory)
        logger.info(
            f"Service: Saved updated status for artifact '{artifact_name}' in project '{project_name}'"
        )

        # Emit artifact_updated event
        await self._emit_event(
            "artifact_updated",
            {
                "project_name": project_name,
                "artifact_id": str(artifact.id),
                "artifact_name": artifact_name,
                "bucket": bucket.value,
                "old_status": old_status.value,
                "new_status": new_status.value,
                "timestamp": artifact.updated_at.isoformat()
                if artifact.updated_at
                else None,
                "modified_by": modifier or artifact.modified_by,
            },
        )

        return memory

================
File: paelladoc/ports/input/mcp_port.py
================
from abc import ABC, abstractmethod
from typing import Any, Dict


class MCPPort(ABC):
    """Input port for MCP (Model-Command-Process) operations."""

    @abstractmethod
    def process_command(self, command: str, args: Dict[str, Any]) -> Dict[str, Any]:
        """Process an MCP command with its arguments."""
        pass

    @abstractmethod
    def register_plugin(self, plugin: Any) -> None:
        """Register a new plugin."""
        pass

================
File: paelladoc/ports/input/mcp_server_adapter.py
================
#!/usr/bin/env python3
"""
PAELLADOC MCP Server entry point (Input Adapter).

Relies on paelladoc_core.py (now core_logic.py in domain) for MCP functionality and FastMCP instance.
Simply runs the imported MCP instance.
Adds server-specific resources and prompts using decorators.
"""

import sys
import logging
from pathlib import Path
import time  # Add time import

# Import TextContent for prompt definition
from mcp.types import TextContent  # Assuming mcp is installed in .venv

# Import the core FastMCP instance and logger from the domain layer
from paelladoc.domain.core_logic import mcp, logger  # Corrected import path

# --- Import plugin packages to trigger their __init__.py dynamic loading --- #
# This ensures decorators within the package modules are executed when the server starts

# Import core plugins package
# This will execute plugins/core/__init__.py which dynamically loads modules like paella.py

# We might need other plugin packages later, e.g.:
# from paelladoc.adapters.plugins import code_analysis
# from paelladoc.adapters.plugins import product_management


# --- Add specific tools/resources/prompts for this entry point using decorators --- #
# These are defined directly in this adapter file and might be deprecated later


@mcp.resource("docs://readme")  # Use decorator
def get_readme() -> str:
    """Get the project README content."""
    try:
        # Assuming README.md is in the project root (cwd)
        readme_path = Path("README.md")
        if readme_path.exists():
            return readme_path.read_text()
        else:
            logger.warning("README.md not found in project root.")
            return "README.md not found"  # Keep simple return for resource
    except Exception as e:
        logger.error(f"Error reading README.md: {e}", exc_info=True)
        return f"Error reading README.md: {str(e)}"


@mcp.resource("docs://templates/{template_name}")  # Use decorator
def get_template(template_name: str) -> str:
    """Get a documentation template."""
    # Corrected path relative to src directory
    base_path = Path(__file__).parent.parent.parent.parent  # Should point to src/
    template_path = (
        base_path
        / "paelladoc"
        / "adapters"
        / "plugins"
        / "templates"
        / f"{template_name}.md"
    )
    try:
        if template_path.exists():
            return template_path.read_text()
        else:
            logger.warning(f"Template {template_name} not found at {template_path}")
            return f"Error: Template {template_name} not found"
    except Exception as e:
        logger.error(f"Error reading template {template_name}: {e}", exc_info=True)
        return f"Error reading template {template_name}: {str(e)}"


@mcp.prompt()  # Use decorator
def paella_command(project_name: str) -> TextContent:
    """Create a PAELLA command prompt."""
    return TextContent(
        type="text",
        text=f"Initiating PAELLADOC for project: {project_name}.\n"
        f"Please specify: 1. Project type, 2. Methodologies, 3. Git workflow.",
    )


# --- Main Execution Logic --- #

if __name__ == "__main__":
    # Configure file logging
    try:
        log_file = "paelladoc_server.log"
        file_handler = logging.FileHandler(log_file)
        file_handler.setFormatter(
            logging.Formatter("%(asctime)s - %(name)s - %(levelname)s - %(message)s")
        )
        logging.getLogger().addHandler(file_handler)
        logging.getLogger().setLevel(logging.DEBUG)
        logger.info(f"Logging configured. Outputting to {log_file}")
    except Exception as log_e:
        logging.basicConfig(
            level=logging.DEBUG, format="%(asctime)s - %(levelname)s - %(message)s"
        )
        # Re-get logger after basicConfig potentially reconfigured root
        logger = logging.getLogger(__name__)
        logger.error(f"Could not configure file logging: {log_e}. Logging to stderr.")

    # Check command line arguments to determine run mode
    run_mode = (
        "stdio" if "--stdio" in sys.argv else "web"
    )  # Default to stdio if --stdio present

    try:
        if run_mode == "stdio":
            logger.info(
                "Starting PAELLADOC MCP server in STDIO mode via FastMCP mcp.run(transport='stdio')..."
            )
            logger.debug("Waiting 10 seconds before mcp.run()...")
            time.sleep(10)  # Add sleep before run
            logger.debug('Attempting mcp.run(transport="stdio")')
            mcp.run(transport="stdio")  # Explicitly request stdio transport
        else:
            # Attempt to run the default web server (SSE)
            # Note: FastMCP's default run() might try stdio first anyway if no host/port specified
            logger.warning(
                "Starting PAELLADOC MCP server in default mode (likely web/SSE) via FastMCP mcp.run()..."
            )
            logger.warning("Use --stdio argument for direct client integration.")
            mcp.run()  # Run with default settings (tries SSE/web)

        logger.info(f"PAELLADOC MCP server finished (mode: {run_mode}).")
    except Exception as e:
        logger.critical(f"Failed to start or run MCP server: {e}", exc_info=True)
        sys.exit(1)

================
File: paelladoc/ports/output/taxonomy_provider.py
================
from abc import ABC, abstractmethod
from typing import Dict, List


class TaxonomyProvider(ABC):
    """Abstract interface for providing available taxonomy information."""

    @abstractmethod
    def get_available_taxonomies(self) -> Dict[str, List[str]]:
        """Returns a dictionary of available taxonomies grouped by category.

        Example:
            {
                "platform": ["web-frontend", "ios-native", ...],
                "domain": ["ecommerce", "ai-ml", ...],
                ...
            }
        """
        pass

================
File: paelladoc/ports/output/vector_store_port.py
================
from abc import ABC, abstractmethod
from typing import List, Dict, Any, Optional


class SearchResult(ABC):
    """Represents a single search result from the vector store."""

    # Define common attributes for a search result
    id: str
    distance: Optional[float] = None
    metadata: Optional[Dict[str, Any]] = None
    document: Optional[str] = None


class VectorStorePort(ABC):
    """Output Port defining operations for a vector store."""

    @abstractmethod
    async def add_documents(
        self,
        collection_name: str,
        documents: List[str],
        metadatas: Optional[List[Dict[str, Any]]] = None,
        ids: Optional[List[str]] = None,
    ) -> List[str]:
        """Adds documents (text) to a specific collection in the vector store.

        Embeddings are typically generated automatically by the implementation.

        Args:
            collection_name: The name of the collection to add documents to.
            documents: A list of text documents to add.
            metadatas: Optional list of metadata dictionaries corresponding to each document.
            ids: Optional list of unique IDs for each document.

        Returns:
            A list of IDs for the added documents.
        """
        pass

    @abstractmethod
    async def search_similar(
        self,
        collection_name: str,
        query_texts: List[str],
        n_results: int = 5,
        where: Optional[Dict[str, Any]] = None,
        where_document: Optional[Dict[str, Any]] = None,
        include: Optional[List[str]] = ["metadatas", "documents", "distances"],
    ) -> List[List[SearchResult]]:
        """Searches for documents in a collection similar to the query texts.

        Args:
            collection_name: The name of the collection to search within.
            query_texts: A list of query texts to find similar documents for.
            n_results: The maximum number of results to return for each query.
            where: Optional filter criteria for metadata.
            where_document: Optional filter criteria for document content.
            include: Optional list specifying what data to include in results.

        Returns:
            A list of lists of SearchResult objects, one list per query text.
        """
        pass

    @abstractmethod
    async def get_or_create_collection(self, collection_name: str) -> Any:
        """Gets or creates a collection in the vector store.

        The return type is Any for now, as it depends on the specific library's
        collection object representation (e.g., Chroma's Collection).

        Args:
            collection_name: The name of the collection.

        Returns:
            The collection object.
        """
        pass

    @abstractmethod
    async def delete_collection(self, collection_name: str) -> None:
        """Deletes a collection from the vector store.

        Args:
            collection_name: The name of the collection to delete.
        """
        pass

    # Add other potential methods like:
    # async def delete_documents(self, collection_name: str, ids: List[str]) -> None: ...
    # async def update_documents(...) -> None: ...

================
File: paelladoc/ports/output/user_management_port.py
================
"""Port interface for User Management operations."""

from abc import ABC, abstractmethod
from typing import Optional, Any, List
from paelladoc.domain.models.user import User


class UserManagementPort(ABC):
    """
    Abstract interface defining operations related to user identity and permissions.
    This allows decoupling the core application from specific user management implementations
    (e.g., a simple dummy one for OSS, a complex one for SaaS).
    """

    @abstractmethod
    async def get_current_user_id(self) -> Optional[str]:
        """
        Retrieves the unique identifier of the user making the current request.

        Returns:
            A string representing the user ID, or None if the user is anonymous
            or identity cannot be determined.
        """
        pass

    @abstractmethod
    async def check_permission(
        self, user_id: Optional[str], action: str, resource_id: Optional[Any] = None
    ) -> bool:
        """
        Checks if a given user has permission to perform a specific action,
        optionally on a specific resource.

        Args:
            user_id: The unique identifier of the user to check permissions for.
                     Can be None for anonymous users.
            action: A string identifying the action being attempted
                    (e.g., 'core_delete_project', 'core_update_template').
                    It's recommended to use the MCP tool name or a similar unique identifier.
            resource_id: (Optional) The identifier of the resource being accessed
                         (e.g., project name, template ID). Used for object-level permissions.

        Returns:
            True if the user has the necessary permission, False otherwise.
        """
        pass

    @abstractmethod
    async def create_user(self, user_identifier: str) -> User:
        """Creates a new user."""
        pass

    @abstractmethod
    async def get_user_by_identifier(self, user_identifier: str) -> Optional[User]:
        """Retrieves a user by their identifier."""
        pass

    @abstractmethod
    async def get_all_users(self) -> List[User]:
        """Retrieves all users."""
        pass

    @abstractmethod
    async def delete_user(self, user_identifier: str) -> bool:
        """Deletes a user by their identifier."""
        pass

    @abstractmethod
    async def check_if_any_user_exists(self) -> bool:
        """Checks if at least one user exists in the system."""
        pass

    # Potential future methods:
    # async def get_user_display_name(self, user_id: str) -> Optional[str]: ...
    # async def get_user_roles(self, user_id: str) -> List[str]: ...

================
File: paelladoc/ports/output/memory_port.py
================
from abc import ABC, abstractmethod
from typing import Optional, List

# Import the domain model it needs to interact with
from paelladoc.domain.models.project import (
    ProjectMemory,
    ProjectInfo,
)


class MemoryPort(ABC):
    """Output Port defining operations for project memory persistence."""

    @abstractmethod
    async def save_memory(self, memory: ProjectMemory) -> None:
        """Saves the entire project memory state.

        Args:
            memory: The ProjectMemory object to save.
        """
        pass

    @abstractmethod
    async def load_memory(self, project_name: str) -> Optional[ProjectMemory]:
        """Loads the project memory state for a given project name.

        Args:
            project_name: The unique name of the project to load.

        Returns:
            The ProjectMemory object if found, otherwise None.
        """
        pass

    @abstractmethod
    async def project_exists(self, project_name: str) -> bool:
        """Checks if a project memory exists for the given name.

        Args:
            project_name: The unique name of the project to check.

        Returns:
            True if the project memory exists, False otherwise.
        """
        pass

    @abstractmethod
    async def list_projects(self) -> List[ProjectInfo]:
        """Lists the names of all existing projects.

        Returns:
            A list of project names as strings. Returns an empty list if no projects exist.
        """
        pass

    @abstractmethod
    async def delete_memory(self, project_name: str) -> None:
        """Deletes the project memory state."""
        pass

    @abstractmethod
    async def get_active_project(self) -> Optional[ProjectInfo]:
        """Gets the currently active project, if any."""
        pass

    @abstractmethod
    async def set_active_project(self, project_name: str) -> bool:
        """Sets the specified project as active, deactivating others. Returns True on success."""
        pass

    # Potentially add other methods later if needed, e.g., delete_memory

================
File: paelladoc/ports/output/configuration_port.py
================
"""Configuration port for accessing system configurations."""

from abc import ABC, abstractmethod
from typing import Dict, Any, List, Optional


class ConfigurationPort(ABC):
    """Port for accessing system configurations and metadata."""

    @abstractmethod
    async def get_behavior_config(
        self, category: Optional[str] = None
    ) -> Dict[str, Any]:
        """Retrieves behavior configuration settings.

        Args:
            category: Optional category to filter configurations.

        Returns:
            Dictionary of configuration key-value pairs.
        """
        pass

    @abstractmethod
    async def get_mece_dimensions(self) -> Dict[str, Dict[str, Any]]:
        """Retrieves MECE dimension configurations.

        Returns:
            Dictionary containing:
            - allowed_dimensions: List of allowed dimension names
            - required_dimensions: List of required dimension names
            - validations: Dictionary of validation rules
        """
        pass

    @abstractmethod
    async def get_bucket_order(self, category: Optional[str] = None) -> List[str]:
        """Retrieves the ordered list of buckets.

        Args:
            category: Optional category to filter bucket types.

        Returns:
            List of bucket names in their defined order.
        """
        pass

    @abstractmethod
    async def get_commands_metadata(self) -> Dict[str, Dict[str, Any]]:
        """Retrieves metadata about available commands.

        Returns:
            Dictionary of command definitions including descriptions,
            parameters, and examples.
        """
        pass

    @abstractmethod
    async def save_behavior_config(
        self, config: Dict[str, Any], category: Optional[str] = None
    ) -> None:
        """Saves behavior configuration settings.

        Args:
            config: Dictionary of configuration key-value pairs.
            category: Optional category for the configuration.
        """
        pass

================
File: paelladoc/ports/output/taxonomy_repository.py
================
from abc import ABC, abstractmethod
from typing import List, Set, Optional


class TaxonomyRepository(ABC):
    """Abstract interface for providing taxonomy information."""

    @abstractmethod
    def get_available_dimensions(self) -> List[str]:
        """Returns a list of available dimensions (e.g., platform, domain, size, compliance, lifecycle)."""
        pass

    @abstractmethod
    def get_dimension_values(self, dimension: str) -> List[str]:
        """Returns a list of available values for a specific dimension."""
        pass

    @abstractmethod
    def get_buckets_for_dimension_value(self, dimension: str, value: str) -> List[str]:
        """Returns the list of taxonomy buckets associated with a specific dimension value."""
        pass

    @abstractmethod
    def get_buckets_for_project(
        self,
        platform: str,
        domain: str,
        size: str,
        lifecycle: str,
        compliance: Optional[str] = None,
    ) -> Set[str]:
        """
        Returns the combined set of taxonomy buckets for a project with the given dimensions.

        This is the union of all buckets from each dimension value, with duplicates removed.
        """
        pass

    @abstractmethod
    def get_all_available_buckets(self) -> List[str]:
        """Returns a list of all available bucket IDs from the taxonomy."""
        pass

    @abstractmethod
    def get_bucket_description(self, bucket_id: str) -> Optional[str]:
        """Returns the description for a specific bucket ID, or None if not found."""
        pass

================
File: paelladoc/ports/output/mcp_config_port.py
================
"""Port interface for MCP configuration."""

from abc import ABC, abstractmethod
from typing import Dict, List, Optional


class MCPConfigPort(ABC):
    """Abstract interface for MCP configuration access."""

    @abstractmethod
    async def get_available_mcps(self) -> List[str]:
        """Returns a list of all available MCP names."""
        pass

    @abstractmethod
    async def get_mcp_config(self, mcp_name: str) -> Optional[Dict]:
        """Returns the configuration for a specific MCP."""
        pass

    @abstractmethod
    async def get_tools_for_taxonomy(self, taxonomy_bucket: str) -> List[str]:
        """Returns a list of MCP tools available for a given taxonomy bucket."""
        pass

    @abstractmethod
    async def get_enabled_mcps_for_project(
        self, project_buckets: List[str]
    ) -> List[str]:
        """Returns a list of MCPs that should be enabled based on project buckets."""
        pass

================
File: paelladoc/domain/core_logic.py
================
"""
Core PAELLADOC MCP Logic.

Handles MCP instance creation, plugin loading, and base tool registration.
Uses FastMCP for compatibility with decorators.
"""

import logging
from importlib.util import find_spec
from mcp.server.fastmcp.server import FastMCP
from typing import Dict, Any

# Configure base logger (handlers will be added by server.py)
logging.basicConfig(
    level=logging.INFO, format="%(asctime)s - %(levelname)s - %(message)s"
)
logger = logging.getLogger(__name__)

# Create the MCP server instance using FastMCP
mcp = FastMCP("PAELLADOC")

# --- Register Tools/Prompts --- #

# Import plugins dynamically to register tools/prompts
try:
    # Check if plugins module is available
    if find_spec("paelladoc.adapters.plugins"):
        import paelladoc.adapters.plugins  # noqa: F401

        logger.info("Successfully loaded plugins from paelladoc.adapters.plugins")
    else:
        logger.warning("Could not find paelladoc.adapters.plugins module")
except ImportError as e:
    # Log as warning, server might still be usable with base tools
    logger.warning(f"Could not import plugins from paelladoc.adapters.plugins: {e}")
except Exception as e:
    # Log as error for unexpected issues during import
    logger.error(
        f"An unexpected error occurred during plugin import: {e}", exc_info=True
    )


@mcp.tool()  # Use decorator again
def ping(random_string: str = "") -> Dict[str, Any]:
    """
    Basic health check; returns pong.

    Args:
        random_string (str, optional): Dummy parameter for no-parameter tools

    Returns:
        Dict[str, Any]: Response with status and message
    """
    logger.debug(f"Ping tool called with parameter: {random_string}")
    return {"status": "ok", "message": "pong"}


# Tools will be registered here by plugins

# Note: No `if __name__ == "__main__":` block here.
# This file is intended to be imported by the entry point (server.py).

================
File: paelladoc/domain/models/user.py
================
"""Domain models for User."""

from datetime import datetime

# Remove SQLModel imports as UserDB is no longer defined here
# from sqlmodel import SQLModel, Field
from pydantic import BaseModel


# --- Database Model (SQLModel) ---
# UserDB definition REMOVED. It lives in adapters/output/sqlite/db_models.py


# --- Domain Model (Pydantic) ---
class User(BaseModel):
    """Domain model representing a user, used for data transfer."""

    id: int
    user_identifier: str
    created_at: datetime

    class Config:
        from_attributes = True  # Allow creating from ORM objects (like UserDB)

================
File: paelladoc/domain/models/enums.py
================
from enum import Enum
from typing import Set


class DocumentStatus(str, Enum):
    """Status of a document in the project memory"""

    PENDING = "pending"
    IN_PROGRESS = "in_progress"
    COMPLETED = "completed"
    ARCHIVED = "archived"


class Bucket(str, Enum):
    """MECE taxonomy buckets for categorizing artifacts"""

    # Initiate categories
    INITIATE_CORE_SETUP = "Initiate::CoreSetup"
    INITIATE_INITIAL_PRODUCT_DOCS = "Initiate::InitialProductDocs"

    # Elaborate categories
    ELABORATE_DISCOVERY_AND_RESEARCH = "Elaborate::DiscoveryAndResearch"
    ELABORATE_IDEATION_AND_DESIGN = "Elaborate::IdeationAndDesign"
    ELABORATE_SPECIFICATION_AND_PLANNING = "Elaborate::SpecificationAndPlanning"
    ELABORATE_CORE_AND_SUPPORT = "Elaborate::CoreAndSupport"

    # Govern categories
    GOVERN_CORE_SYSTEM = "Govern::CoreSystem"
    GOVERN_STANDARDS_METHODOLOGIES = "Govern::StandardsMethodologies"
    GOVERN_VERIFICATION_VALIDATION = "Govern::VerificationValidation"
    GOVERN_MEMORY_TEMPLATES = "Govern::MemoryTemplates"
    GOVERN_TOOLING_SCRIPTS = "Govern::ToolingScripts"

    # Generate categories
    GENERATE_CORE_FUNCTIONALITY = "Generate::CoreFunctionality"
    GENERATE_SUPPORTING_ELEMENTS = "Generate::SupportingElements"

    # Maintain categories
    MAINTAIN_CORE_FUNCTIONALITY = "Maintain::CoreFunctionality"
    MAINTAIN_SUPPORTING_ELEMENTS = "Maintain::SupportingElements"

    # Deploy categories
    DEPLOY_PIPELINES_AND_AUTOMATION = "Deploy::PipelinesAndAutomation"
    DEPLOY_INFRASTRUCTURE_AND_CONFIG = "Deploy::InfrastructureAndConfig"
    DEPLOY_GUIDES_AND_CHECKLISTS = "Deploy::GuidesAndChecklists"
    DEPLOY_SECURITY = "Deploy::Security"

    # Operate categories
    OPERATE_RUNBOOKS_AND_SOPS = "Operate::RunbooksAndSOPs"
    OPERATE_MONITORING_AND_ALERTING = "Operate::MonitoringAndAlerting"
    OPERATE_MAINTENANCE = "Operate::Maintenance"

    # Iterate categories
    ITERATE_LEARNING_AND_ANALYSIS = "Iterate::LearningAndAnalysis"
    ITERATE_PLANNING_AND_RETROSPECTION = "Iterate::PlanningAndRetrospection"

    # Special bucket for artifacts not matching any pattern
    UNKNOWN = "Unknown"

    @classmethod
    def get_phase_buckets(cls, phase: str) -> Set["Bucket"]:
        """Get all buckets belonging to a specific phase"""
        return {bucket for bucket in cls if bucket.value.startswith(f"{phase}::")}

================
File: paelladoc/domain/models/fix_metadata.py
================
import re

# Leer el archivo
with open("project.py", "r") as f:
    content = f.read()

# Cambiar nombre de clase Metadata a ProjectInfo para evitar palabras reservadas
content = re.sub(
    r"class ProjectMetadata\(BaseModel\):", "class ProjectInfo(BaseModel):", content
)

# Cambiar referencias a project_metadata para usar project_info
content = re.sub(
    r"project_metadata: ProjectMetadata", "project_info: ProjectInfo", content
)

# Asegurarse de usar project_info en lugar de metadata en todo el código
content = re.sub(r"memory\.metadata\.", "memory.project_info.", content)
content = re.sub(r"self\.metadata\.", "self.project_info.", content)

# Corregir timestamps utcnow()
content = content.replace(
    "datetime.utcnow()", "datetime.datetime.now(datetime.timezone.utc)"
)

# Guardar el archivo modificado
with open("project.py", "w") as f:
    f.write(content)

print("Modificación completada correctamente")

================
File: paelladoc/domain/models/language.py
================
"""Language model for PAELLADOC.

This module defines the supported languages and their metadata.
Following BCP 47 language tags (e.g., en-US, es-ES).
"""

from dataclasses import dataclass
from typing import Dict, List
from enum import Enum


@dataclass
class Language:
    """Represents a supported language with its code and name."""

    code: str
    name: str
    native_name: str = ""


class LanguageService:
    """Service for managing supported languages."""

    # Core supported languages (minimal set for initial implementation)
    SUPPORTED_LANGUAGES: Dict[str, Language] = {
        "es-ES": Language("es-ES", "Spanish (Spain)", "Español (España)"),
        "en-US": Language("en-US", "English (US)", "English (US)"),
    }

    @classmethod
    def get_language(cls, code: str) -> Language:
        """Get language by code."""
        return cls.SUPPORTED_LANGUAGES.get(code, Language(code, code, code))

    @classmethod
    def get_all_languages(cls) -> List[Language]:
        """Get all supported languages."""
        return list(cls.SUPPORTED_LANGUAGES.values())

    @classmethod
    def is_supported(cls, code: str) -> bool:
        """Check if a language code is supported."""
        return code in cls.SUPPORTED_LANGUAGES


class SupportedLanguage(str, Enum):
    """
    Supported languages for PAELLADOC interaction and documentation.
    Uses standard language codes (e.g., en-US, es-ES).
    """

    EN_US = "en-US"  # English (US)
    ES_ES = "es-ES"  # Spanish (Spain)

    @classmethod
    def from_code(cls, code: str) -> "SupportedLanguage":
        """Convert a language code to a SupportedLanguage enum."""
        code = code.lower()
        if code in ["en", "en-us"]:
            return cls.EN_US
        elif code in ["es", "es-es"]:
            return cls.ES_ES
        raise ValueError(f"Unsupported language code: {code}")

    def __str__(self) -> str:
        """Return the language code as a string."""
        return self.value

================
File: paelladoc/domain/models/project.py
================
from typing import List, Dict, Optional, Any
from pydantic import BaseModel, Field
import datetime
from pathlib import Path
import uuid
from .enums import DocumentStatus, Bucket
from ..services.time_service import TimeService
import logging
from pydantic import ConfigDict

logger = logging.getLogger(__name__)

# Singleton instance of the time service
# This will be injected by the application layer
time_service: TimeService = None


def set_time_service(service: TimeService):
    """Set the time service instance to be used by the domain models."""
    global time_service
    time_service = service


class ProjectDocument(BaseModel):
    name: str  # e.g., "README.md", "CONTRIBUTING.md"
    template_origin: Optional[str] = None  # Path or identifier of the template used
    status: DocumentStatus = DocumentStatus.PENDING


class ProjectInfo(BaseModel):
    """Basic project metadata."""

    model_config = ConfigDict(arbitrary_types_allowed=True)

    name: str = Field(..., min_length=1)
    language: Optional[str] = None  # Should be validated against SupportedLanguage
    purpose: Optional[str] = None
    target_audience: Optional[str] = None
    objectives: Optional[List[str]] = []
    base_path: Optional[Path] = None  # Path is allowed by arbitrary_types_allowed
    interaction_language: Optional[str] = None  # Should be validated
    documentation_language: Optional[str] = None  # Should be validated
    taxonomy_version: str = "1.0"  # Default or fetched from config?

    # MECE Taxonomy Selection
    platform_taxonomy: Optional[str] = None
    domain_taxonomy: Optional[str] = None
    size_taxonomy: Optional[str] = None
    compliance_taxonomy: Optional[str] = None
    lifecycle_taxonomy: Optional[str] = None

    # Custom taxonomy configuration
    custom_taxonomy: Optional[Dict[str, Any]] = {}

    # Validation status (optional, can be populated later)
    taxonomy_validation: Optional[Dict[str, Any]] = {}

    # KG / Audit Fields (NEW)
    created_by: Optional[str] = None
    modified_by: Optional[str] = None
    created_at: Optional[datetime] = None  # Allow None initially, set by adapter/mapper
    last_updated_at: Optional[datetime] = (
        None  # Allow None initially, set by adapter/mapper
    )


class ArtifactMeta(BaseModel):
    """Metadata for an artifact categorized according to the MECE taxonomy"""

    id: uuid.UUID = Field(default_factory=uuid.uuid4)
    name: str
    bucket: Bucket
    path: Path  # Relative path from project root
    created_at: datetime.datetime = None
    updated_at: datetime.datetime = None
    created_by: Optional[str] = None
    modified_by: Optional[str] = None
    status: DocumentStatus = DocumentStatus.PENDING

    def __init__(self, **data: Any):
        super().__init__(**data)
        if not time_service:
            raise RuntimeError("TimeService not initialized")

        now = time_service.get_current_time()
        if self.created_at is None:
            self.created_at = now
        if self.updated_at is None:
            self.updated_at = now

        if self.created_by is not None and self.modified_by is None:
            self.modified_by = self.created_by

    def update_timestamp(self, modifier: Optional[str] = None):
        if not time_service:
            raise RuntimeError("TimeService not initialized")
        self.updated_at = time_service.get_current_time()
        if modifier:
            self.modified_by = modifier

    def update_status(self, status: DocumentStatus, modifier: Optional[str] = None):
        self.status = status
        self.update_timestamp(modifier=modifier)


class ProjectMemory(BaseModel):
    project_info: ProjectInfo
    documents: Dict[str, ProjectDocument] = {}  # Dict key is document name/path
    # New taxonomy-based structure
    taxonomy_version: str = "0.5"
    artifacts: Dict[Bucket, List[ArtifactMeta]] = Field(
        default_factory=lambda: {bucket: [] for bucket in Bucket}
    )
    # Consider adding: achievements, issues, decisions later?
    # Ensure KG/Audit fields from ProjectInfo are also present here if needed for direct mapping
    created_at: Optional[datetime.datetime] = None  # Set by __init__ or adapter
    last_updated_at: Optional[datetime.datetime] = None  # Set by __init__ or adapter
    created_by: Optional[str] = None  # Mirrors ProjectInfo, set by adapter
    modified_by: Optional[str] = None  # Mirrors ProjectInfo, set by adapter

    # Add the new taxonomy fields here directly if they belong to the ProjectMemory state
    # Or ensure they are loaded/accessed via metadata if that's the design
    platform_taxonomy: str
    domain_taxonomy: str
    size_taxonomy: str
    compliance_taxonomy: str  # Consider if this one could truly be optional sometimes?
    lifecycle_taxonomy: str  # Required field for lifecycle dimension
    custom_taxonomy: Optional[Dict[str, Any]] = Field(default_factory=dict)
    taxonomy_validation: Optional[Dict[str, Any]] = Field(default_factory=dict)

    model_config = {
        "arbitrary_types_allowed": True,  # Allow Path and other non-JSON types
        "validate_assignment": True,  # Validate when attributes are set
        "extra": "forbid",  # Forbid extra attributes not in the model
    }

    def __init__(self, **data):
        # Rename metadata to project_metadata if needed for backward compatibility
        if "metadata" in data and "project_metadata" not in data:
            data["project_metadata"] = data.pop("metadata")

        super().__init__(**data)
        if not time_service:
            raise RuntimeError("TimeService not initialized")
        if self.created_at is None:
            self.created_at = time_service.get_current_time()
        if self.last_updated_at is None:
            self.last_updated_at = time_service.get_current_time()

    def update_timestamp(self):
        if not time_service:
            raise RuntimeError("TimeService not initialized")
        self.last_updated_at = time_service.get_current_time()

    def get_document(self, name: str) -> Optional[ProjectDocument]:
        return self.documents.get(name)

    def update_document_status(self, name: str, status: DocumentStatus):
        doc = self.get_document(name)
        if doc:
            doc.status = status
            self.update_timestamp()
        else:
            # TODO: Decide error handling (log or raise?)
            # For now, just pass
            # Consider logging: logger.warning(
            #     f"Attempted to update status for non-existent doc: {name}"
            # )
            pass

    def add_document(self, doc: ProjectDocument):
        if doc.name not in self.documents:
            self.documents[doc.name] = doc
            self.update_timestamp()
        else:
            # TODO: Decide error handling (log or raise?)
            # For now, just pass
            # Consider logging: logger.warning(
            #     f"Attempted to add duplicate document: {doc.name}"
            # )
            pass

    # New methods for artifact management
    def get_artifact(self, bucket: Bucket, name: str) -> Optional[ArtifactMeta]:
        """Get an artifact by bucket and name"""
        for artifact in self.artifacts.get(bucket, []):
            if artifact.name == name:
                return artifact
        return None

    def get_artifact_by_path(self, path: Path) -> Optional[ArtifactMeta]:
        """Get an artifact by path, searching across all buckets"""
        path_str = str(path)
        for bucket_artifacts in self.artifacts.values():
            for artifact in bucket_artifacts:
                if str(artifact.path) == path_str:
                    return artifact
        return None

    def add_artifact(self, artifact: ArtifactMeta) -> bool:
        """Adds an artifact to the correct bucket, checking for path duplicates."""
        # Check if artifact with the same path already exists in any bucket
        for bucket_artifacts in self.artifacts.values():
            for existing_artifact in bucket_artifacts:
                if existing_artifact.path == artifact.path:
                    logger.warning(
                        f"Artifact with path {artifact.path} already exists."
                    )
                    return False  # Indicate artifact was not added

        if artifact.bucket not in self.artifacts:
            self.artifacts[artifact.bucket] = []
        self.artifacts[artifact.bucket].append(artifact)
        self._update_timestamp()
        return True  # Indicate artifact was added

    def update_artifact_status(
        self,
        bucket: Bucket,
        artifact_name: str,
        new_status: DocumentStatus,
        modifier: Optional[str] = None,
    ) -> bool:
        """Updates the status of a specific artifact."""
        artifact = self.get_artifact(bucket, artifact_name)
        if artifact:
            artifact.status = new_status
            artifact.updated_at = datetime.datetime.now(datetime.timezone.utc)
            if modifier:
                artifact.modified_by = modifier
            self._update_timestamp()
            return True
        return False

    def _update_timestamp(self):
        """Updates the last_updated_at timestamp."""
        self.last_updated_at = datetime.datetime.now(datetime.timezone.utc)

    def get_bucket_completion(self, bucket: Bucket) -> dict:
        """Get completion stats for a bucket"""
        artifacts = self.artifacts.get(bucket, [])
        total = len(artifacts)
        completed = sum(1 for a in artifacts if a.status == DocumentStatus.COMPLETED)
        in_progress = sum(
            1 for a in artifacts if a.status == DocumentStatus.IN_PROGRESS
        )
        pending = total - completed - in_progress

        return {
            "total": total,
            "completed": completed,
            "in_progress": in_progress,
            "pending": pending,
            "completion_percentage": (completed / total * 100) if total > 0 else 0,
        }

    def get_phase_completion(self, phase: str) -> dict:
        """Get completion stats for an entire phase"""
        phase_buckets = Bucket.get_phase_buckets(phase)

        total = 0
        completed = 0
        in_progress = 0

        for bucket in phase_buckets:
            stats = self.get_bucket_completion(bucket)
            total += stats["total"]
            completed += stats["completed"]
            in_progress += stats["in_progress"]

        pending = total - completed - in_progress

        return {
            "phase": phase,
            "buckets": len(phase_buckets),
            "total": total,
            "completed": completed,
            "in_progress": in_progress,
            "pending": pending,
            "completion_percentage": (completed / total * 100) if total > 0 else 0,
        }

================
File: paelladoc/domain/services/time_service.py
================
"""Time service for domain timestamp handling."""

from abc import ABC, abstractmethod
import datetime


class TimeService(ABC):
    """Abstract base class for time operations in the domain."""

    @abstractmethod
    def get_current_time(self) -> datetime.datetime:
        """Get current timestamp in UTC.

        Returns:
            datetime.datetime: Current time in UTC.
        """
        pass

    @abstractmethod
    def ensure_utc(self, dt: datetime.datetime) -> datetime.datetime:
        """Ensure a datetime is in UTC.

        If the datetime has no timezone info, assumes it's in UTC.

        Args:
            dt: Datetime to convert

        Returns:
            datetime.datetime: UTC datetime with timezone info
        """
        pass

================
File: paelladoc/taxonomies/platform/pwa.json
================
{
  "name": "pwa",
  "description": "Progressive Web Applications that combine web and native app capabilities",
  "characteristics": [
    "Works offline or with intermittent connectivity",
    "Installable on home screen",
    "Push notifications capability",
    "Responsive across all device types",
    "Uses service workers for background processing"
  ],
  "typical_frameworks": [
    "Workbox",
    "Lighthouse",
    "PWA Builder",
    "Angular PWA",
    "React PWA tools"
  ],
  "documentation_emphasis": [
    "Offline capability",
    "Service worker implementation",
    "App manifest configuration",
    "Installation flow",
    "Background sync strategies",
    "Push notification management"
  ]
}

================
File: paelladoc/taxonomies/platform/vscode-extension.json
================
{
  "name": "vscode-extension",
  "description": "Extensions for Visual Studio Code IDE that enhance developer workflows",
  "characteristics": [
    "Runs within VS Code environment",
    "Extends IDE functionality",
    "Targets developer workflows",
    "Uses VS Code extension API",
    "Packaged as VSIX files"
  ],
  "typical_frameworks": [
    "VS Code Extension API",
    "TypeScript",
    "Node.js",
    "WebView API",
    "Language Server Protocol"
  ],
  "documentation_emphasis": [
    "Command registration",
    "Extension activation events",
    "Configuration settings",
    "Keybinding integration",
    "UI contributions",
    "API usage examples"
  ]
}

================
File: paelladoc/taxonomies/platform/flutter.json
================
{
  "name": "flutter",
  "description": "Cross-platform applications built using Google's Flutter framework",
  "characteristics": [
    "Single codebase for multiple platforms (iOS, Android, web, desktop)",
    "Written in Dart programming language",
    "Widget-based UI development",
    "Hot reload development workflow",
    "Native-like performance with custom rendering engine"
  ],
  "typical_frameworks": [
    "Flutter SDK",
    "Provider/Riverpod",
    "Bloc/Cubit",
    "GetX",
    "Flutter Hooks",
    "Go Router",
    "Firebase Flutter SDK"
  ],
  "documentation_emphasis": [
    "Widget hierarchy and composition",
    "State management approaches",
    "Platform-specific adaptations",
    "Performance optimization",
    "Plugin integration",
    "Navigation patterns",
    "Deployment for multiple platforms"
  ]
}

================
File: paelladoc/taxonomies/platform/browser-extension.json
================
{
  "name": "browser-extension",
  "description": "Extensions for web browsers like Chrome, Firefox, Edge, and Safari that enhance browsing experience",
  "characteristics": [
    "Runs within browser environment",
    "Extends browser functionality",
    "Accesses web content",
    "Uses browser extension APIs",
    "Distributed through browser extension stores",
    "Subject to browser security model"
  ],
  "typical_frameworks": [
    "WebExtension API",
    "Chrome Extension API",
    "JavaScript/TypeScript",
    "React/Vue/Svelte for UI",
    "Browser storage APIs",
    "Content script patterns",
    "Message passing architecture"
  ],
  "documentation_emphasis": [
    "Manifest configuration",
    "Permission model",
    "Content script injection",
    "Background script patterns",
    "Cross-browser compatibility",
    "Extension lifecycle management",
    "Store submission guidelines"
  ]
}

================
File: paelladoc/taxonomies/platform/cli-tool.json
================
{
  "name": "cli-tool",
  "description": "Command-line interface applications that run in terminal environments",
  "characteristics": [
    "Text-based interface",
    "Runs in terminal/console environments",
    "Typically process-oriented or automation-focused",
    "May be installed globally or project-specific",
    "Often follows Unix philosophy (do one thing well)"
  ],
  "typical_frameworks": [
    "Python Click/Typer",
    "Node.js Commander/Yargs",
    "Go Cobra",
    "Rust Clap",
    "Ruby Thor",
    "Bash/Shell scripting",
    "PowerShell modules"
  ],
  "documentation_emphasis": [
    "Command syntax and options",
    "Input/output formats",
    "Installation instructions",
    "Configuration files",
    "Exit codes and error handling",
    "Shell completion",
    "Example usage patterns"
  ]
}

================
File: paelladoc/taxonomies/platform/backend-service.json
================
{
  "name": "backend-service",
  "description": "Server-side applications and services that provide APIs, process data, or perform business logic",
  "characteristics": [
    "Runs on server infrastructure",
    "No direct user interface",
    "Exposes APIs or processes data",
    "Scales horizontally or vertically",
    "May run as containers, serverless functions, or traditional servers"
  ],
  "typical_frameworks": [
    "Node.js/Express",
    "Django/Flask",
    "Spring Boot",
    "ASP.NET Core",
    "Ruby on Rails",
    "Laravel",
    "FastAPI"
  ],
  "documentation_emphasis": [
    "API specifications (REST, GraphQL, gRPC)",
    "Database schema design",
    "Authentication and authorization",
    "Performance and scalability",
    "Deployment architecture",
    "Error handling and logging",
    "Service integration patterns"
  ]
}

================
File: paelladoc/taxonomies/platform/react-native.json
================
{
  "name": "react-native",
  "description": "Cross-platform mobile applications built using Meta's React Native framework",
  "characteristics": [
    "Single JavaScript/TypeScript codebase for iOS and Android",
    "React-based component model",
    "Bridge to native components",
    "Uses native rendering",
    "Hot reloading for development"
  ],
  "typical_frameworks": [
    "React Native CLI/Expo",
    "Redux/MobX",
    "React Navigation",
    "React Native Paper",
    "React Native Reanimated",
    "React Native Gesture Handler",
    "React Native Firebase"
  ],
  "documentation_emphasis": [
    "Component architecture",
    "Native module integration",
    "Navigation implementation",
    "Performance optimization",
    "Platform-specific code",
    "App store deployment",
    "Upgrade and maintenance strategies"
  ]
}

================
File: paelladoc/taxonomies/platform/android-native.json
================
{
  "name": "android-native",
  "description": "Native mobile applications developed specifically for Android devices",
  "characteristics": [
    "Runs on Android phones, tablets, and other devices",
    "Written in Kotlin or Java",
    "Uses Android SDK and Jetpack",
    "Direct access to device hardware and sensors",
    "Distributed through Google Play Store"
  ],
  "typical_frameworks": [
    "AndroidX",
    "Jetpack Compose",
    "Room",
    "WorkManager",
    "Navigation",
    "CameraX",
    "Material Design Components"
  ],
  "documentation_emphasis": [
    "Activity and fragment lifecycle",
    "Material Design implementation",
    "Permissions model",
    "Background processing",
    "Device fragmentation handling",
    "Data storage options",
    "Google Play policies"
  ]
}

================
File: paelladoc/taxonomies/platform/macos.json
================
{
  "name": "macos",
  "description": "Desktop applications developed specifically for Apple macOS operating system",
  "characteristics": [
    "Runs on Mac computers (MacBook, iMac, Mac mini, Mac Pro)",
    "Written in Swift or Objective-C",
    "Uses macOS frameworks and AppKit/SwiftUI",
    "Desktop-oriented user experience",
    "Distributed through Mac App Store or direct download"
  ],
  "typical_frameworks": [
    "AppKit",
    "SwiftUI for macOS",
    "Combine",
    "Core Data",
    "Core Animation",
    "Metal",
    "AVFoundation"
  ],
  "documentation_emphasis": [
    "Human Interface Guidelines compliance",
    "Window and view management",
    "File system integration",
    "Menu bar and dock integration",
    "macOS sandboxing requirements",
    "Mac App Store submission",
    "Apple Silicon optimization"
  ]
}

================
File: paelladoc/taxonomies/platform/web-frontend.json
================
{
  "name": "web-frontend",
  "description": "Web-based frontend applications including single-page applications, progressive web apps, and static websites",
  "characteristics": [
    "Runs in web browsers",
    "Uses HTML, CSS, and JavaScript/TypeScript",
    "May use frameworks like React, Angular, Vue, etc.",
    "Delivered via HTTP/HTTPS"
  ],
  "typical_frameworks": [
    "React",
    "Angular",
    "Vue",
    "Svelte",
    "Next.js"
  ],
  "documentation_emphasis": [
    "Component architecture",
    "State management",
    "API integration",
    "Responsive design",
    "Browser compatibility",
    "Performance optimization"
  ],
  "buckets": [
    "Generate::SupportingElements",
    "Generate::CoreFunctionality",
    "Generate::Validation",
    "Generate::Infrastructure",
    "Elaborate::IdeationAndDesign",
    "Elaborate::SpecificationAndPlanning",
    "Generate::Security"
  ]
}

================
File: paelladoc/taxonomies/platform/desktop-app.json
================
{
  "name": "desktop-app",
  "description": "Cross-platform desktop applications for Windows, macOS, and Linux",
  "characteristics": [
    "Runs on multiple desktop operating systems",
    "Native-like UI and performance",
    "Access to file system and system APIs",
    "May use web technologies (Electron) or native frameworks",
    "Distributed via installers, app stores, or direct download"
  ],
  "typical_frameworks": [
    "Electron",
    "Qt",
    "Tauri",
    ".NET MAUI",
    "JavaFX",
    "wxWidgets",
    "Avalonia UI"
  ],
  "documentation_emphasis": [
    "Installation and setup procedures",
    "Platform-specific considerations",
    "Resource management",
    "File system operations",
    "UI/UX consistency across platforms",
    "Packaging and distribution",
    "Auto-update mechanisms"
  ]
}

================
File: paelladoc/taxonomies/platform/ios-native.json
================
{
  "name": "ios-native",
  "description": "Native mobile applications developed specifically for Apple iOS devices",
  "characteristics": [
    "Runs on iOS devices (iPhone, iPad, iPod touch)",
    "Written in Swift or Objective-C",
    "Uses iOS SDK and frameworks",
    "Direct access to device hardware",
    "Distributed through App Store"
  ],
  "typical_frameworks": [
    "UIKit",
    "SwiftUI",
    "Combine",
    "CoreData",
    "ARKit",
    "HealthKit",
    "CoreML"
  ],
  "documentation_emphasis": [
    "UI/UX design guidelines",
    "App lifecycle management",
    "Data persistence strategies",
    "Permission handling",
    "App Store submission process",
    "Privacy considerations",
    "Platform-specific optimizations"
  ]
}

================
File: paelladoc/taxonomies/lifecycle/mvp.json
================
{
  "name": "mvp",
  "description": "Minimum Viable Product phase focused on delivering core functionality for initial market validation",
  "characteristics": [
    "Core functionality only",
    "Production-ready quality",
    "Focused on user value",
    "Minimal feature set",
    "Designed for feedback collection",
    "Releasable to early users"
  ],
  "documentation_emphasis": [
    "User onboarding",
    "Core feature documentation",
    "Known limitations",
    "Roadmap for future features",
    "Feedback collection mechanisms",
    "Support processes"
  ],
  "typical_artifacts": [
    "User guides for core features",
    "Basic API documentation",
    "Setup instructions",
    "System architecture overview",
    "Feedback collection plan",
    "Analytics implementation"
  ],
  "buckets": [
    "Initiate::CoreSetup",
    "Initiate::InitialProductDocs",
    "Elaborate::DiscoveryAndResearch",
    "Elaborate::IdeationAndDesign", 
    "Elaborate::SpecificationAndPlanning",
    "Generate::CoreFunctionality",
    "Generate::Validation",
    "Generate::SupportingElements", 
    "Generate::Deployment"
  ]
}

================
File: paelladoc/taxonomies/lifecycle/growth.json
================
{
  "name": "growth",
  "description": "Growth phase focused on expanding features, users, and market reach based on validated product-market fit",
  "characteristics": [
    "Expanding feature set",
    "Growing user base",
    "Scaling infrastructure",
    "Feature optimization based on data",
    "Increasing team size",
    "Process formalization"
  ],
  "documentation_emphasis": [
    "Comprehensive user documentation",
    "Feature-specific guides",
    "Integration documentation",
    "Performance best practices",
    "Administration guides",
    "Developer onboarding"
  ],
  "typical_artifacts": [
    "Complete product documentation",
    "Advanced use case guides",
    "Integration examples",
    "API reference documentation",
    "Technical architecture documentation",
    "Operational runbooks"
  ]
}

================
File: paelladoc/taxonomies/lifecycle/poc.json
================
{
  "name": "poc",
  "description": "Proof of Concept phase where the focus is on demonstrating technical feasibility",
  "characteristics": [
    "Limited scope implementation",
    "Focus on technical feasibility",
    "Quick, iterative development",
    "Minimal concern for production quality",
    "Experimental approach",
    "Time-constrained exploration"
  ],
  "documentation_emphasis": [
    "Technical discovery findings",
    "Implementation insights",
    "Considered alternatives",
    "Technical limitations",
    "Next steps recommendations",
    "Success criteria evaluation"
  ],
  "typical_artifacts": [
    "Demo application",
    "Technical finding report",
    "Experiment results",
    "Technology evaluation",
    "Architecture sketches",
    "Decision points"
  ]
}

================
File: paelladoc/taxonomies/lifecycle/legacy.json
================
{
  "name": "legacy",
  "description": "Legacy phase where the product is maintained but no longer actively developed with new features",
  "characteristics": [
    "Maintenance-only development",
    "Established user base relying on stability",
    "Older technology stack",
    "Limited active development",
    "Focus on security patches and critical fixes",
    "Potential replacement planning"
  ],
  "documentation_emphasis": [
    "System preservation knowledge",
    "Maintenance procedures",
    "Security update processes",
    "Troubleshooting guides",
    "Migration paths",
    "Knowledge retention"
  ],
  "typical_artifacts": [
    "Maintenance handbooks",
    "System architecture documentation",
    "Known issues catalog",
    "Component dependencies",
    "Migration planning documents",
    "Historical change logs"
  ]
}

================
File: paelladoc/taxonomies/lifecycle/mature.json
================
{
  "name": "mature",
  "description": "Mature phase with established market presence, focusing on optimization, stability, and refinement",
  "characteristics": [
    "Complete feature set",
    "Focus on stability and reliability",
    "Optimization for performance",
    "Established user base",
    "Well-defined processes",
    "Industry recognition"
  ],
  "documentation_emphasis": [
    "Enterprise integration scenarios",
    "Performance tuning guides",
    "Advanced customization",
    "Security hardening",
    "Compliance documentation",
    "Migration and upgrade guides"
  ],
  "typical_artifacts": [
    "Complete reference documentation",
    "Best practices guides",
    "Case studies",
    "Certification materials",
    "Performance benchmarks",
    "Governance frameworks"
  ]
}

================
File: paelladoc/taxonomies/compliance/ccpa.json
================
{
  "name": "ccpa",
  "description": "Projects requiring compliance with California Consumer Privacy Act for handling California residents' data",
  "key_requirements": [
    "Consumer right to know",
    "Consumer right to delete",
    "Consumer right to opt-out of sale",
    "Non-discrimination protection",
    "Data inventory and mapping",
    "Privacy notice requirements",
    "Verification procedures"
  ],
  "documentation_requirements": [
    "Privacy policy",
    "Data subject request procedures",
    "Opt-out mechanisms",
    "Data inventory",
    "Employee training materials",
    "Vendor contract provisions",
    "Verification method documentation"
  ],
  "implementation_patterns": [
    "Data inventory systems",
    "Consumer request handling workflows",
    "Consent management",
    "Sale opt-out mechanisms",
    "Identity verification",
    "Response tracking",
    "Request handling automation"
  ]
}

================
File: paelladoc/taxonomies/compliance/none.json
================
{
  "name": "none",
  "description": "Projects without specific regulatory compliance requirements",
  "characteristics": [
    "Standard security best practices",
    "No regulated data handling",
    "No industry-specific requirements",
    "General privacy considerations",
    "Standard data protection measures"
  ],
  "documentation_recommendations": [
    "Basic privacy policy",
    "Data handling guidelines",
    "Security overview",
    "User data management",
    "Cookie policy (if applicable)"
  ],
  "implementation_patterns": [
    "Standard authentication",
    "Basic logging",
    "Common encryption practices",
    "General data protection",
    "Regular security updates"
  ]
}

================
File: paelladoc/taxonomies/compliance/gdpr.json
================
{
  "name": "gdpr",
  "description": "Projects requiring compliance with the General Data Protection Regulation for EU data subjects",
  "key_requirements": [
    "Privacy by design and default",
    "Legal basis for processing",
    "Data subject rights fulfillment",
    "Data breach notification",
    "Records of processing activities",
    "Data Protection Impact Assessments"
  ],
  "documentation_requirements": [
    "Privacy policy",
    "Data processing agreements",
    "Data retention policy",
    "Consent management documentation",
    "Data subject request procedures",
    "Data breach response plan"
  ],
  "implementation_patterns": [
    "Data minimization",
    "Pseudonymization/anonymization",
    "Consent management",
    "Data portability mechanisms",
    "Right to be forgotten implementation",
    "Data inventory management"
  ],
  "buckets": [
    "Generate::Privacy",
    "Generate::Security",
    "Generate::Compliance",
    "Generate::DataPipelines",
    "Govern::StandardsMethodologies",
    "Elaborate::SpecificationAndPlanning"
  ]
}

================
File: paelladoc/taxonomies/compliance/fedramp.json
================
{
  "name": "fedramp",
  "description": "Projects requiring compliance with Federal Risk and Authorization Management Program for US government cloud services",
  "key_requirements": [
    "NIST 800-53 security controls",
    "Authorization boundary definition",
    "Continuous monitoring",
    "Independent assessment",
    "Plan of Action and Milestones",
    "Incident response capabilities",
    "Categorized security impact levels"
  ],
  "documentation_requirements": [
    "System Security Plan (SSP)",
    "Security Assessment Report (SAR)",
    "Plan of Action and Milestones (POA&M)",
    "Control implementation evidence",
    "Continuous monitoring plan",
    "Incident response plan",
    "Configuration management plan"
  ],
  "implementation_patterns": [
    "Control inheritance documentation",
    "Boundary diagrams",
    "Security control implementation",
    "Vulnerability scanning",
    "Penetration testing",
    "Configuration management",
    "Separation of duties"
  ]
}

================
File: paelladoc/taxonomies/compliance/hipaa.json
================
{
  "name": "hipaa",
  "description": "Projects requiring compliance with the Health Insurance Portability and Accountability Act for healthcare data in the US",
  "key_requirements": [
    "Privacy Rule compliance",
    "Security Rule implementation",
    "Breach Notification procedures",
    "Business Associate Agreements",
    "Administrative safeguards",
    "Technical safeguards",
    "Physical safeguards"
  ],
  "documentation_requirements": [
    "Notice of Privacy Practices",
    "Security Risk Assessment",
    "Policies and procedures documentation",
    "Breach notification protocol",
    "Workforce training materials",
    "Authorization forms",
    "Audit controls documentation"
  ],
  "implementation_patterns": [
    "Access controls",
    "Audit logs",
    "Encryption of PHI",
    "Secure transmission protocols",
    "Authentication mechanisms",
    "Disaster recovery procedures",
    "De-identification techniques"
  ]
}

================
File: paelladoc/taxonomies/compliance/soc2.json
================
{
  "name": "soc2",
  "description": "Projects requiring compliance with SOC 2 for security, availability, processing integrity, confidentiality, and privacy controls",
  "key_requirements": [
    "Security controls",
    "Availability management",
    "Processing integrity",
    "Confidentiality practices",
    "Privacy protection",
    "Risk management",
    "Vendor management"
  ],
  "documentation_requirements": [
    "Control documentation",
    "Security policies and procedures",
    "Incident response plan",
    "Business continuity plan",
    "System description documentation",
    "Risk assessment methodology",
    "Evidence collection procedures"
  ],
  "implementation_patterns": [
    "Access control matrices",
    "Continuous monitoring",
    "Change management workflows",
    "Encryption standards",
    "Logging and alerting",
    "Disaster recovery testing",
    "Employee training programs"
  ]
}

================
File: paelladoc/taxonomies/compliance/sox.json
================
{
  "name": "sox",
  "description": "Projects requiring compliance with Sarbanes-Oxley Act for financial reporting and corporate governance",
  "key_requirements": [
    "Internal controls over financial reporting",
    "Audit trail maintenance",
    "Change management controls",
    "Data integrity verification",
    "Segregation of duties",
    "Access control documentation",
    "Management certification of controls"
  ],
  "documentation_requirements": [
    "Control documentation",
    "Process flow diagrams",
    "Risk assessment documentation",
    "Testing procedures",
    "Remediation plans",
    "Audit committee reports",
    "Executive certification evidence"
  ],
  "implementation_patterns": [
    "Automated control monitoring",
    "Comprehensive access logging",
    "System change controls",
    "Approval workflows",
    "Financial data validation",
    "Reconciliation processes",
    "Segregated system access"
  ]
}

================
File: paelladoc/taxonomies/compliance/pci-dss.json
================
{
  "name": "pci-dss",
  "description": "Projects requiring compliance with Payment Card Industry Data Security Standard for handling payment card data",
  "key_requirements": [
    "Secure network and systems",
    "Cardholder data protection",
    "Vulnerability management",
    "Strong access control",
    "Network monitoring and testing",
    "Information security policy",
    "Regular security assessments"
  ],
  "documentation_requirements": [
    "Security policy documentation",
    "Cardholder data flow diagrams",
    "Network architecture diagrams",
    "Risk assessment procedures",
    "Incident response plan",
    "Data retention policy",
    "Encryption implementation details"
  ],
  "implementation_patterns": [
    "Tokenization",
    "End-to-end encryption",
    "Network segmentation",
    "Strong authentication",
    "Regular security scanning",
    "Audit logging",
    "Secure coding practices"
  ]
}

================
File: paelladoc/taxonomies/compliance/accessibility.json
================
{
  "name": "accessibility",
  "description": "Projects requiring compliance with accessibility standards like WCAG, ADA, or Section 508",
  "key_requirements": [
    "Perceivable content",
    "Operable interfaces",
    "Understandable information",
    "Robust content compatibility",
    "Keyboard accessibility",
    "Screen reader compatibility",
    "Color contrast compliance"
  ],
  "documentation_requirements": [
    "Accessibility conformance report",
    "VPAT (Voluntary Product Accessibility Template)",
    "User accessibility guide",
    "Keyboard navigation documentation",
    "Alternative text guidelines",
    "Color usage policies",
    "Accessibility testing procedures"
  ],
  "implementation_patterns": [
    "Semantic HTML",
    "ARIA attributes",
    "Focus management",
    "Alternative text for images",
    "Keyboard navigation",
    "Color contrast ratios",
    "Responsive design patterns"
  ]
}

================
File: paelladoc/taxonomies/compliance/iso27001.json
================
{
  "name": "iso27001",
  "description": "Projects requiring compliance with ISO/IEC 27001 Information Security Management System standard",
  "key_requirements": [
    "Information security policy",
    "Risk assessment and treatment",
    "Security controls implementation",
    "Information asset management",
    "Access control framework",
    "Cryptography implementation",
    "Operational security"
  ],
  "documentation_requirements": [
    "Statement of Applicability",
    "Risk treatment plan",
    "Information security policy",
    "Asset inventory",
    "Internal audit documentation",
    "Management review records",
    "Incident management procedures"
  ],
  "implementation_patterns": [
    "ISMS framework implementation",
    "Security control objectives mapping",
    "Documented risk methodology",
    "Security awareness programs",
    "Continuous monitoring and measurement",
    "Management system integration",
    "Plan-Do-Check-Act cycle"
  ]
}

================
File: paelladoc/taxonomies/size/startup.json
================
{
  "name": "startup",
  "description": "Early-stage projects focused on rapid iteration, market validation, and growth",
  "characteristics": [
    "Evolving feature set",
    "Quick adaptation to market feedback",
    "Scalability considerations from early stages",
    "Minimal viable infrastructure",
    "Experimental approach",
    "Focused on core value proposition"
  ],
  "documentation_scope": [
    "Technical specification essentials",
    "Product roadmap",
    "Key API documentation",
    "Onboarding for new team members",
    "Architecture decision records",
    "Feature flag management",
    "Analytics implementation"
  ],
  "typical_documentation_volume": "Low to medium (30-100 pages)",
  "maintenance_level": "Adaptive with frequent changes"
}

================
File: paelladoc/taxonomies/size/enterprise.json
================
{
  "name": "enterprise",
  "description": "Large-scale projects intended for organizational use with many users and complex requirements",
  "characteristics": [
    "Large number of users (1000+)",
    "Complex multi-tier architecture",
    "High scalability requirements",
    "Extensive feature set",
    "Sophisticated infrastructure",
    "Multiple integration points"
  ],
  "documentation_scope": [
    "Comprehensive user manuals",
    "Technical architecture documentation",
    "Deployment and operations guides",
    "Security documentation",
    "API reference",
    "Integration specifications",
    "Training materials",
    "Governance documentation"
  ],
  "typical_documentation_volume": "High (200+ pages)",
  "maintenance_level": "High with formal processes",
  "buckets": [
    "Govern::CoreSystem",
    "Govern::StandardsMethodologies",
    "Govern::VerificationValidation",
    "Govern::MemoryTemplates",
    "Generate::Deployment",
    "Generate::Security",
    "Generate::SupportingElements",
    "Elaborate::SpecificationAndPlanning",
    "Generate::Infrastructure",
    "Generate::EnterpriseIntegration",
    "Generate::Compliance"
  ]
}

================
File: paelladoc/taxonomies/size/open-source-community.json
================
{
  "name": "open-source-community",
  "description": "Open source projects maintained by a distributed community of contributors",
  "characteristics": [
    "Distributed development team",
    "Public source code",
    "Community governance",
    "Volunteer contributors",
    "Public issue tracking",
    "Transparent development process",
    "Variable team size and activity levels"
  ],
  "documentation_scope": [
    "Project vision and goals",
    "Contributor guidelines",
    "Code of conduct",
    "Architecture overview",
    "Development setup",
    "API documentation",
    "User tutorials",
    "Release notes and roadmap"
  ],
  "typical_documentation_volume": "Medium to high (varies greatly)",
  "maintenance_level": "Community-driven, often cyclical based on contributor availability"
}

================
File: paelladoc/taxonomies/size/micro.json
================
{
  "name": "micro",
  "description": "Extremely small projects with minimal scope, typically for individual use or specific utility",
  "characteristics": [
    "Single developer",
    "Extremely focused functionality",
    "Minimal features (often single-purpose)",
    "Simple architecture",
    "Very limited scope",
    "Short development timeframe"
  ],
  "documentation_scope": [
    "Basic README",
    "Installation instructions",
    "Usage examples",
    "Simple API reference (if applicable)",
    "Known limitations"
  ],
  "typical_documentation_volume": "Minimal (under 10 pages)",
  "maintenance_level": "Light, ad-hoc maintenance"
}

================
File: paelladoc/taxonomies/size/personal.json
================
{
  "name": "personal",
  "description": "Small-scale projects intended for individual use or small teams",
  "characteristics": [
    "Limited number of users (1-5)",
    "Simple architecture",
    "Low to medium complexity",
    "Limited feature set",
    "Minimal infrastructure requirements"
  ],
  "documentation_scope": [
    "Basic setup instructions",
    "Core functionality overview",
    "Simple troubleshooting guide",
    "Development setup",
    "Configuration options"
  ],
  "typical_documentation_volume": "Low (under 50 pages)",
  "maintenance_level": "Low to moderate"
}

================
File: paelladoc/taxonomies/size/team.json
================
{
  "name": "team",
  "description": "Medium-scale projects intended for team or department use",
  "characteristics": [
    "Moderate number of users (5-1000)",
    "Modular architecture",
    "Medium complexity",
    "Balanced feature set",
    "Standard infrastructure requirements",
    "Some integration needs"
  ],
  "documentation_scope": [
    "User guides",
    "Administrative documentation",
    "Setup and configuration",
    "Common workflows",
    "Basic architectural overview",
    "Integration guidelines",
    "Maintenance procedures"
  ],
  "typical_documentation_volume": "Medium (50-200 pages)",
  "maintenance_level": "Moderate with regular updates"
}

================
File: paelladoc/taxonomies/domain/social-media.json
================
{
  "name": "social-media",
  "description": "Applications focused on social networking, content sharing, and community engagement",
  "key_features": [
    "User profile management",
    "Content creation and sharing",
    "Social connections/following",
    "Engagement mechanisms",
    "Content discovery",
    "Notification systems",
    "Privacy controls"
  ],
  "typical_components": [
    "News feeds",
    "Profile pages",
    "Messaging systems",
    "Content recommendation engines",
    "Notification centers",
    "Community management tools",
    "Content moderation systems"
  ],
  "documentation_emphasis": [
    "User experience flows",
    "Content moderation policies",
    "Privacy and data usage",
    "Engagement metrics",
    "Content distribution algorithms",
    "Social graph architecture",
    "Real-time updating mechanisms"
  ]
}

================
File: paelladoc/taxonomies/domain/productivity.json
================
{
  "name": "productivity",
  "description": "Applications for personal and team productivity, task management, and workflow optimization",
  "key_features": [
    "Task and project tracking",
    "Time management",
    "Document creation/editing",
    "Collaboration features",
    "Calendar and scheduling",
    "Note-taking and organization",
    "Process automation"
  ],
  "typical_components": [
    "Task boards/lists",
    "Calendar views",
    "Document editors",
    "Collaboration spaces",
    "Notification systems",
    "Search functionality",
    "Integration with productivity ecosystems"
  ],
  "documentation_emphasis": [
    "Workflow optimization",
    "Keyboard shortcuts and efficiency",
    "Data synchronization",
    "Collaboration patterns",
    "Offline functionality",
    "Integration capabilities",
    "Data import/export options"
  ]
}

================
File: paelladoc/taxonomies/domain/enterprise-management.json
================
{
  "name": "enterprise-management",
  "description": "Applications for business operations, enterprise resource planning, and organizational management",
  "key_features": [
    "Business process automation",
    "Resource allocation",
    "Organization management",
    "Financial planning",
    "Supply chain tracking",
    "Workforce management",
    "Enterprise reporting"
  ],
  "typical_components": [
    "Administrative dashboards",
    "Process workflow engines",
    "Resource management interfaces",
    "Business intelligence tools",
    "Reporting systems",
    "Integration hubs",
    "Compliance monitoring"
  ],
  "documentation_emphasis": [
    "Business process modeling",
    "System integration architecture",
    "Data governance",
    "Role-based permissions",
    "Audit and compliance reporting",
    "Configuration management",
    "Enterprise deployment considerations"
  ]
}

================
File: paelladoc/taxonomies/domain/iot-embedded.json
================
{
  "name": "iot-embedded",
  "description": "Applications for Internet of Things, embedded systems, and connected devices",
  "key_features": [
    "Device management",
    "Sensor data processing",
    "Remote control capabilities",
    "Device monitoring",
    "Firmware updates",
    "Edge computing",
    "Connectivity management"
  ],
  "typical_components": [
    "Device dashboards",
    "Data visualization",
    "Device configuration interfaces",
    "Connectivity status monitors",
    "Automation rule engines",
    "Alert systems",
    "Historical data analysis"
  ],
  "documentation_emphasis": [
    "Device onboarding process",
    "Communication protocols",
    "Security and authentication",
    "Power management considerations",
    "Data collection architecture",
    "Over-the-air update procedures",
    "Integration with cloud services"
  ]
}

================
File: paelladoc/taxonomies/domain/education.json
================
{
  "name": "education",
  "description": "Applications for learning, teaching, educational administration, and academic management",
  "key_features": [
    "Course management",
    "Student assessment",
    "Learning material delivery",
    "Progress tracking",
    "Interactive learning",
    "Collaborative tools",
    "Educational analytics"
  ],
  "typical_components": [
    "Learning management systems",
    "Assessment engines",
    "Digital content libraries",
    "Student portals",
    "Instructor dashboards",
    "Discussion forums",
    "Certification management"
  ],
  "documentation_emphasis": [
    "Accessibility compliance",
    "Student data privacy",
    "Content structure",
    "Assessment methodologies",
    "Integration with educational tools",
    "Curriculum mapping",
    "Progress reporting"
  ]
}

================
File: paelladoc/taxonomies/domain/finance-banking.json
================
{
  "name": "finance-banking",
  "description": "Applications focused on financial services, banking operations, and money management",
  "key_features": [
    "Account management",
    "Transaction processing",
    "Financial reporting",
    "Investment tracking",
    "Risk assessment",
    "Fraud detection",
    "Regulatory compliance"
  ],
  "typical_components": [
    "Account dashboards",
    "Payment gateways",
    "Transaction ledgers",
    "Financial calculators",
    "Statement generators",
    "Notification systems",
    "Audit trails"
  ],
  "documentation_emphasis": [
    "Security architecture",
    "Transactional integrity",
    "Compliance requirements",
    "Data accuracy guarantees",
    "Integration with financial systems",
    "Reconciliation processes",
    "Audit procedures"
  ]
}

================
File: paelladoc/taxonomies/domain/entertainment.json
================
{
  "name": "entertainment",
  "description": "Applications for media consumption, gaming, streaming, and digital entertainment",
  "key_features": [
    "Content delivery",
    "Media playback",
    "User preferences",
    "Recommendation systems",
    "Subscription management",
    "Interactive experiences",
    "Digital rights management"
  ],
  "typical_components": [
    "Media libraries",
    "Player interfaces",
    "Content discovery systems",
    "User rating mechanisms",
    "Playlists/collections",
    "Streaming infrastructure",
    "Interactive content controls"
  ],
  "documentation_emphasis": [
    "Media format handling",
    "Streaming protocols",
    "Content licensing",
    "User experience optimization",
    "Performance for media delivery",
    "Content recommendation algorithms",
    "Cross-device experience consistency"
  ]
}

================
File: paelladoc/taxonomies/domain/healthcare.json
================
{
  "name": "healthcare",
  "description": "Applications for healthcare services, medical record management, and patient care",
  "key_features": [
    "Electronic health records",
    "Patient management",
    "Clinical workflows",
    "Medical imaging integration",
    "Prescription management",
    "Appointment scheduling",
    "Health monitoring"
  ],
  "typical_components": [
    "Patient portals",
    "Clinical dashboards",
    "Medical data visualization",
    "Health analytics",
    "Telemedicine interfaces",
    "Care plan management",
    "Health information exchange"
  ],
  "documentation_emphasis": [
    "Data privacy and security",
    "Regulatory compliance (HIPAA, etc.)",
    "Integration with medical systems",
    "Clinical workflows",
    "Data accuracy and validation",
    "Patient confidentiality",
    "Emergency access protocols"
  ]
}

================
File: paelladoc/taxonomies/domain/ecommerce.json
================
{
  "name": "ecommerce",
  "description": "Applications focused on online selling and purchasing of goods or services",
  "key_features": [
    "Product catalog management",
    "Shopping cart functionality",
    "Payment processing",
    "Order management",
    "Customer accounts",
    "Inventory management"
  ],
  "typical_components": [
    "Product listings",
    "Search and filtering",
    "Checkout process",
    "Payment gateway integration",
    "Order tracking",
    "User reviews and ratings"
  ],
  "documentation_emphasis": [
    "Product data modeling",
    "Shopping flow diagrams",
    "Payment security",
    "Order processing workflows",
    "Inventory synchronization",
    "Analytics integration"
  ]
}

================
File: paelladoc/taxonomies/domain/ai-ml.json
================
{
  "name": "ai-ml",
  "description": "Applications that utilize artificial intelligence and machine learning capabilities",
  "key_features": [
    "Model training pipelines",
    "Prediction services",
    "Data preprocessing",
    "Feature engineering",
    "Model evaluation",
    "Inference optimization"
  ],
  "typical_components": [
    "Data ingestion systems",
    "Training infrastructure",
    "Model registry",
    "Experiment tracking",
    "Feature stores",
    "Model serving endpoints"
  ],
  "documentation_emphasis": [
    "Data schemas and preprocessing",
    "Model architecture",
    "Training methodology",
    "Evaluation metrics",
    "Deployment strategies",
    "Monitoring and retraining"
  ],
  "buckets": [
    "Generate::DataPipelines",
    "Generate::CoreML",
    "Generate::ModelEvaluation",
    "Elaborate::DiscoveryAndResearch",
    "Generate::Infrastructure",
    "Generate::APIDesign",
    "Generate::Monitoring"
  ]
}

================
File: paelladoc/taxonomies/domain/cms.json
================
{
  "name": "cms",
  "description": "Content Management Systems for creating, managing, and publishing digital content",
  "key_features": [
    "Content modeling",
    "Editorial workflows",
    "Content versioning",
    "User role management",
    "Publishing controls",
    "Media management"
  ],
  "typical_components": [
    "Content editor interface",
    "Asset library",
    "Template system",
    "Content repository",
    "Search functionality",
    "Content delivery API"
  ],
  "documentation_emphasis": [
    "Content architecture",
    "Editorial guidelines",
    "Workflow diagrams",
    "Integration patterns",
    "Templating system",
    "Performance optimization"
  ]
}

================
File: tests/conftest.py
================
import pytest
from datetime import datetime, timezone, timedelta
from pathlib import Path
import sys

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.absolute()
sys.path.insert(0, str(project_root))

# Import TimeService components
from paelladoc.domain.services.time_service import TimeService
from paelladoc.domain.models.project import set_time_service


class MockTimeService(TimeService):
    """Mock time service for testing."""

    def __init__(self, fixed_time=None):
        """Initialize with optional fixed time."""
        self.fixed_time = fixed_time or datetime.now(timezone.utc)
        self.call_count = 0

    def get_current_time(self) -> datetime:
        """Get the mocked current time, incrementing by microseconds on each call."""
        # Increment call count
        self.call_count += 1

        # Return fixed time plus microseconds based on call count to ensure
        # timestamps are different when multiple calls happen in sequence
        return self.fixed_time + timedelta(microseconds=self.call_count)

    def ensure_utc(self, dt: datetime) -> datetime:
        """Ensure a datetime is in UTC."""
        if dt.tzinfo is None:
            return dt.replace(tzinfo=timezone.utc)
        return dt.astimezone(timezone.utc)


@pytest.fixture(scope="session", autouse=True)
def setup_time_service():
    """Set up the time service globally for all tests."""
    # Using a fixed time for consistent testing
    fixed_time = datetime(2025, 4, 20, 12, 0, 0, tzinfo=timezone.utc)
    mock_service = MockTimeService(fixed_time)
    set_time_service(mock_service)
    return mock_service

================
File: tests/README.md
================
# MCP Server Tests

This directory contains tests for the Paelladoc MCP server following hexagonal architecture principles. Tests are organized into three main categories:

## Test Structure

```
tests/
├── unit/            # Unit tests for individual components
│   └── test_tools.py  # Tests for MCP tools in isolation
├── integration/     # Integration tests for component interactions
│   └── test_server.py # Tests for server STDIO communication
└── e2e/             # End-to-end tests simulating real-world usage
    └── test_cursor_simulation.py # Simulates Cursor interaction
```

## Test Categories

1. **Unit Tests** (`unit/`)
   - Test individual functions/components in isolation
   - Don't require a running server
   - Fast to execute
   - Example: Testing the `ping()` function directly

2. **Integration Tests** (`integration/`)
   - Test interactions between components
   - Verify STDIO communication with the server
   - Example: Starting the server and sending/receiving messages

3. **End-to-End Tests** (`e2e/`)
   - Simulate real-world usage scenarios
   - Test the system as a whole
   - Example: Simulating how Cursor would interact with the server

## Running Tests

### Run All Tests

```bash
python -m unittest discover mcp_server/tests
```

### Run Tests by Category

```bash
# Unit tests only
python -m unittest discover mcp_server/tests/unit

# Integration tests only
python -m unittest discover mcp_server/tests/integration

# End-to-end tests only
python -m unittest discover mcp_server/tests/e2e
```

### Run a Specific Test File

```bash
python -m unittest mcp_server/tests/unit/test_tools.py
```

### Run a Specific Test Case

```bash
python -m unittest mcp_server.tests.unit.test_tools.TestToolsPing
```

### Run a Specific Test Method

```bash
python -m unittest mcp_server.tests.unit.test_tools.TestToolsPing.test_ping_returns_dict
```

## TDD Process

These tests follow the Test-Driven Development (TDD) approach:

1. **RED**: Write failing tests first
2. **GREEN**: Implement the minimal code to make tests pass
3. **REFACTOR**: Improve the code while keeping tests passing

## Adding New Tests

When adding new MCP tools:

1. Create unit tests for the tool's functionality
2. Add integration tests for the tool's STDIO communication
3. Update E2E tests to verify Cursor interaction with the tool

================
File: tests/unit/test_ping_tool.py
================
"""
Unit tests for Paelladoc MCP tools.

Following TDD approach - tests are written before implementation.
"""

import unittest
import sys
from pathlib import Path

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.absolute()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Import directly from the domain layer
from paelladoc.domain import core_logic


class TestPingTool(unittest.TestCase):
    """Unit tests for the ping tool following TDD methodology."""

    def test_ping_exists(self):
        """Test that the ping function exists."""
        self.assertTrue(
            hasattr(core_logic, "ping"),
            "The ping function does not exist in core_logic",
        )

    def test_ping_returns_dict(self):
        """Test that ping returns a dictionary."""
        result = core_logic.ping()
        self.assertIsInstance(result, dict, "ping should return a dictionary")

    def test_ping_has_required_fields(self):
        """Test that ping response has the required fields."""
        result = core_logic.ping()
        self.assertIn("status", result, "ping response should contain a 'status' field")
        self.assertIn(
            "message", result, "ping response should contain a 'message' field"
        )

    def test_ping_returns_expected_values(self):
        """Test that ping returns the expected values."""
        result = core_logic.ping()
        self.assertEqual(
            result["status"],
            "ok",
            f"ping status should be 'ok', got '{result['status']}'",
        )
        self.assertEqual(
            result["message"],
            "pong",
            f"ping message should be 'pong', got '{result['message']}'",
        )


if __name__ == "__main__":
    unittest.main()

================
File: tests/unit/config/test_database.py
================
"""Unit tests for database configuration module."""

import os
from pathlib import Path
import pytest

from paelladoc.config.database import (
    get_project_root,
    get_db_path,
    PRODUCTION_DB_PATH,
)


@pytest.fixture
def clean_env():
    """Remove relevant environment variables before each test."""
    # Store original values
    original_db_path = os.environ.get("PAELLADOC_DB_PATH")
    original_env = os.environ.get("PAELLADOC_ENV")

    # Remove variables
    if "PAELLADOC_DB_PATH" in os.environ:
        del os.environ["PAELLADOC_DB_PATH"]
    if "PAELLADOC_ENV" in os.environ:
        del os.environ["PAELLADOC_ENV"]

    yield

    # Restore original values
    if original_db_path is not None:
        os.environ["PAELLADOC_DB_PATH"] = original_db_path
    if original_env is not None:
        os.environ["PAELLADOC_ENV"] = original_env


def test_get_project_root():
    """Test that get_project_root returns a valid directory."""
    root = get_project_root()
    assert isinstance(root, Path)
    assert root.exists()
    assert root.is_dir()
    assert (root / "src").exists()
    assert (root / "src" / "paelladoc").exists()
    assert (root / "pyproject.toml").exists()


def test_get_db_path_with_env_var(clean_env):
    """Test that PAELLADOC_DB_PATH environment variable takes precedence."""
    custom_path = "/custom/path/db.sqlite"
    os.environ["PAELLADOC_DB_PATH"] = custom_path

    db_path = get_db_path()
    assert isinstance(db_path, Path)
    assert str(db_path) == custom_path


def test_get_db_path_production_default(clean_env):
    """Test that production mode uses home directory."""
    db_path = get_db_path()
    assert isinstance(db_path, Path)
    assert db_path == PRODUCTION_DB_PATH
    assert db_path.name == "memory.db"
    assert db_path.parent.name == ".paelladoc"
    assert db_path.parent.parent == Path.home()


def test_production_db_path_constant():
    """Test that PRODUCTION_DB_PATH is correctly set."""
    assert isinstance(PRODUCTION_DB_PATH, Path)
    assert PRODUCTION_DB_PATH.name == "memory.db"
    assert PRODUCTION_DB_PATH.parent.name == ".paelladoc"
    assert PRODUCTION_DB_PATH.parent.parent == Path.home()

================
File: tests/unit/adapters/output/filesystem/test_mcp_config_repository.py
================
"""Unit tests for FileSystemMCPConfigRepository."""

import pytest
from pathlib import Path
import json
from typing import Dict

from paelladoc.adapters.output.filesystem.mcp_config_repository import (
    FileSystemMCPConfigRepository,
)


@pytest.fixture
def mock_config_dir(tmp_path: Path) -> Path:
    """Create a temporary directory with mock MCP config."""
    config_dir = tmp_path / "config"
    config_dir.mkdir()
    return config_dir


@pytest.fixture
def mock_mcp_config(mock_config_dir: Path) -> Dict:
    """Create a mock MCP configuration file."""
    config = {
        "version": "1.0",
        "mcp_configuration": {
            "core": {
                "enabled": True,
                "linked_taxonomies": ["Initiate::CoreSetup"],
                "tools": ["core_help", "paella_init"],
            },
            "product": {
                "enabled": True,
                "linked_taxonomies": ["Elaborate::SpecificationAndPlanning"],
                "tools": ["core_manage_story", "core_manage_task"],
            },
            "disabled_mcp": {
                "enabled": False,
                "linked_taxonomies": ["Test::Disabled"],
                "tools": ["should_not_appear"],
            },
        },
    }

    config_file = mock_config_dir / "mcp_config.json"
    with open(config_file, "w") as f:
        json.dump(config, f)
    return config


@pytest.fixture
def repository(monkeypatch, mock_config_dir: Path) -> FileSystemMCPConfigRepository:
    """Create a repository instance with mocked config path."""
    repo = FileSystemMCPConfigRepository()
    monkeypatch.setattr(repo, "config_path", mock_config_dir / "mcp_config.json")
    return repo


@pytest.mark.asyncio
async def test_get_available_mcps(
    repository: FileSystemMCPConfigRepository, mock_mcp_config: Dict
):
    """Test that only enabled MCPs are returned."""
    mcps = await repository.get_available_mcps()
    assert len(mcps) == 2
    assert "core" in mcps
    assert "product" in mcps
    assert "disabled_mcp" not in mcps


@pytest.mark.asyncio
async def test_get_mcp_config(
    repository: FileSystemMCPConfigRepository, mock_mcp_config: Dict
):
    """Test retrieving specific MCP configuration."""
    config = await repository.get_mcp_config("core")
    assert config is not None
    assert config["enabled"] is True
    assert "core_help" in config["tools"]


@pytest.mark.asyncio
async def test_get_tools_for_taxonomy(
    repository: FileSystemMCPConfigRepository, mock_mcp_config: Dict
):
    """Test getting tools for a specific taxonomy bucket."""
    tools = await repository.get_tools_for_taxonomy("Initiate::CoreSetup")
    assert len(tools) == 2
    assert "core_help" in tools
    assert "paella_init" in tools


@pytest.mark.asyncio
async def test_get_enabled_mcps_for_project(
    repository: FileSystemMCPConfigRepository, mock_mcp_config: Dict
):
    """Test getting enabled MCPs based on project buckets."""
    mcps = await repository.get_enabled_mcps_for_project(
        ["Initiate::CoreSetup", "Elaborate::SpecificationAndPlanning"]
    )
    assert len(mcps) == 2
    assert "core" in mcps
    assert "product" in mcps


@pytest.mark.asyncio
async def test_no_config_file(repository: FileSystemMCPConfigRepository):
    """Test behavior when config file doesn't exist."""
    if repository.config_path.exists():
        repository.config_path.unlink()

    mcps = await repository.get_available_mcps()
    assert len(mcps) == 0


@pytest.mark.asyncio
async def test_invalid_config_file(
    repository: FileSystemMCPConfigRepository, mock_config_dir: Path
):
    """Test behavior with invalid JSON in config file."""
    with open(repository.config_path, "w") as f:
        f.write("invalid json")

    mcps = await repository.get_available_mcps()
    assert len(mcps) == 0


@pytest.mark.asyncio
async def test_cache_behavior(
    repository: FileSystemMCPConfigRepository, mock_mcp_config: Dict
):
    """Test that configuration is properly cached."""
    # First call should read from file
    mcps1 = await repository.get_available_mcps()

    # Delete the file
    repository.config_path.unlink()

    # Second call should use cache
    mcps2 = await repository.get_available_mcps()

    assert mcps1 == mcps2

================
File: tests/unit/application/utils/test_behavior_enforcer.py
================
"""
Unit tests for the BehaviorEnforcer utility.
"""

import unittest
import sys
from pathlib import Path
from typing import Set, Optional

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.parent.parent.absolute()
sys.path.insert(0, str(project_root))

# Module to test
from paelladoc.application.utils.behavior_enforcer import (
    BehaviorEnforcer,
    BehaviorViolationError,
)


# Mock context object for tests
class MockContext:
    def __init__(self, collected_params: Optional[Set[str]] = None):
        self.progress = {
            "collected_params": collected_params
            if collected_params is not None
            else set()
        }


class TestBehaviorEnforcer(unittest.TestCase):
    """Unit tests for the BehaviorEnforcer."""

    def setUp(self):
        self.tool_name = "test.tool"
        self.sequence = ["param1", "param2", "param3"]
        self.behavior_config = {"fixed_question_order": self.sequence}

    def test_enforce_no_config(self):
        """Test that enforcement passes if no behavior_config is provided."""
        try:
            BehaviorEnforcer.enforce(self.tool_name, None, MockContext(), {"arg": 1})
        except BehaviorViolationError:
            self.fail("Enforcement should pass when no config is given.")

    def test_enforce_no_fixed_order(self):
        """Test enforcement passes if 'fixed_question_order' is not in config."""
        config = {"other_rule": True}
        try:
            BehaviorEnforcer.enforce(
                self.tool_name, config, MockContext(), {"param1": "value"}
            )
        except BehaviorViolationError:
            self.fail(
                "Enforcement should pass when fixed_question_order is not defined."
            )

    def test_enforce_no_context_or_args(self):
        """Test enforcement passes (logs warning) if context or args are missing."""
        # Note: Current implementation returns None (passes), might change behavior later.
        try:
            BehaviorEnforcer.enforce(self.tool_name, self.behavior_config, None, None)
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, MockContext(), None
            )
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, None, {"param1": "a"}
            )
        except BehaviorViolationError:
            self.fail("Enforcement should pass when context or args are missing.")

    def test_enforce_no_new_params_provided(self):
        """Test enforcement passes if no *new* parameters are provided."""
        ctx = MockContext(collected_params={"param1"})
        # Providing only already collected param
        provided_args = {"param1": "new_value", "param2": None}
        try:
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, ctx, provided_args
            )
        except BehaviorViolationError as e:
            self.fail(
                f"Enforcement should pass when only old params are provided. Raised: {e}"
            )

    def test_enforce_correct_first_param(self):
        """Test enforcement passes when the correct first parameter is provided."""
        ctx = MockContext()
        provided_args = {"param1": "value1"}
        try:
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, ctx, provided_args
            )
        except BehaviorViolationError as e:
            self.fail(f"Enforcement failed for correct first param. Raised: {e}")

    def test_enforce_correct_second_param(self):
        """Test enforcement passes when the correct second parameter is provided."""
        ctx = MockContext(collected_params={"param1"})
        provided_args = {
            "param1": "value1",
            "param2": "value2",
        }  # param1 is old, param2 is new
        try:
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, ctx, provided_args
            )
        except BehaviorViolationError as e:
            self.fail(f"Enforcement failed for correct second param. Raised: {e}")

    def test_enforce_incorrect_first_param(self):
        """Test enforcement fails when the wrong first parameter is provided."""
        ctx = MockContext()
        provided_args = {"param2": "value2"}  # Should be param1
        with self.assertRaisesRegex(
            BehaviorViolationError,
            "Expected next: 'param1'. Got unexpected new parameter: 'param2'",
        ):
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, ctx, provided_args
            )

    def test_enforce_incorrect_second_param(self):
        """Test enforcement fails when the wrong second parameter is provided."""
        ctx = MockContext(collected_params={"param1"})
        provided_args = {"param1": "val1", "param3": "value3"}  # Should be param2
        with self.assertRaisesRegex(
            BehaviorViolationError,
            "Expected next: 'param2'. Got unexpected new parameter: 'param3'",
        ):
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, ctx, provided_args
            )

    def test_enforce_multiple_new_params_fails(self):
        """Test enforcement fails when multiple new parameters are provided at once."""
        ctx = MockContext()
        provided_args = {"param1": "value1", "param2": "value2"}  # Both are new
        # Adjust regex to match the more detailed error message
        expected_regex = (
            r"Tool 'test.tool' expects parameters sequentially. "
            r"Expected next: 'param1'. "
            # Use regex to handle potential set order variations {'param1', 'param2'} or {'param2', 'param1'}
            r"Provided multiple new parameters: {('param1', 'param2'|'param2', 'param1')}. "
            r"Collected so far: set\(\)."
        )
        with self.assertRaisesRegex(BehaviorViolationError, expected_regex):
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, ctx, provided_args
            )

    def test_enforce_multiple_new_params_later_fails(self):
        """Test enforcement fails when multiple new params are provided later in sequence."""
        ctx = MockContext(collected_params={"param1"})
        provided_args = {
            "param1": "v1",
            "param2": "value2",
            "param3": "value3",
        }  # param2 and param3 are new
        # Adjust regex to match the more detailed error message
        expected_regex = (
            r"Tool 'test.tool' expects parameters sequentially. "
            r"Expected next: 'param2'. "
            # Use regex to handle potential set order variations
            r"Provided multiple new parameters: {('param2', 'param3'|'param3', 'param2')}. "
            r"Collected so far: {'param1'}."
        )
        with self.assertRaisesRegex(BehaviorViolationError, expected_regex):
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, ctx, provided_args
            )

    def test_enforce_params_after_sequence_complete_passes(self):
        """Test enforcement passes when providing args after the sequence is complete."""
        ctx = MockContext(collected_params={"param1", "param2", "param3"})
        provided_args = {
            "param1": "v1",
            "param2": "v2",
            "param3": "v3",
            "optional_param": "opt",
        }
        try:
            BehaviorEnforcer.enforce(
                self.tool_name, self.behavior_config, ctx, provided_args
            )
        except BehaviorViolationError as e:
            self.fail(
                f"Enforcement should pass for args after sequence complete. Raised: {e}"
            )


# if __name__ == "__main__":
#     unittest.main()

================
File: tests/unit/application/services/test_memory_service.py
================
"""
Unit tests for the MemoryService.
"""

from unittest.mock import AsyncMock  # Use AsyncMock for async methods
import sys
from pathlib import Path
import pytest

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.parent.parent.absolute()
sys.path.insert(0, str(project_root))

# Modules to test
from paelladoc.application.services.memory_service import MemoryService
from paelladoc.domain.models.project import (
    ProjectMemory,
    ProjectInfo,  # Ensure this line is correct
)
from paelladoc.ports.output.memory_port import MemoryPort

# --- Pytest Fixtures ---


@pytest.fixture
def mock_memory_port() -> AsyncMock:
    """Provides a mocked MemoryPort instance for tests."""
    return AsyncMock(spec=MemoryPort)


@pytest.fixture
def memory_service(mock_memory_port: AsyncMock) -> MemoryService:
    """Provides a MemoryService instance initialized with a mocked port."""
    return MemoryService(memory_port=mock_memory_port)


# --- Tests for Taxonomy Events (Pytest Style) ---


@pytest.mark.asyncio
async def test_update_project_memory_emits_taxonomy_updated_event(
    memory_service: MemoryService, mock_memory_port: AsyncMock
):
    """Test that taxonomy_updated event is emitted when taxonomy fields change."""
    # Arrange
    project_name = "tax-event-project"
    old_memory = ProjectMemory(
        project_info=ProjectInfo(
            name=project_name,
            base_path="/fake",
            taxonomy_version="1.0",
            platform_taxonomy="web-frontend",
            domain_taxonomy="ecommerce",
            size_taxonomy="smb",
            compliance_taxonomy="none",
            lifecycle_taxonomy="test_lifecycle_old",
        ),
        platform_taxonomy="web-frontend",
        domain_taxonomy="ecommerce",
        size_taxonomy="smb",
        compliance_taxonomy="none",
        lifecycle_taxonomy="test_lifecycle_old",
    )

    new_memory = ProjectMemory(
        project_info=ProjectInfo(
            name=project_name,
            base_path="/fake",
            taxonomy_version="1.0",
            platform_taxonomy="ios-native",
            domain_taxonomy="ecommerce",
            size_taxonomy="enterprise",
            compliance_taxonomy="gdpr",
            lifecycle_taxonomy="test_lifecycle_new",
        ),
        platform_taxonomy="ios-native",
        domain_taxonomy="ecommerce",
        size_taxonomy="enterprise",
        compliance_taxonomy="gdpr",
        lifecycle_taxonomy="test_lifecycle_new",
    )

    # Mock the port methods
    mock_memory_port.project_exists.return_value = True
    mock_memory_port.load_memory.return_value = old_memory
    mock_memory_port.save_memory.return_value = None  # Async function returns None

    # Create and register a mock event handler
    mock_handler = AsyncMock()
    memory_service.register_event_handler("taxonomy_updated", mock_handler)
    # Also register for project_updated to ensure it's still called
    mock_project_updated_handler = AsyncMock()
    memory_service.register_event_handler(
        "project_updated", mock_project_updated_handler
    )

    # Act
    await memory_service.update_project_memory(new_memory)

    # Assert
    mock_memory_port.save_memory.assert_awaited_once_with(new_memory)

    # Check project_updated event was called
    mock_project_updated_handler.assert_awaited_once()
    assert mock_project_updated_handler.await_args[0][0] == "project_updated"

    # Check taxonomy_updated event was called with correct data
    mock_handler.assert_awaited_once()
    event_name, event_data = mock_handler.await_args[0]
    assert event_name == "taxonomy_updated"
    assert event_data["project_name"] == project_name
    assert event_data["new_taxonomy"] == {
        "platform": "ios-native",
        "domain": "ecommerce",
        "size": "enterprise",
        "compliance": "gdpr",
    }
    assert event_data["old_taxonomy"] == {
        "platform": "web-frontend",
        "domain": "ecommerce",
        "size": "smb",
        "compliance": "none",
    }


@pytest.mark.asyncio
async def test_update_project_memory_no_taxonomy_change_no_event(
    memory_service: MemoryService, mock_memory_port: AsyncMock
):
    """Test that taxonomy_updated event is NOT emitted if taxonomy fields don't change."""
    # Arrange
    project_name = "no-tax-event-project"
    old_memory = ProjectMemory(
        project_info=ProjectInfo(
            name=project_name,
            base_path="/fake",
            taxonomy_version="1.0",
            platform_taxonomy="web-frontend",
            domain_taxonomy="ecommerce",
            size_taxonomy="smb",
            compliance_taxonomy="none",
            lifecycle_taxonomy="test_lifecycle",
        ),
        platform_taxonomy="web-frontend",
        domain_taxonomy="ecommerce",
        size_taxonomy="smb",
        compliance_taxonomy="none",
        lifecycle_taxonomy="test_lifecycle",
    )

    new_memory = ProjectMemory(
        project_info=ProjectInfo(
            name=project_name,
            base_path="/fake",
            taxonomy_version="1.0",
            platform_taxonomy="web-frontend",
            domain_taxonomy="ecommerce",
            size_taxonomy="smb",
            compliance_taxonomy="none",
            lifecycle_taxonomy="test_lifecycle",
        ),
        platform_taxonomy="web-frontend",
        domain_taxonomy="ecommerce",
        size_taxonomy="smb",
        compliance_taxonomy="none",
        lifecycle_taxonomy="test_lifecycle",
    )
    # Make some other change to trigger update
    new_memory.project_info.taxonomy_version = "1.1"

    # Mock the port methods
    mock_memory_port.project_exists.return_value = True
    mock_memory_port.load_memory.return_value = old_memory
    mock_memory_port.save_memory.return_value = None

    # Create and register a mock event handler
    mock_handler = AsyncMock()
    memory_service.register_event_handler("taxonomy_updated", mock_handler)
    # Also register for project_updated to ensure it's still called
    mock_project_updated_handler = AsyncMock()
    memory_service.register_event_handler(
        "project_updated", mock_project_updated_handler
    )

    # Act
    await memory_service.update_project_memory(new_memory)

    # Assert
    mock_memory_port.save_memory.assert_awaited_once_with(new_memory)

    # Check project_updated event was called (because metadata changed)
    mock_project_updated_handler.assert_awaited_once()

    # Check taxonomy_updated event was NOT called
    mock_handler.assert_not_awaited()


# NOTE: Keep the existing unittest class for other tests for now, or refactor all later.
# If keeping both styles, ensure imports and module structure support it.

# class TestMemoryService(unittest.IsolatedAsyncioTestCase):
#    ... (existing unittest tests) ...

================
File: tests/unit/application/services/test_vector_store_service.py
================
"""
Unit tests for the VectorStoreService.
"""

import unittest
from unittest.mock import AsyncMock, MagicMock  # Added MagicMock for SearchResult
import sys
from pathlib import Path
from typing import List, Dict, Any

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.parent.parent.absolute()
sys.path.insert(0, str(project_root))

# Modules to test
from paelladoc.application.services.vector_store_service import VectorStoreService
from paelladoc.ports.output.vector_store_port import VectorStorePort, SearchResult


# Dummy SearchResult implementation for tests
class MockSearchResult(SearchResult):
    def __init__(
        self, id: str, distance: float, metadata: Dict[str, Any], document: str
    ):
        self.id = id
        self.distance = distance
        self.metadata = metadata
        self.document = document


class TestVectorStoreService(unittest.IsolatedAsyncioTestCase):
    """Unit tests for the VectorStoreService using a mocked VectorStorePort."""

    def setUp(self):
        """Set up a mocked VectorStorePort before each test."""
        self.mock_vector_store_port = AsyncMock(spec=VectorStorePort)
        self.vector_store_service = VectorStoreService(
            vector_store_port=self.mock_vector_store_port
        )

    # --- Test Cases --- #

    async def test_add_texts_to_collection_calls_port(self):
        """Verify add_texts_to_collection calls add_documents on the port."""
        collection_name = "test_coll"
        documents = ["doc1", "doc2"]
        metadatas = [{"s": 1}, {"s": 2}]
        ids = ["id1", "id2"]
        expected_ids = ids

        self.mock_vector_store_port.add_documents.return_value = expected_ids

        actual_ids = await self.vector_store_service.add_texts_to_collection(
            collection_name, documents, metadatas, ids
        )

        self.mock_vector_store_port.add_documents.assert_awaited_once_with(
            collection_name=collection_name,
            documents=documents,
            metadatas=metadatas,
            ids=ids,
        )
        self.assertEqual(actual_ids, expected_ids)

    async def test_add_texts_to_collection_reraises_exception(self):
        """Verify add_texts_to_collection re-raises port exceptions."""
        collection_name = "test_coll_fail"
        documents = ["doc1"]
        test_exception = ValueError("Port error")
        self.mock_vector_store_port.add_documents.side_effect = test_exception

        with self.assertRaises(ValueError) as cm:
            await self.vector_store_service.add_texts_to_collection(
                collection_name, documents
            )

        self.assertEqual(cm.exception, test_exception)
        self.mock_vector_store_port.add_documents.assert_awaited_once()

    async def test_find_similar_texts_calls_port(self):
        """Verify find_similar_texts calls search_similar on the port."""
        collection_name = "test_search_coll"
        query_texts = ["query1"]
        n_results = 3
        filter_metadata = {"year": 2024}
        filter_document = None  # Example
        expected_results: List[List[SearchResult]] = [
            [MockSearchResult("res1", 0.5, {"year": 2024}, "doc text")]
        ]

        self.mock_vector_store_port.search_similar.return_value = expected_results

        actual_results = await self.vector_store_service.find_similar_texts(
            collection_name, query_texts, n_results, filter_metadata, filter_document
        )

        self.mock_vector_store_port.search_similar.assert_awaited_once_with(
            collection_name=collection_name,
            query_texts=query_texts,
            n_results=n_results,
            where=filter_metadata,
            where_document=filter_document,
            include=[
                "metadatas",
                "documents",
                "distances",
                "ids",
            ],  # Check default include
        )
        self.assertEqual(actual_results, expected_results)

    async def test_find_similar_texts_reraises_exception(self):
        """Verify find_similar_texts re-raises port exceptions."""
        collection_name = "test_search_fail"
        query_texts = ["query1"]
        test_exception = RuntimeError("Search failed")
        self.mock_vector_store_port.search_similar.side_effect = test_exception

        with self.assertRaises(RuntimeError) as cm:
            await self.vector_store_service.find_similar_texts(
                collection_name, query_texts
            )

        self.assertEqual(cm.exception, test_exception)
        self.mock_vector_store_port.search_similar.assert_awaited_once()

    async def test_ensure_collection_exists_calls_port(self):
        """Verify ensure_collection_exists calls get_or_create_collection on the port."""
        collection_name = "ensure_coll"
        # Mock the port method to return a dummy collection object (can be anything)
        self.mock_vector_store_port.get_or_create_collection.return_value = MagicMock()

        await self.vector_store_service.ensure_collection_exists(collection_name)

        self.mock_vector_store_port.get_or_create_collection.assert_awaited_once_with(
            collection_name
        )

    async def test_ensure_collection_exists_reraises_exception(self):
        """Verify ensure_collection_exists re-raises port exceptions."""
        collection_name = "ensure_coll_fail"
        test_exception = ConnectionError("DB down")
        self.mock_vector_store_port.get_or_create_collection.side_effect = (
            test_exception
        )

        with self.assertRaises(ConnectionError) as cm:
            await self.vector_store_service.ensure_collection_exists(collection_name)

        self.assertEqual(cm.exception, test_exception)
        self.mock_vector_store_port.get_or_create_collection.assert_awaited_once_with(
            collection_name
        )

    async def test_remove_collection_calls_port(self):
        """Verify remove_collection calls delete_collection on the port."""
        collection_name = "remove_coll"
        self.mock_vector_store_port.delete_collection.return_value = (
            None  # Method returns None
        )

        await self.vector_store_service.remove_collection(collection_name)

        self.mock_vector_store_port.delete_collection.assert_awaited_once_with(
            collection_name
        )

    async def test_remove_collection_reraises_exception(self):
        """Verify remove_collection re-raises port exceptions."""
        collection_name = "remove_coll_fail"
        test_exception = TimeoutError("Delete timed out")
        self.mock_vector_store_port.delete_collection.side_effect = test_exception

        with self.assertRaises(TimeoutError) as cm:
            await self.vector_store_service.remove_collection(collection_name)

        self.assertEqual(cm.exception, test_exception)
        self.mock_vector_store_port.delete_collection.assert_awaited_once_with(
            collection_name
        )


# if __name__ == "__main__":
#     unittest.main()

================
File: tests/unit/domain/models/test_project.py
================
import json
import pytest
from datetime import datetime
from pathlib import Path

from paelladoc.domain.models.project import (
    DocumentStatus,
    Bucket,
    ArtifactMeta,
    ProjectInfo,
    ProjectMemory,
    # ProjectDocument, # Assuming this was removed or is internal
)
from paelladoc.adapters.services.system_time_service import SystemTimeService
from paelladoc.domain.models.project import set_time_service, time_service


class TestBucket:
    """Tests for the Bucket enum"""

    def test_bucket_values(self):
        """Test that all buckets have the correct string format"""
        for bucket in Bucket:
            if bucket is not Bucket.UNKNOWN:
                # Format should be "Phase::Subcategory"
                assert "::" in bucket.value
                phase, subcategory = bucket.value.split("::")
                assert phase in [
                    "Initiate",
                    "Elaborate",
                    "Govern",
                    "Generate",
                    "Maintain",
                    "Deploy",
                    "Operate",
                    "Iterate",
                ]
                assert len(subcategory) > 0
            else:
                assert bucket.value == "Unknown"

    def test_get_phase_buckets(self):
        """Test the get_phase_buckets class method"""
        initiate_buckets = Bucket.get_phase_buckets("Initiate")
        assert len(initiate_buckets) == 2
        assert Bucket.INITIATE_CORE_SETUP in initiate_buckets
        assert Bucket.INITIATE_INITIAL_PRODUCT_DOCS in initiate_buckets

        elaborate_buckets = Bucket.get_phase_buckets("Elaborate")
        assert len(elaborate_buckets) == 4

        # Should return empty set for non-existent phase
        nonexistent_buckets = Bucket.get_phase_buckets("NonExistent")
        assert len(nonexistent_buckets) == 0


class TestArtifactMeta:
    """Tests for the ArtifactMeta model"""

    def test_create_artifact_meta(self):
        """Test creating an ArtifactMeta instance"""
        artifact = ArtifactMeta(
            name="test_artifact",
            bucket=Bucket.INITIATE_CORE_SETUP,
            path=Path("docs/test_artifact.md"),
            status=DocumentStatus.IN_PROGRESS,
        )

        assert artifact.name == "test_artifact"
        assert artifact.bucket == Bucket.INITIATE_CORE_SETUP
        assert artifact.path == Path("docs/test_artifact.md")
        assert artifact.status == DocumentStatus.IN_PROGRESS
        assert isinstance(artifact.created_at, datetime)
        assert isinstance(artifact.updated_at, datetime)

    def test_update_status(self):
        """Test updating an artifact's status"""
        artifact = ArtifactMeta(
            name="test_artifact",
            bucket=Bucket.INITIATE_CORE_SETUP,
            path=Path("docs/test_artifact.md"),
        )

        # Default status should be PENDING
        assert artifact.status == DocumentStatus.PENDING

        # Store the original timestamp
        original_updated_at = artifact.updated_at

        # Update the status
        artifact.update_status(DocumentStatus.COMPLETED)

        # Check that status was updated
        assert artifact.status == DocumentStatus.COMPLETED

        # Check that timestamp was updated
        assert artifact.updated_at > original_updated_at

    def test_serialization_deserialization(self):
        """Test that ArtifactMeta can be serialized and deserialized"""
        artifact = ArtifactMeta(
            name="test_artifact",
            bucket=Bucket.ELABORATE_DISCOVERY_AND_RESEARCH,
            path=Path("docs/research/test_artifact.md"),
            status=DocumentStatus.COMPLETED,
        )

        # Serialize to JSON
        artifact_json = artifact.model_dump_json()

        # Deserialize from JSON
        loaded_artifact = ArtifactMeta.model_validate_json(artifact_json)

        # Check that all fields were preserved
        assert loaded_artifact.name == artifact.name
        assert loaded_artifact.bucket == artifact.bucket
        assert loaded_artifact.path == artifact.path
        assert loaded_artifact.status == artifact.status
        assert loaded_artifact.created_at == artifact.created_at
        assert loaded_artifact.updated_at == artifact.updated_at


@pytest.fixture
def sample_project_memory() -> ProjectMemory:
    """Fixture to provide a sample ProjectMemory instance for testing."""
    # Set up TimeService if needed
    if time_service is None:
        set_time_service(SystemTimeService())

    project_info = ProjectInfo(
        name="Test Project",
        platform_taxonomy="test_platform",
        domain_taxonomy="test_domain",
        size_taxonomy="test_size",
        compliance_taxonomy="test_compliance",
        lifecycle_taxonomy="test_lifecycle",
    )

    # Create the ProjectMemory instance first
    project_memory_instance = ProjectMemory(
        project_info=project_info,
        platform_taxonomy="test_platform",
        domain_taxonomy="test_domain",
        size_taxonomy="test_size",
        compliance_taxonomy="test_compliance",
        lifecycle_taxonomy="test_lifecycle",
    )

    # Now create and add the artifacts expected by the tests
    artifacts_to_add = [
        ArtifactMeta(
            name="vision_doc",
            bucket=Bucket.INITIATE_INITIAL_PRODUCT_DOCS,
            path=Path("docs/initiation/product_vision.md"),
            status=DocumentStatus.PENDING,  # Changed status back to PENDING
        ),
        ArtifactMeta(
            name="user_research",
            bucket=Bucket.ELABORATE_DISCOVERY_AND_RESEARCH,
            path=Path("docs/research/user_research.md"),
            status=DocumentStatus.IN_PROGRESS,  # Specific status for this test artifact
        ),
        ArtifactMeta(
            name="api_spec",
            bucket=Bucket.ELABORATE_SPECIFICATION_AND_PLANNING,
            path=Path("docs/specs/api_specification.md"),
            status=DocumentStatus.COMPLETED,  # Specific status for this test artifact
        ),
    ]

    for artifact in artifacts_to_add:
        project_memory_instance.add_artifact(artifact)

    return project_memory_instance


def test_project_info_initialization():
    """Test ProjectInfo initialization."""
    info = ProjectInfo(
        name="Another Test",
        # description="Detailed desc.", # Removed
        base_path="/tmp",
        documentation_language="es",
        interaction_language="es",
        platform_taxonomy="test_platform_2",
        domain_taxonomy="test_domain_2",
        size_taxonomy="test_size_2",
        compliance_taxonomy="test_compliance_2",
        lifecycle_taxonomy="test_lifecycle_2",  # Added lifecycle
    )
    assert info.name == "Another Test"
    # assert info.description == "Detailed desc." # Removed assertion for non-existent field
    assert str(info.base_path) == "/tmp"  # Check path conversion if needed
    assert info.documentation_language == "es"
    assert info.interaction_language == "es"
    assert info.platform_taxonomy == "test_platform_2"
    assert info.domain_taxonomy == "test_domain_2"
    assert info.size_taxonomy == "test_size_2"
    assert info.compliance_taxonomy == "test_compliance_2"
    assert info.lifecycle_taxonomy == "test_lifecycle_2"


def test_project_memory_initialization(sample_project_memory):
    """Test ProjectMemory initialization using the fixture."""
    assert sample_project_memory.project_info.name == "Test Project"
    # assert "version" in sample_project_memory.metadata # Removed check for metadata
    assert isinstance(sample_project_memory.created_at, datetime)
    assert isinstance(
        sample_project_memory.last_updated_at, datetime
    )  # Corrected field name
    # Check taxonomy fields added in fixture (both in project_info and directly)
    assert sample_project_memory.project_info.platform_taxonomy == "test_platform"
    assert sample_project_memory.project_info.domain_taxonomy == "test_domain"
    assert sample_project_memory.project_info.size_taxonomy == "test_size"
    assert sample_project_memory.project_info.compliance_taxonomy == "test_compliance"
    assert sample_project_memory.platform_taxonomy == "test_platform"
    assert sample_project_memory.domain_taxonomy == "test_domain"
    assert sample_project_memory.size_taxonomy == "test_size"
    assert sample_project_memory.compliance_taxonomy == "test_compliance"


def test_project_memory_update(sample_project_memory):
    """Test ProjectMemory update."""
    # Implementation of the test_project_memory_update method
    pass


class TestProjectMemory:
    """Tests for the ProjectMemory model with taxonomy support"""

    @pytest.fixture(autouse=True)
    def setup_test_time_service(self):
        """Ensure time service is set for each test in this class."""
        if time_service is None:
            set_time_service(SystemTimeService())

    def test_project_memory_initialization(self):
        """Test initializing ProjectMemory with taxonomy support"""
        project = ProjectMemory(
            project_info=ProjectInfo(
                name="test_project",
                # Add required taxonomy fields to ProjectInfo
                platform_taxonomy="test_platform",
                domain_taxonomy="test_domain",
                size_taxonomy="test_size",
                compliance_taxonomy="test_compliance",
                lifecycle_taxonomy="test_lifecycle",  # Added lifecycle
            ),
            taxonomy_version="0.5",
            # Add required taxonomy fields also directly to ProjectMemory
            platform_taxonomy="test_platform",
            domain_taxonomy="test_domain",
            size_taxonomy="test_size",
            compliance_taxonomy="test_compliance",
            lifecycle_taxonomy="test_lifecycle",  # Added lifecycle
        )
        assert project.project_info.name == "test_project"
        assert project.lifecycle_taxonomy == "test_lifecycle"
        assert project.platform_taxonomy == "test_platform"
        assert len(project.artifacts) == len(
            Bucket
        )  # Should have all buckets initialized

    def test_add_artifact(self, sample_project_memory):
        """Test adding artifacts to ProjectMemory"""
        project = sample_project_memory

        # Check that artifacts were added to the correct buckets
        assert len(project.artifacts[Bucket.INITIATE_INITIAL_PRODUCT_DOCS]) == 1
        assert len(project.artifacts[Bucket.ELABORATE_DISCOVERY_AND_RESEARCH]) == 1
        assert len(project.artifacts[Bucket.ELABORATE_SPECIFICATION_AND_PLANNING]) == 1

        # Check that artifact was added with correct fields
        initiate_artifact = project.artifacts[Bucket.INITIATE_INITIAL_PRODUCT_DOCS][0]
        assert initiate_artifact.name == "vision_doc"
        assert initiate_artifact.path == Path("docs/initiation/product_vision.md")
        assert initiate_artifact.status == DocumentStatus.PENDING

        # Test adding a duplicate (should return False)
        duplicate = ArtifactMeta(
            name="dup_vision",
            bucket=Bucket.INITIATE_CORE_SETUP,
            path=Path(
                "docs/initiation/product_vision.md"
            ),  # Same path as existing artifact
        )
        assert not project.add_artifact(duplicate)

        # Check that original buckets still have the same count
        assert len(project.artifacts[Bucket.INITIATE_INITIAL_PRODUCT_DOCS]) == 1
        assert (
            len(project.artifacts[Bucket.INITIATE_CORE_SETUP]) == 0
        )  # Duplicate wasn't added

    def test_get_artifact(self, sample_project_memory):
        """Test retrieving artifacts by bucket and name"""
        project = sample_project_memory

        # Get existing artifact
        artifact = project.get_artifact(
            Bucket.ELABORATE_DISCOVERY_AND_RESEARCH, "user_research"
        )
        assert artifact is not None
        assert artifact.name == "user_research"
        assert artifact.bucket == Bucket.ELABORATE_DISCOVERY_AND_RESEARCH

        # Get non-existent artifact
        non_existent = project.get_artifact(Bucket.DEPLOY_SECURITY, "security_plan")
        assert non_existent is None

    def test_get_artifact_by_path(self, sample_project_memory):
        """Test retrieving artifacts by path"""
        project = sample_project_memory

        # Get existing artifact by path
        artifact = project.get_artifact_by_path(Path("docs/specs/api_specification.md"))
        assert artifact is not None
        assert artifact.name == "api_spec"
        assert artifact.bucket == Bucket.ELABORATE_SPECIFICATION_AND_PLANNING

        # Get non-existent artifact
        non_existent = project.get_artifact_by_path(Path("nonexistent/path.md"))
        assert non_existent is None

    def test_update_artifact_status(self, sample_project_memory):
        """Test updating artifact status"""
        project = sample_project_memory

        # Update existing artifact
        success = project.update_artifact_status(
            Bucket.INITIATE_INITIAL_PRODUCT_DOCS, "vision_doc", DocumentStatus.COMPLETED
        )
        assert success

        # Verify the status was updated
        artifact = project.get_artifact(
            Bucket.INITIATE_INITIAL_PRODUCT_DOCS, "vision_doc"
        )
        assert artifact.status == DocumentStatus.COMPLETED

        # Try to update non-existent artifact
        success = project.update_artifact_status(
            Bucket.DEPLOY_SECURITY, "nonexistent", DocumentStatus.COMPLETED
        )
        assert not success

    def test_get_bucket_completion(self, sample_project_memory):
        """Test getting completion stats for buckets"""
        project = sample_project_memory

        # Bucket with one completed artifact
        elaborate_spec_stats = project.get_bucket_completion(
            Bucket.ELABORATE_SPECIFICATION_AND_PLANNING
        )
        assert elaborate_spec_stats["total"] == 1
        assert elaborate_spec_stats["completed"] == 1
        assert elaborate_spec_stats["in_progress"] == 0
        assert elaborate_spec_stats["pending"] == 0
        assert elaborate_spec_stats["completion_percentage"] == 100.0

        # Bucket with one in-progress artifact
        elaborate_research_stats = project.get_bucket_completion(
            Bucket.ELABORATE_DISCOVERY_AND_RESEARCH
        )
        assert elaborate_research_stats["total"] == 1
        assert elaborate_research_stats["completed"] == 0
        assert elaborate_research_stats["in_progress"] == 1
        assert elaborate_research_stats["pending"] == 0
        assert elaborate_research_stats["completion_percentage"] == 0.0

        # Empty bucket
        empty_bucket_stats = project.get_bucket_completion(Bucket.DEPLOY_SECURITY)
        assert empty_bucket_stats["total"] == 0
        assert empty_bucket_stats["completion_percentage"] == 0.0

    def test_get_phase_completion(self, sample_project_memory):
        """Test getting completion stats for entire phases"""
        project = sample_project_memory

        # Elaborate phase has 2 artifacts (1 completed, 1 in-progress)
        elaborate_stats = project.get_phase_completion("Elaborate")
        assert elaborate_stats["total"] == 2
        assert elaborate_stats["completed"] == 1
        assert elaborate_stats["in_progress"] == 1
        assert elaborate_stats["pending"] == 0
        assert elaborate_stats["completion_percentage"] == 50.0
        assert elaborate_stats["buckets"] == 4  # All Elaborate buckets

        # Initiate phase has 1 pending artifact
        initiate_stats = project.get_phase_completion("Initiate")
        assert initiate_stats["total"] == 1
        assert initiate_stats["completed"] == 0
        assert initiate_stats["pending"] == 1
        assert initiate_stats["completion_percentage"] == 0.0

        # Deploy phase has 0 artifacts
        deploy_stats = project.get_phase_completion("Deploy")
        assert deploy_stats["total"] == 0
        assert deploy_stats["completion_percentage"] == 0.0

    def test_serialization_deserialization(self, sample_project_memory):
        """Test that ProjectMemory with taxonomy can be serialized and deserialized"""
        project = sample_project_memory

        # Serialize to JSON
        project_json = project.model_dump_json()

        # Check that JSON is valid
        parsed_json = json.loads(project_json)
        assert parsed_json["taxonomy_version"] == "0.5"
        assert "artifacts" in parsed_json

        # Deserialize from JSON
        loaded_project = ProjectMemory.model_validate_json(project_json)

        # Check that all fields were preserved
        assert loaded_project.project_info.name == project.project_info.name
        assert loaded_project.taxonomy_version == project.taxonomy_version

        # Check artifacts
        assert Bucket.INITIATE_INITIAL_PRODUCT_DOCS in loaded_project.artifacts
        assert Bucket.ELABORATE_DISCOVERY_AND_RESEARCH in loaded_project.artifacts
        assert Bucket.ELABORATE_SPECIFICATION_AND_PLANNING in loaded_project.artifacts

        # Check specific artifact fields were preserved
        loaded_artifact = loaded_project.get_artifact(
            Bucket.ELABORATE_SPECIFICATION_AND_PLANNING, "api_spec"
        )
        assert loaded_artifact is not None
        assert loaded_artifact.name == "api_spec"
        assert loaded_artifact.path == Path("docs/specs/api_specification.md")
        assert loaded_artifact.status == DocumentStatus.COMPLETED

        # Verify completion stats are calculated correctly after deserialization
        stats = loaded_project.get_phase_completion("Elaborate")
        assert stats["completion_percentage"] == 50.0

================
File: tests/integration/test_server.py
================
#!/usr/bin/env python
"""
Integration tests for the Paelladoc MCP server.

These tests verify that the server correctly starts and responds to requests
via STDIO communication.
"""

import unittest
import sys
import os
import subprocess
from pathlib import Path

# Removed pty/select imports as PTY test is skipped
import signal

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.parent
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Constants
SERVER_SCRIPT = project_root / "server.py"


class TestServerIntegration(unittest.TestCase):
    """Integration tests for the MCP server STDIO communication."""

    @unittest.skip(
        "Skipping PTY/STDIO test: FastMCP stdio interaction difficult to replicate reliably outside actual client environment."
    )
    def test_server_responds_to_ping(self):
        """Verify that the server responds to a ping request via PTY STDIO. (SKIPPED)"""
        # request_id = str(uuid.uuid4()) # F841 - Removed
        env = os.environ.copy()
        env["PYTHONPATH"] = str(project_root)
        env["PYTHONUNBUFFERED"] = "1"

        # --- Start server using PTY ---
        # master_fd, slave_fd = pty.openpty() # PTY logic commented out
        server_process = None
        master_fd = None  # Ensure master_fd is defined for finally block

        try:
            # server_process = subprocess.Popen(...)
            # os.close(slave_fd)

            # --- Test Communication ---
            # response_data = None # F841 - Removed
            # stderr_output = "" # F841 - Removed again

            # time.sleep(2)

            # if server_process.poll() is not None:
            #     ...

            # mcp_request = {...}
            # request_json = json.dumps(mcp_request) + "\n"

            # print(f"Sending request via PTY: {request_json.strip()}")
            # os.write(master_fd, request_json.encode())

            # # Read response from PTY master fd with timeout
            # stdout_line = ""
            # buffer = b""
            # end_time = time.time() + 5

            # while time.time() < end_time:
            #     ...

            # print(f"Received raw response line: {stdout_line.strip()}")

            # if not stdout_line:
            #      ...

            # response_data = json.loads(stdout_line)
            # print(f"Parsed response: {response_data}")

            # self.assertEqual(...)
            pass  # Keep test structure but do nothing as it's skipped

        except Exception as e:
            # stderr_output = "" # F841 - Removed
            # ... (error handling commented out) ...
            self.fail(f"An error occurred during the PTY test (should be skipped): {e}")

        finally:
            # --- Cleanup ---
            if master_fd:
                try:
                    os.close(master_fd)
                except OSError:
                    pass
            if server_process and server_process.poll() is None:
                print("Terminating server process (if it was started)...")
                try:
                    os.killpg(os.getpgid(server_process.pid), signal.SIGTERM)
                    server_process.wait(timeout=2)
                except (ProcessLookupError, subprocess.TimeoutExpired, AttributeError):
                    # Handle cases where process/pgid might not exist if startup failed early
                    print(
                        "Server cleanup notification: process termination might have failed or was not needed."
                    )
                    if server_process and server_process.poll() is None:
                        try:
                            os.killpg(os.getpgid(server_process.pid), signal.SIGKILL)
                        except Exception:
                            pass  # Final attempt
                except Exception as term_e:
                    print(f"Error during termination: {term_e}")
            # Read any remaining stderr
            if server_process and server_process.stderr:
                stderr_rem = server_process.stderr.read().decode(errors="ignore")
                if stderr_rem:
                    print(f"Remaining stderr: {stderr_rem}")


if __name__ == "__main__":
    unittest.main()

================
File: tests/integration/test_alembic_config.py
================
"""Integration tests for Alembic configuration."""

import os
import pytest
from pathlib import Path
import uuid
import subprocess  # Import subprocess
from alembic.config import Config
from alembic.script import ScriptDirectory
from alembic.runtime.migration import MigrationContext
from sqlalchemy.ext.asyncio import create_async_engine
import sys

# Import get_db_path to test its behavior directly
from paelladoc.config.database import get_db_path

# Get project root to build absolute paths if needed
PROJECT_ROOT = Path(__file__).parent.parent.parent.parent.absolute()
sys.path.insert(0, str(PROJECT_ROOT))


@pytest.fixture
def clean_env():
    """Remove relevant environment variables before each test."""
    original_db_path = os.environ.get("PAELLADOC_DB_PATH")
    original_env = os.environ.get("PAELLADOC_ENV")

    if "PAELLADOC_DB_PATH" in os.environ:
        del os.environ["PAELLADOC_DB_PATH"]
    if "PAELLADOC_ENV" in os.environ:
        del os.environ["PAELLADOC_ENV"]

    yield

    if original_db_path is not None:
        os.environ["PAELLADOC_DB_PATH"] = original_db_path
    if original_env is not None:
        os.environ["PAELLADOC_ENV"] = original_env


@pytest.fixture
def temp_db_path():
    """Create a temporary database path."""
    test_db_name = f"test_alembic_{uuid.uuid4()}.db"
    # Use a simpler temp directory structure to avoid potential permission issues
    test_dir = Path("/tmp") / "paelladoc_test_dbs"
    test_db_path = test_dir / test_db_name
    test_db_path.parent.mkdir(parents=True, exist_ok=True)

    yield test_db_path

    # Cleanup
    try:
        if test_db_path.exists():
            # No need for asyncio.sleep here as subprocess runs separately
            os.remove(test_db_path)
        if test_dir.exists() and not any(test_dir.iterdir()):
            test_dir.rmdir()
    except Exception as e:
        print(f"Error during cleanup: {e}")


def run_alembic_command(command: list, env: dict):
    """Helper function to run alembic CLI commands via subprocess."""
    # Ensure alembic is callable, adjust path if needed (e.g., use .venv/bin/alembic)
    alembic_executable = PROJECT_ROOT / ".venv" / "bin" / "alembic"
    if not alembic_executable.exists():
        # Fallback or error if venv structure is different
        pytest.fail(f"Alembic executable not found at {alembic_executable}")

    cmd = [str(alembic_executable)] + command
    print(f"\nRunning subprocess: {' '.join(cmd)}")
    result = subprocess.run(
        cmd,
        capture_output=True,
        text=True,
        env={**os.environ, **env},  # Merge OS env with test-specific env
        cwd=PROJECT_ROOT,  # Run from project root where alembic.ini is
        check=False,  # Don't raise exception on non-zero exit, check manually
    )
    print(f"Subprocess stdout:\n{result.stdout}")
    print(f"Subprocess stderr:\n{result.stderr}")
    if result.returncode != 0:
        pytest.fail(
            f"Alembic command {' '.join(command)} failed with exit code {result.returncode}\nStderr: {result.stderr}"
        )
    return result


def test_alembic_config_uses_db_path_via_env(clean_env, temp_db_path):
    """Test that env.py logic picks up PAELLADOC_DB_PATH."""
    os.environ["PAELLADOC_DB_PATH"] = str(temp_db_path)

    # Verify that get_db_path() returns the expected path
    # as this is what env.py uses to construct the URL.
    resolved_path = get_db_path()
    assert resolved_path == temp_db_path


@pytest.mark.asyncio
async def test_alembic_migrations_work_with_config(clean_env, temp_db_path):
    """Test that migrations work by running alembic upgrade via subprocess."""
    test_env = {"PAELLADOC_DB_PATH": str(temp_db_path)}

    # Ensure the temporary database file exists before running Alembic
    if not temp_db_path.exists():
        temp_db_path.touch()

    # Run alembic upgrade head in a subprocess
    run_alembic_command(["upgrade", "head"], env=test_env)

    # Verify migrations applied using an async engine
    # Need the actual URL alembic used (which comes from env var)
    db_url = f"sqlite+aiosqlite:///{temp_db_path}"
    engine = create_async_engine(db_url)
    try:
        async with engine.connect() as conn:
            # Define a sync function to get revision
            def get_rev_sync(sync_conn):
                # Need alembic config to find script directory
                cfg = Config("alembic.ini")  # Load config to get script location
                migration_context = MigrationContext.configure(
                    connection=sync_conn,
                    opts={"script": ScriptDirectory.from_config(cfg)},
                )
                return migration_context.get_current_revision()

            # Run the sync function using run_sync
            current_rev = await conn.run_sync(get_rev_sync)

            # Get head revision directly from script directory
            cfg = Config("alembic.ini")
            script = ScriptDirectory.from_config(cfg)
            head_rev = script.get_current_head()

            assert current_rev is not None, "DB revision is None after upgrade."
            assert current_rev == head_rev, (
                f"DB revision {current_rev} does not match head {head_rev}"
            )
    finally:
        await engine.dispose()


@pytest.mark.asyncio
async def test_alembic_downgrade_works_with_config(clean_env, temp_db_path):
    """Test that downgrades work by running alembic via subprocess."""
    test_env = {"PAELLADOC_DB_PATH": str(temp_db_path)}

    # Ensure the temporary database file exists before running Alembic
    if not temp_db_path.exists():
        temp_db_path.touch()

    # Run migrations up first
    run_alembic_command(["upgrade", "head"], env=test_env)

    # Run migrations down
    run_alembic_command(["downgrade", "base"], env=test_env)

    # Verify database is at base (no revision)
    db_url = f"sqlite+aiosqlite:///{temp_db_path}"
    engine = create_async_engine(db_url)
    try:
        async with engine.connect() as conn:
            # Define a sync function to get revision
            def get_rev_sync(sync_conn):
                cfg = Config("alembic.ini")  # Load config to get script location
                migration_context = MigrationContext.configure(
                    connection=sync_conn,
                    opts={"script": ScriptDirectory.from_config(cfg)},
                )
                return migration_context.get_current_revision()

            # Run the sync function using run_sync
            current_rev = await conn.run_sync(get_rev_sync)
            assert current_rev is None, (
                f"Expected base revision (None), got {current_rev}"
            )
    finally:
        await engine.dispose()


def test_alembic_respects_environment_precedence(clean_env, temp_db_path):
    """Test that PAELLADOC_DB_PATH takes precedence over PAELLADOC_ENV."""
    # Set both environment variables
    os.environ["PAELLADOC_DB_PATH"] = str(temp_db_path)
    os.environ["PAELLADOC_ENV"] = "development"  # This should be ignored

    # Verify that get_db_path() returns the path from PAELLADOC_DB_PATH
    resolved_path = get_db_path()
    assert resolved_path == temp_db_path

================
File: tests/integration/adapters/plugins/core/test_paella.py
================
"""
Integration tests for the core.paella plugin.
"""

import pytest
import sys
import os
from pathlib import Path
import uuid

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.parent.parent.parent.absolute()
sys.path.insert(0, str(project_root))

from paelladoc.domain.models.language import SupportedLanguage
from paelladoc.adapters.plugins.core.paella import (
    paella_init,
    paella_list,
    paella_select,
)

# Adapter for verification
from paelladoc.adapters.output.sqlite.sqlite_memory_adapter import SQLiteMemoryAdapter
from paelladoc.adapters.output.sqlite.sqlite_user_management_adapter import (
    SQLiteUserManagementAdapter,
)
from paelladoc.dependencies import dependencies
from paelladoc.ports.output.memory_port import MemoryPort
from paelladoc.ports.output.user_management_port import UserManagementPort

# --- Fixtures ---

TEMP_DB_DIR = Path("src/tests/integration/adapters/plugins/core/temp_dbs")


@pytest.fixture(scope="function", autouse=True)
def setup_test_db_dir():
    """Ensure the temporary DB directory exists for each test function."""
    TEMP_DB_DIR.mkdir(parents=True, exist_ok=True)
    yield
    # No cleanup here, let the adapter fixture handle its own DB file


@pytest.fixture(scope="function")
async def paella_test_env(monkeypatch, setup_test_db_dir):
    """Provides an isolated environment (DB, User Manager, Dependencies) for paella plugin tests."""
    db_name = f"test_paella_plugin_{uuid.uuid4()}.db"
    db_path = TEMP_DB_DIR / db_name
    print(f"\nSetting up paella_test_env with DB: {db_path}")

    # Create isolated adapter instances
    memory_adapter = SQLiteMemoryAdapter(db_path=str(db_path))
    # Instantiate SQLiteUserManagementAdapter using the same session factory
    user_manager = SQLiteUserManagementAdapter(
        async_session_factory=memory_adapter.async_session
    )

    # Inject into dependencies using monkeypatch FIRST
    monkeypatch.setitem(dependencies, MemoryPort, memory_adapter)
    monkeypatch.setitem(dependencies, UserManagementPort, user_manager)

    # NOW Initialize DB tables (will create default admin user via injected dependency)
    await memory_adapter._create_db_and_tables()

    yield {"memory_adapter": memory_adapter, "user_manager": user_manager}

    # Teardown: Monkeypatch reverts. Close engine and remove DB file.
    print(f"Tearing down paella_test_env, closing engine and removing DB: {db_path}")
    # Explicitly dispose the engine associated with this test's adapter
    if memory_adapter.async_engine:
        await memory_adapter.async_engine.dispose()
        print(f"Disposed engine for DB: {db_path}")

    if db_path.exists():
        try:
            os.remove(db_path)
            print(f"Removed DB: {db_path}")
        except Exception as e:
            print(f"Error removing DB {db_path}: {e}")


# --- Test Cases --- #


@pytest.mark.asyncio
async def test_create_new_project_asks_for_base_path_and_saves_it(
    paella_test_env: dict,  # Use the environment fixture
    monkeypatch,  # Keep monkeypatch if still needed, otherwise remove
):
    """
    Verify the interactive flow for creating a new project:
    1. Asks for interaction language.
    2. Lists projects (if any) and asks action (create new).
    3. Asks for documentation language.
    4. Asks for new project name (checks for existence).
    5. Asks for base path.
    6. Creates the project, saves absolute base path, saves initial memory.
    """
    print("\nRunning: test_create_new_project_asks_for_base_path_and_saves_it")
    # Get adapter from the environment fixture
    memory_adapter = paella_test_env["memory_adapter"]

    interaction_lang = SupportedLanguage.EN_US.value
    doc_lang = SupportedLanguage.EN_US.value
    project_name = f"test-project-{uuid.uuid4()}"
    base_path_input = "./test_paella_docs"  # Relative path input
    expected_abs_base_path = Path(base_path_input).resolve()

    # Act: Call paella_init directly - dependencies are injected via fixtures
    init_result = await paella_init(
        base_path=base_path_input,
        documentation_language=doc_lang,
        interaction_language=interaction_lang,
        new_project_name=project_name,
        platform_taxonomy="test_platform",
        domain_taxonomy="test_domain",
        size_taxonomy="test_size",
        compliance_taxonomy="test_compliance",
        lifecycle_taxonomy="test_lifecycle",
    )

    assert init_result["status"] == "ok", (
        f"paella_init failed: {init_result.get('message')}"
    )
    # Check returned values directly
    assert init_result["project_name"] == project_name
    assert Path(init_result["base_path"]) == expected_abs_base_path
    assert Path(init_result["base_path"]).is_absolute()

    # Assert: Verify project was saved in the adapter
    # Use the adapter provided by the fixture
    loaded_memory = await memory_adapter.load_memory(project_name)
    assert loaded_memory is not None
    assert loaded_memory.project_info.name == project_name
    assert (
        Path(loaded_memory.project_info.base_path).resolve() == expected_abs_base_path
    )
    # Verify other saved fields if necessary
    assert loaded_memory.project_info.documentation_language == doc_lang
    assert loaded_memory.platform_taxonomy == "test_platform"

    # Cleanup the created directory
    if expected_abs_base_path.exists():
        import shutil

        shutil.rmtree(expected_abs_base_path)
        print(f"Cleaned up test project dir: {expected_abs_base_path}")


@pytest.mark.asyncio
async def test_paella_workflow(paella_test_env: dict):  # Use the environment fixture
    """Test the complete PAELLA workflow: init -> list -> select."""
    # Get adapter from the environment fixture
    memory_adapter = paella_test_env["memory_adapter"]
    # Test data
    project_name = f"test_project_{uuid.uuid4().hex[:8]}"
    base_path = f"docs/{project_name}"
    doc_language = SupportedLanguage.EN_US.value
    int_language = SupportedLanguage.EN_US.value

    # 1. Initialize project - dependencies are injected
    init_result = await paella_init(
        base_path=base_path,
        documentation_language=doc_language,
        interaction_language=int_language,
        new_project_name=project_name,
        platform_taxonomy="test_platform",
        domain_taxonomy="test_domain",
        size_taxonomy="test_size",
        compliance_taxonomy="test_compliance",
        lifecycle_taxonomy="test_lifecycle",
    )
    assert init_result["status"] == "ok", (
        f"paella_init failed: {init_result.get('message')}"
    )
    # Check returned project name directly
    created_project_name = init_result["project_name"]
    assert created_project_name == project_name

    # 2. List projects - dependencies are injected
    list_result = await paella_list()
    assert list_result["status"] == "ok"
    assert isinstance(list_result["projects"], list)
    # Find the created project in the list (accessing dict keys)
    found_project = next(
        (p for p in list_result["projects"] if p["name"] == project_name), None
    )
    assert found_project is not None
    assert found_project["is_active"] is False  # Access key

    # 3. Select the project - dependencies are injected
    select_result = await paella_select(project_name=project_name)
    assert select_result["status"] == "ok"
    assert (
        f"Project '{project_name}' selected and activated" in select_result["message"]
    )

    # Verify it's active via the adapter
    active_project = await memory_adapter.get_active_project()
    assert active_project is not None
    assert active_project.name == project_name

    # List again to check active status
    list_result_after_select = await paella_list()
    found_project_after_select = next(
        (p for p in list_result_after_select["projects"] if p["name"] == project_name),
        None,
    )
    assert found_project_after_select is not None
    assert found_project_after_select["is_active"] is True  # Access key

    # Cleanup project directory
    abs_base_path = Path(base_path).resolve()
    if abs_base_path.exists():
        import shutil

        shutil.rmtree(abs_base_path)
        print(f"Cleaned up test project dir: {abs_base_path}")

================
File: tests/integration/adapters/plugins/core/test_project_crud.py
================
"""
Integration tests for project CRUD operations.
"""

import pytest
import sys
import os
from pathlib import Path
import uuid
from typing import Dict, Any
import shutil

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.parent.parent.parent.absolute()
sys.path.insert(0, str(project_root))

from paelladoc.domain.models.language import SupportedLanguage
from paelladoc.adapters.plugins.core.paella import paella_init
from paelladoc.adapters.plugins.core.project_crud import (
    update_project,
    delete_project,
    get_project,
)
from paelladoc.adapters.output.sqlite.sqlite_memory_adapter import SQLiteMemoryAdapter
from paelladoc.adapters.plugins.core.project_utils import (
    validate_project_updates,
    format_project_info,
)
from paelladoc.ports.output.user_management_port import UserManagementPort
from paelladoc.ports.output.mcp_config_port import MCPConfigPort
from paelladoc.ports.output.memory_port import MemoryPort
from paelladoc.adapters.output.sqlite.sqlite_user_management_adapter import (
    SQLiteUserManagementAdapter,
)
from paelladoc.adapters.output.filesystem.mcp_config_repository import (
    FileSystemMCPConfigRepository,
)
from paelladoc.dependencies import dependencies

# --- Test Fixtures --- #

# Directory for temporary test databases
TEMP_DB_DIR = Path("src/tests/integration/adapters/plugins/core/temp_dbs")
TEMP_PROJECTS_DIR = Path("test_projects")  # Base for test project files


@pytest.fixture(scope="function", autouse=True)
def setup_test_dirs():
    """Create temporary directories for DBs and project files."""
    TEMP_DB_DIR.mkdir(parents=True, exist_ok=True)
    TEMP_PROJECTS_DIR.mkdir(parents=True, exist_ok=True)
    print(f"\nCreated directories: {TEMP_DB_DIR}, {TEMP_PROJECTS_DIR}")
    yield
    # Teardown: remove directories after tests in the module run
    # Scope is function, so this runs after each test
    try:
        if TEMP_DB_DIR.exists():
            shutil.rmtree(TEMP_DB_DIR)
            print(f"Removed directory: {TEMP_DB_DIR}")
        if TEMP_PROJECTS_DIR.exists():
            shutil.rmtree(TEMP_PROJECTS_DIR)
            print(f"Removed directory: {TEMP_PROJECTS_DIR}")
    except Exception as e:
        print(f"Error during teardown: {e}")


@pytest.fixture(scope="function")
async def crud_test_env(monkeypatch, setup_test_dirs):
    """Provides an isolated environment (DB, User Manager, Dependencies) for CRUD tests."""
    db_name = f"test_project_crud_{uuid.uuid4()}.db"
    db_path = TEMP_DB_DIR / db_name
    print(f"\nSetting up crud_test_env with DB: {db_path}")

    # Use the global session factory from the container - NOT USED, REMOVED
    # global_session_factory = container._async_session_factory

    # Create isolated adapter instances
    # Pass the db_path to SQLiteMemoryAdapter, it will create its own engine/session factory for that path
    memory_adapter = SQLiteMemoryAdapter(db_path=str(db_path))
    # Pass the *global* session factory to the user manager - THIS IS LIKELY WRONG if DBs are separate
    # The user manager needs to operate on the *same DB* as the memory adapter for the test.
    # Let's pass the session factory created by the specific memory_adapter instance for THIS test DB.
    user_manager = SQLiteUserManagementAdapter(
        async_session_factory=memory_adapter.async_session
    )
    mcp_config_adapter = FileSystemMCPConfigRepository()

    # Inject into dependencies using monkeypatch FIRST
    monkeypatch.setitem(dependencies, MemoryPort, memory_adapter)
    monkeypatch.setitem(dependencies, UserManagementPort, user_manager)
    monkeypatch.setitem(dependencies, MCPConfigPort, mcp_config_adapter)

    # NOW initialize DB tables using the correct, injected dependencies
    # _ensure_initial_admin_user will now get the patched user_manager
    await memory_adapter._create_db_and_tables()

    # NO need to setitem again here

    yield {"memory_adapter": memory_adapter, "user_manager": user_manager}

    # Teardown: Monkeypatch reverts. Close engine and remove DB file.
    print(f"Tearing down crud_test_env, closing engine and removing DB: {db_path}")
    # Explicitly dispose the engine associated with this test's adapter
    if memory_adapter.async_engine:
        await memory_adapter.async_engine.dispose()
        print(f"Disposed engine for DB: {db_path}")

    if db_path.exists():
        try:
            os.remove(db_path)
            print(f"Removed DB: {db_path}")
        except Exception as e:
            print(f"Error removing DB {db_path}: {e}")


@pytest.fixture
async def test_project(
    crud_test_env: Dict[str, Any],
) -> Dict[str, Any]:  # Use the env fixture
    """Creates a test project within the isolated environment."""
    # Dependencies are already injected by crud_test_env

    project_name = f"test-project-{uuid.uuid4()}"
    # Place project files inside the managed TEMP_PROJECTS_DIR
    base_path = TEMP_PROJECTS_DIR / project_name
    base_path_str = str(base_path.resolve())  # Use absolute path for consistency

    # Create project using the dependency-injected adapter
    init_result = await paella_init(
        base_path=base_path_str,
        documentation_language=SupportedLanguage.EN_US.value,
        interaction_language=SupportedLanguage.EN_US.value,
        new_project_name=project_name,
        platform_taxonomy="test_platform",
        domain_taxonomy="test_domain",
        size_taxonomy="test_size",
        compliance_taxonomy="test_compliance",
        lifecycle_taxonomy="test_lifecycle",
    )

    assert init_result["status"] == "ok", (
        f"Failed to initialize test project: {init_result.get('message', 'Unknown error')}"
    )
    # Verify base path creation directly from init_result
    returned_base_path = Path(init_result["base_path"])
    assert returned_base_path.exists(), (
        "Project base path directory was not created by paella_init"
    )
    assert returned_base_path == base_path.resolve(), (
        "Returned base path does not match expected resolved path"
    )

    # Fetch the project details AFTER creation using get_project
    # This ensures we test the full flow and get the complete ProjectInfo
    get_result = await get_project(project_name=project_name)
    assert get_result["status"] == "ok", (
        f"Failed to retrieve project '{project_name}' after creation."
    )
    initial_data = get_result["project"]  # Get the full project data as stored

    # Return info needed by tests
    return {
        "project_name": project_name,
        "base_path": base_path,  # Return Path object for easier manipulation
        "initial_data": initial_data,  # Return the fetched full project data
    }


# --- Test Cases (Injecting Dependencies) --- #


@pytest.mark.asyncio
async def test_get_project_returns_correct_info(crud_test_env, test_project):
    """Test retrieving a project returns all expected fields."""
    project_name = test_project["project_name"]
    initial_data = test_project["initial_data"]  # Use the fetched initial data

    # Call get_project - dependencies resolved internally
    result = await get_project(project_name=project_name)

    assert result["status"] == "ok"
    assert (
        result["project"] == initial_data
    )  # Compare fetched data with originally fetched data
    assert result["project"]["name"] == project_name
    assert Path(result["project"]["base_path"]) == test_project["base_path"].resolve()
    assert result["project"]["platform_taxonomy"] == "test_platform"


@pytest.mark.asyncio
async def test_get_project_returns_error_for_nonexistent_project(crud_test_env):
    """Test that get_project handles nonexistent projects correctly."""
    # Call get_project - dependencies resolved internally
    result = await get_project(
        project_name="nonexistent-project",
        # REMOVE: memory_adapter=crud_test_env["memory_adapter"] # No longer injected
    )
    assert result["status"] == "error"
    assert "not found" in result["message"]


@pytest.mark.asyncio
async def test_update_project_modifies_specific_fields(crud_test_env, test_project):
    """Test updating specific fields like purpose and language."""
    project_name = test_project["project_name"]
    updates = {
        "purpose": "Updated test purpose",
        "documentation_language": SupportedLanguage.ES_ES.value,
        "platform_taxonomy": "updated_platform",  # Example of updating taxonomy
    }

    # Call update_project - dependencies resolved internally
    update_result = await update_project(
        project_name=project_name,
        updates=updates,
        create_backup=False,  # Disable backup for simplicity
    )

    assert update_result["status"] == "ok"
    updated_project_info = update_result["project"]
    assert updated_project_info["purpose"] == "Updated test purpose"
    assert (
        updated_project_info["documentation_language"] == SupportedLanguage.ES_ES.value
    )
    assert updated_project_info["platform_taxonomy"] == "updated_platform"

    # Verify persistence by reading again
    get_result = await get_project(project_name=project_name)
    assert get_result["status"] == "ok"
    assert get_result["project"]["purpose"] == "Updated test purpose"
    assert get_result["project"]["platform_taxonomy"] == "updated_platform"


@pytest.mark.asyncio
async def test_update_project_validates_fields(crud_test_env, test_project):
    """Test that updates fail with invalid language codes."""
    project_name = test_project["project_name"]
    updates = {"documentation_language": "invalid-lang-code"}

    update_result = await update_project(
        project_name=project_name,
        updates=updates,
    )

    assert update_result["status"] == "error"
    assert "Validation failed" in update_result["message"]
    assert "documentation_language" in update_result["message"]


@pytest.mark.asyncio
async def test_update_project_validates_multiple_fields(crud_test_env, test_project):
    """Test that multiple invalid fields are reported."""
    project_name = test_project["project_name"]
    updates = {
        "documentation_language": "invalid-lang",
        "interaction_language": "also-invalid",
    }

    update_result = await update_project(
        project_name=project_name,
        updates=updates,
    )

    assert update_result["status"] == "error"
    assert "Validation failed" in update_result["message"]
    assert "documentation_language" in update_result["message"]
    assert "interaction_language" in update_result["message"]


@pytest.mark.asyncio
async def test_delete_project_removes_project_and_files(crud_test_env, test_project):
    """Test deleting a project removes DB entry and files."""
    project_name = test_project["project_name"]
    base_path = test_project["base_path"]

    # Create a dummy file in the project directory
    dummy_file = base_path / "dummy.txt"
    dummy_file.touch()
    assert dummy_file.exists()
    assert base_path.exists()

    # Call delete_project - dependencies resolved internally
    delete_result = await delete_project(
        project_name=project_name, confirm=True, create_backup=False
    )

    assert delete_result["status"] == "ok"

    # Verify deletion from DB
    get_result = await get_project(project_name=project_name)
    assert get_result["status"] == "error"
    assert "not found" in get_result["message"]

    # Verify directory deletion
    assert not base_path.exists(), f"Project directory {base_path} was not deleted."


@pytest.mark.asyncio
async def test_delete_project_requires_confirmation(crud_test_env, test_project):
    """Test delete fails without explicit confirmation."""
    project_name = test_project["project_name"]

    # Call delete_project without confirm=True
    delete_result = await delete_project(
        project_name=project_name,
        confirm=False,  # Default is False, but explicit here
        create_backup=False,
    )

    assert delete_result["status"] == "error"
    assert (
        "Deletion requires explicit confirmation. Set 'confirm=True'."
        in delete_result["message"]
    )

    # Verify project still exists
    get_result = await get_project(project_name=project_name)
    assert get_result["status"] == "ok"


@pytest.mark.asyncio
async def test_delete_project_creates_backup(crud_test_env, test_project):
    """Test that delete creates a backup when requested."""
    project_name = test_project["project_name"]
    base_path = test_project["base_path"]

    # Call delete_project with create_backup=True
    delete_result = await delete_project(
        project_name=project_name, confirm=True, create_backup=True
    )

    assert delete_result["status"] == "ok"
    assert "backup_path" in delete_result
    backup_path = Path(delete_result["backup_path"])
    assert backup_path.exists()
    assert backup_path.is_file()
    assert backup_path.name.endswith(".zip")

    # Clean up backup file
    if backup_path.exists():
        backup_path.unlink()

    # Verify project itself is deleted
    get_result = await get_project(project_name=project_name)
    assert get_result["status"] == "error"
    assert not base_path.exists()


# --- Utility Function Tests --- #


@pytest.mark.asyncio
async def test_validate_project_updates_checks_languages():
    """Test validation of language fields."""
    valid_updates = {"documentation_language": "en-US", "interaction_language": "es-ES"}
    invalid_updates = {"documentation_language": "xx-XX"}
    multiple_invalid = {
        "documentation_language": "yy-YY",
        "interaction_language": "zz-ZZ",
    }

    assert not validate_project_updates(valid_updates)
    errors = validate_project_updates(invalid_updates)
    assert len(errors) == 1
    assert "documentation_language" in errors[0]

    errors_multi = validate_project_updates(multiple_invalid)
    assert len(errors_multi) == 2
    assert any("documentation_language" in e for e in errors_multi)
    assert any("interaction_language" in e for e in errors_multi)


@pytest.mark.asyncio
async def test_validate_project_updates_checks_base_path():
    """Test validation that base_path cannot be updated directly."""
    updates = {"base_path": "/new/path"}
    errors = validate_project_updates(updates)
    assert len(errors) == 1
    assert "base_path cannot be updated" in errors[0]


@pytest.mark.asyncio
async def test_format_project_info_converts_paths():
    """Test that format_project_info converts Path objects to strings."""
    project_info_dict = {
        "name": "test",
        "base_path": Path("/absolute/path/to/project"),
        "other_field": 123,
    }
    formatted = format_project_info(project_info_dict)
    assert isinstance(formatted["base_path"], str)
    assert formatted["base_path"] == "/absolute/path/to/project"
    assert formatted["other_field"] == 123

================
File: tests/integration/adapters/plugins/core/test_list_projects.py
================
"""
Integration tests for the project listing functionality.
"""

import pytest
import sys
import os
from pathlib import Path
import uuid

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.parent.parent.parent.absolute()
sys.path.insert(0, str(project_root))

# Adapter is needed to pre-populate the DB for the test
from paelladoc.adapters.output.sqlite.sqlite_memory_adapter import SQLiteMemoryAdapter

# Import the SQLite adapter instead of the dummy one
from paelladoc.adapters.output.sqlite.sqlite_user_management_adapter import (
    SQLiteUserManagementAdapter,
)
from paelladoc.ports.output.user_management_port import UserManagementPort
from paelladoc.ports.output.memory_port import MemoryPort

# Import domain models to create test data
from paelladoc.domain.models.project import (
    ProjectMemory,
    ProjectInfo,
    Bucket,
    ArtifactMeta,
)
from paelladoc.domain.models.language import SupportedLanguage

# Import paella_list instead of the deleted module
from paelladoc.adapters.plugins.core.paella import paella_list

# Directory for temporary test databases
TEMP_DB_DIR = Path("src/tests/integration/adapters/plugins/core/temp_dbs_list")


@pytest.fixture(scope="function", autouse=True)
def setup_test_db_dir():
    """Ensure the temporary DB directory exists for each test function."""
    TEMP_DB_DIR.mkdir(parents=True, exist_ok=True)
    yield
    # Cleanup directory if empty
    try:
        TEMP_DB_DIR.rmdir()
    except OSError:
        pass  # Not empty, other tests might still be using it


@pytest.fixture(scope="function")
async def list_test_env(monkeypatch, setup_test_db_dir):
    """Provides an isolated environment (DB, User Manager, Dependencies) for list tests."""
    db_name = f"test_list_projects_{uuid.uuid4()}.db"
    db_path = TEMP_DB_DIR / db_name
    print(f"\nSetting up list_test_env with DB: {db_path}")

    # Create isolated adapter instances
    memory_adapter = SQLiteMemoryAdapter(db_path=str(db_path))
    # Instantiate SQLiteUserManagementAdapter using the same session factory as memory_adapter
    user_manager = SQLiteUserManagementAdapter(
        async_session_factory=memory_adapter.async_session
    )

    # Inject into dependencies using monkeypatch FIRST
    from paelladoc.dependencies import dependencies # Import here if not already at top
    monkeypatch.setitem(dependencies, MemoryPort, memory_adapter)
    monkeypatch.setitem(dependencies, UserManagementPort, user_manager)

    # NOW Initialize DB tables (important for adapter to work)
    # This will also ensure the default admin user is created if none exist
    await memory_adapter._create_db_and_tables()

    # Yield adapters for potential direct use in tests, although not strictly needed
    # if tests only call the plugin functions which use injected dependencies.
    yield {"memory_adapter": memory_adapter, "user_manager": user_manager}

    # Teardown: Monkeypatch handles reverting dependencies.
    # Close engine and remove DB file.
    print(f"Tearing down list_test_env, closing engine and removing DB: {db_path}")
    # Explicitly dispose the engine associated with this test's adapter
    if memory_adapter.async_engine:
        await memory_adapter.async_engine.dispose()
        print(f"Disposed engine for DB: {db_path}")

    if db_path.exists():
        try:
            os.remove(db_path)
            print(f"Removed DB: {db_path}")
        except Exception as e:
            print(f"Error removing DB {db_path}: {e}")


# --- Helper Function to create test data --- #


def _create_sample_memory(name_suffix: str) -> ProjectMemory:
    """Helper to create a sample ProjectMemory object."""
    project_name = f"test-project-{name_suffix}-{uuid.uuid4()}"
    # Add a dummy artifact to make it valid
    artifact = ArtifactMeta(
        name="dummy.md", bucket=Bucket.UNKNOWN, path=Path("dummy.md")
    )
    memory = ProjectMemory(
        project_info=ProjectInfo(
            name=project_name,
            interaction_language=SupportedLanguage.EN_US,
            documentation_language=SupportedLanguage.EN_US,
            base_path=Path(f"./docs/{project_name}").resolve(),
            purpose="testing list projects",
            target_audience="devs",
            objectives=["test list"],
            platform_taxonomy="test_platform",
            domain_taxonomy="test_domain",
            size_taxonomy="test_size",
            compliance_taxonomy="test_compliance",
            lifecycle_taxonomy="test_lifecycle",
        ),
        artifacts={Bucket.UNKNOWN: [artifact]},
        taxonomy_version="0.5",
        platform_taxonomy="test_platform",
        domain_taxonomy="test_domain",
        size_taxonomy="test_size",
        compliance_taxonomy="test_compliance",
        lifecycle_taxonomy="test_lifecycle",
    )
    return memory


# --- Test Case --- #


@pytest.mark.asyncio
async def test_list_projects_returns_saved_projects(
    list_test_env: dict,  # Use the new environment fixture
):
    """
    Verify that listing projects correctly returns previously saved projects.
    """
    print("\nRunning: test_list_projects_returns_saved_projects")
    memory_adapter = list_test_env["memory_adapter"]

    # Arrange: Save some projects directly using the adapter from the fixture
    project1_memory = _create_sample_memory("list1")
    project2_memory = _create_sample_memory("list2")
    await memory_adapter.save_memory(project1_memory)
    await memory_adapter.save_memory(project2_memory)
    expected_project_names = sorted(
        [project1_memory.project_info.name, project2_memory.project_info.name]
    )
    print(f"Saved projects: {expected_project_names}")

    # Act: Call paella_list. It will use the dependencies injected by the fixture.
    result = await paella_list()  # Remove random_string argument

    # Assert: Check the response
    assert result["status"] == "ok", (
        f"Expected status ok, got {result.get('status')}: {result.get('message')}"
    )
    assert "projects" in result
    assert isinstance(result["projects"], list)

    # Verify project names
    retrieved_project_names = sorted(
        [p["name"] for p in result["projects"]]
    )  # Access name via key again
    print(f"Retrieved projects: {retrieved_project_names}")
    assert retrieved_project_names == expected_project_names

================
File: tests/integration/adapters/output/test_sqlite_memory_adapter_config.py
================
"""Integration tests for SQLite adapter configuration."""

import os
import pytest
import asyncio
from pathlib import Path
import uuid

from paelladoc.adapters.output.sqlite.sqlite_memory_adapter import SQLiteMemoryAdapter
from paelladoc.domain.models.project import (
    ProjectMemory,
    ProjectInfo,
)


@pytest.fixture
def clean_env():
    """Remove relevant environment variables before each test."""
    original_db_path = os.environ.get("PAELLADOC_DB_PATH")
    original_env = os.environ.get("PAELLADOC_ENV")

    if "PAELLADOC_DB_PATH" in os.environ:
        del os.environ["PAELLADOC_DB_PATH"]
    if "PAELLADOC_ENV" in os.environ:
        del os.environ["PAELLADOC_ENV"]

    yield

    if original_db_path is not None:
        os.environ["PAELLADOC_DB_PATH"] = original_db_path
    if original_env is not None:
        os.environ["PAELLADOC_ENV"] = original_env


@pytest.fixture
async def temp_adapter():
    """Create a temporary adapter with a unique database."""
    test_db_name = f"test_config_{uuid.uuid4()}.db"
    test_dir = Path(__file__).parent / "temp_dbs"
    test_db_path = test_dir / test_db_name
    test_db_path.parent.mkdir(parents=True, exist_ok=True)

    adapter = SQLiteMemoryAdapter(db_path=test_db_path)
    await adapter._create_db_and_tables()

    yield adapter

    # Cleanup
    await asyncio.sleep(0.01)  # Brief pause for file lock release
    try:
        if test_db_path.exists():
            os.remove(test_db_path)
        test_db_path.parent.rmdir()
    except Exception as e:
        print(f"Error during cleanup: {e}")


@pytest.mark.asyncio
async def test_adapter_uses_custom_path(clean_env):
    """Verify adapter uses the path provided in __init__."""
    custom_path = create_temp_db_path()
    adapter = SQLiteMemoryAdapter(db_path=custom_path)
    assert adapter.db_path == custom_path
    # Clean up the test file if it was created
    if custom_path.exists():
        os.remove(custom_path)


@pytest.mark.asyncio
async def test_adapter_uses_env_var_path(clean_env):
    """Verify adapter uses PAELLADOC_DB_PATH environment variable if set."""
    env_path = create_temp_db_path()
    os.environ["PAELLADOC_DB_PATH"] = str(env_path)
    adapter = SQLiteMemoryAdapter()  # No path given, should use env var
    assert adapter.db_path == env_path
    if env_path.exists():
        os.remove(env_path)


@pytest.mark.asyncio
async def test_adapter_uses_production_path(clean_env):
    """Verify adapter uses PRODUCTION_DB_PATH by default."""
    # Ensure no env vars are set that override the default
    os.environ.pop("PAELLADOC_DB_PATH", None)
    os.environ.pop("PAELLADOC_ENV", None)
    adapter = SQLiteMemoryAdapter()
    expected_path = Path.home() / ".paelladoc" / "memory.db"  # Get default directly
    assert adapter.db_path == expected_path


@pytest.mark.asyncio
async def test_adapter_creates_parent_directory(clean_env):
    """Verify the adapter ensures the parent directory for the DB exists."""
    test_subdir = Path.home() / ".paelladoc_test_dir" / str(uuid.uuid4())
    custom_path = test_subdir / "test_creation.db"
    # Ensure the directory does not exist initially
    if test_subdir.exists():
        for item in test_subdir.iterdir():  # Clear if exists
            os.remove(item)
        os.rmdir(test_subdir)

    assert not test_subdir.exists()

    # The adapter instantiation triggers the directory creation
    _ = SQLiteMemoryAdapter(db_path=custom_path)  # Assign to _ as intentionally unused
    # Initialization should create the parent directory
    assert test_subdir.exists()
    assert test_subdir.is_dir()

    # Clean up
    if custom_path.exists():
        os.remove(custom_path)
    if test_subdir.exists():
        os.rmdir(test_subdir)


@pytest.mark.asyncio
async def test_adapter_operations_with_custom_path(temp_adapter):
    """Test basic adapter operations with custom path."""
    # Create test project
    project = ProjectMemory(
        project_info=ProjectInfo(
            name=f"test-project-{uuid.uuid4()}",
            language="python",  # This might need updating if language model changed
            purpose="Test project",
            target_audience="Developers",
            objectives=["Test database configuration"],
            # Add required taxonomy fields
            platform_taxonomy="test_platform",
            domain_taxonomy="test_domain",
            size_taxonomy="test_size",
            compliance_taxonomy="test_compliance",
            lifecycle_taxonomy="test_lifecycle",
        ),
        # Add required taxonomy fields also directly to ProjectMemory
        platform_taxonomy="test_platform",
        domain_taxonomy="test_domain",
        size_taxonomy="test_size",
        compliance_taxonomy="test_compliance",
        lifecycle_taxonomy="test_lifecycle",
    )

    # Test operations
    await temp_adapter.save_memory(project)
    assert await temp_adapter.project_exists(project.project_info.name)

    loaded = await temp_adapter.load_memory(project.project_info.name)
    assert loaded is not None
    assert loaded.project_info.name == project.project_info.name

    projects_info = await temp_adapter.list_projects()
    # Extract names from the returned dictionaries
    project_names = [info["name"] for info in projects_info]
    assert project.project_info.name in project_names


# Helper function to create a unique temporary DB path
def create_temp_db_path(prefix="test_adapter_config") -> Path:
    test_db_name = f"{prefix}_{uuid.uuid4()}.db"
    # Use /tmp or a similar temporary directory standard across systems
    test_db_path = Path("/tmp") / test_db_name
    # test_db_path.parent.mkdir(parents=True, exist_ok=True) # /tmp should exist
    print(f"\nGenerated temporary DB path: {test_db_path}")
    return test_db_path

================
File: tests/integration/adapters/output/test_sqlite_memory_adapter.py
================
"""
Integration tests for the SQLiteMemoryAdapter.
"""

import pytest  # Use pytest
import asyncio
import sys
import os
from pathlib import Path
import uuid
import datetime
from typing import Dict, List

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.parent.parent.absolute()
sys.path.insert(0, str(project_root))

# Module to test
from paelladoc.adapters.output.sqlite.sqlite_memory_adapter import SQLiteMemoryAdapter
from paelladoc.adapters.output.sqlite.sqlite_user_management_adapter import SQLiteUserManagementAdapter
from paelladoc.ports.output.user_management_port import UserManagementPort
from paelladoc.dependencies import dependencies
from paelladoc.ports.output.memory_port import MemoryPort

# Import updated domain models
from paelladoc.domain.models.project import (
    ProjectMemory,
    ProjectInfo,
    ArtifactMeta,
    DocumentStatus,
    Bucket,
)

# --- Pytest Fixture for Temporary DB --- #


@pytest.fixture(scope="function")  # Recreate DB for each test function
async def memory_adapter():
    """Provides an initialized SQLiteMemoryAdapter with a temporary DB."""
    test_db_name = f"test_memory_{uuid.uuid4()}.db"
    test_db_path = Path("./temp_test_dbs") / test_db_name
    test_db_path.parent.mkdir(parents=True, exist_ok=True)
    print(f"\nSetting up test with DB: {test_db_path}")

    adapter = SQLiteMemoryAdapter(db_path=test_db_path)
    await adapter._create_db_and_tables()

    yield adapter  # Provide the adapter to the test function

    # Teardown: clean up the database
    print(f"Tearing down test, removing DB: {test_db_path}")
    # Dispose engine if needed
    # await adapter.async_engine.dispose()
    await asyncio.sleep(0.01)
    try:
        if test_db_path.exists():
            os.remove(test_db_path)
            print(f"Removed DB: {test_db_path}")
            try:
                test_db_path.parent.rmdir()
                print(f"Removed test directory: {test_db_path.parent}")
            except OSError:
                pass  # Directory not empty or other issue
    except Exception as e:
        print(f"Error during teardown removing {test_db_path}: {e}")


# --- Helper Function --- #


def _create_sample_memory(name_suffix: str) -> ProjectMemory:
    """Helper to create a sample ProjectMemory object with Artifacts."""
    project_name = f"test-project-{name_suffix}"

    # Create sample artifacts
    artifact1 = ArtifactMeta(
        name="README",
        bucket=Bucket.INITIATE_INITIAL_PRODUCT_DOCS,
        path=Path("README.md"),
        status=DocumentStatus.PENDING,
    )
    artifact2 = ArtifactMeta(
        name="main.py generation script",
        bucket=Bucket.GENERATE_SUPPORTING_ELEMENTS,
        path=Path("scripts/generate_main.py"),
        status=DocumentStatus.IN_PROGRESS,
    )

    artifacts_dict: Dict[Bucket, List[ArtifactMeta]] = {
        Bucket.INITIATE_INITIAL_PRODUCT_DOCS: [artifact1],
        Bucket.GENERATE_SUPPORTING_ELEMENTS: [artifact2],
    }

    # Set up TimeService first if needed by ArtifactMeta or ProjectMemory\/Info
    # from paelladoc.adapters.services.system_time_service import SystemTimeService
    # from paelladoc.domain.models.project import set_time_service, time_service
    # if time_service is None:
    #     set_time_service(SystemTimeService())

    memory = ProjectMemory(
        project_info=ProjectInfo(
            name=project_name,
            # language="python", # Removed
            # purpose="testing adapter v2", # Removed
            # target_audience="devs", # Removed
            # objectives=["test save artifacts", "test load artifacts"], # Removed
            # Add required taxonomy fields
            platform_taxonomy="test_platform_adapter",
            domain_taxonomy="test_domain_adapter",
            size_taxonomy="test_size_adapter",
            compliance_taxonomy="test_compliance_adapter",
            lifecycle_taxonomy="test_lifecycle_adapter",
            # Assuming base_path, langs are optional or set elsewhere
        ),
        artifacts=artifacts_dict,
        taxonomy_version="0.5",
        # Add required taxonomy fields also directly to ProjectMemory
        platform_taxonomy="test_platform_adapter",
        domain_taxonomy="test_domain_adapter",
        size_taxonomy="test_size_adapter",
        compliance_taxonomy="test_compliance_adapter",
        lifecycle_taxonomy="test_lifecycle_adapter",
    )

    return memory


# --- Test Cases (using pytest and pytest-asyncio) --- #


@pytest.mark.asyncio
async def test_project_exists_on_empty_db(memory_adapter: SQLiteMemoryAdapter):
    """Test project_exists returns False when the DB is empty/project not saved."""
    print("Running: test_project_exists_on_empty_db")
    exists = await memory_adapter.project_exists("nonexistent-project")
    assert not exists


@pytest.mark.asyncio
async def test_load_memory_on_empty_db(memory_adapter: SQLiteMemoryAdapter):
    """Test load_memory returns None when the DB is empty/project not saved."""
    print("Running: test_load_memory_on_empty_db")
    loaded_memory = await memory_adapter.load_memory("nonexistent-project")
    assert loaded_memory is None


@pytest.mark.asyncio
async def test_save_and_load_new_project(memory_adapter: SQLiteMemoryAdapter):
    """Test saving a new project with artifacts and loading it back."""
    print("Running: test_save_and_load_new_project")
    original_memory = _create_sample_memory("save-load-artifacts")
    project_name = original_memory.project_info.name
    original_artifacts = original_memory.artifacts
    artifact1_id = original_artifacts[Bucket.INITIATE_INITIAL_PRODUCT_DOCS][0].id
    artifact2_id = original_artifacts[Bucket.GENERATE_SUPPORTING_ELEMENTS][0].id

    # Save
    await memory_adapter.save_memory(original_memory)
    print(f"Saved project: {project_name}")

    # Load
    loaded_memory = await memory_adapter.load_memory(project_name)
    print(f"Loaded project: {project_name}")

    # Assertions
    assert loaded_memory is not None
    assert loaded_memory.project_info.name == original_memory.project_info.name
    assert loaded_memory.project_info.language == original_memory.project_info.language
    assert (
        loaded_memory.project_info.objectives == original_memory.project_info.objectives
    )
    assert loaded_memory.taxonomy_version == original_memory.taxonomy_version

    # Check artifacts dictionary structure
    # Note: If the adapter pads with empty buckets, adjust this check
    # For now, assume only buckets with artifacts are loaded
    assert Bucket.INITIATE_INITIAL_PRODUCT_DOCS in loaded_memory.artifacts
    assert Bucket.GENERATE_SUPPORTING_ELEMENTS in loaded_memory.artifacts
    assert len(loaded_memory.artifacts[Bucket.INITIATE_INITIAL_PRODUCT_DOCS]) == 1
    assert len(loaded_memory.artifacts[Bucket.GENERATE_SUPPORTING_ELEMENTS]) == 1
    # assert len(loaded_memory.artifacts[Bucket.DEPLOY_SECURITY]) == 0 # Check depends on adapter behavior

    # Check artifact details
    loaded_artifact1 = loaded_memory.get_artifact_by_path(Path("README.md"))
    assert loaded_artifact1 is not None
    assert loaded_artifact1.id == artifact1_id
    assert loaded_artifact1.name == "README"
    assert loaded_artifact1.bucket == Bucket.INITIATE_INITIAL_PRODUCT_DOCS
    assert loaded_artifact1.status == DocumentStatus.PENDING

    loaded_artifact2 = loaded_memory.get_artifact_by_path(
        Path("scripts/generate_main.py")
    )
    assert loaded_artifact2 is not None
    assert loaded_artifact2.id == artifact2_id
    assert loaded_artifact2.name == "main.py generation script"
    assert loaded_artifact2.bucket == Bucket.GENERATE_SUPPORTING_ELEMENTS
    assert loaded_artifact2.status == DocumentStatus.IN_PROGRESS

    # Check timestamps - don't compare exact values since they'll be different due to persistence/mocking
    # Just verify that created_at is a valid UTC timestamp
    assert loaded_memory.created_at.tzinfo == datetime.timezone.utc
    assert isinstance(loaded_memory.created_at, datetime.datetime)
    assert isinstance(loaded_memory.last_updated_at, datetime.datetime)

    # Verify the loaded timestamps are in a reasonable range
    # Current time should be >= last_updated_at
    assert datetime.datetime.now(datetime.timezone.utc) >= loaded_memory.last_updated_at


@pytest.mark.asyncio
async def test_project_exists_after_save(memory_adapter: SQLiteMemoryAdapter):
    """Test project_exists returns True after a project is saved."""
    print("Running: test_project_exists_after_save")
    memory_to_save = _create_sample_memory("exists-artifacts")
    project_name = memory_to_save.project_info.name

    await memory_adapter.save_memory(memory_to_save)
    print(f"Saved project: {project_name}")

    exists = await memory_adapter.project_exists(project_name)
    assert exists


@pytest.mark.asyncio
async def test_save_updates_project(memory_adapter: SQLiteMemoryAdapter):
    """Test saving updates: changing artifact status, adding, removing."""
    print("Running: test_save_updates_project")
    # 1. Create and save initial state
    memory = _create_sample_memory("update-artifacts")
    project_name = memory.project_info.name
    artifact1 = memory.artifacts[Bucket.INITIATE_INITIAL_PRODUCT_DOCS][0]
    # artifact2 = memory.artifacts[Bucket.GENERATE_SUPPORTING_ELEMENTS][0] # No need to store if removing
    await memory_adapter.save_memory(memory)
    print(f"Initial save for {project_name}")

    # 2. Modify the domain object
    artifact1.update_status(DocumentStatus.COMPLETED)
    artifact3 = ArtifactMeta(
        name="Deployment Script",
        bucket=Bucket.DEPLOY_PIPELINES_AND_AUTOMATION,
        path=Path("deploy.sh"),
    )
    # Add artifact3 - ensure bucket exists in dict first
    if artifact3.bucket not in memory.artifacts:
        memory.artifacts[artifact3.bucket] = []
    memory.artifacts[artifact3.bucket].append(artifact3)
    # Remove artifact2 - remove the list if it becomes empty
    del memory.artifacts[Bucket.GENERATE_SUPPORTING_ELEMENTS][0]
    if not memory.artifacts[Bucket.GENERATE_SUPPORTING_ELEMENTS]:
        del memory.artifacts[Bucket.GENERATE_SUPPORTING_ELEMENTS]

    # 3. Save the updated memory
    await memory_adapter.save_memory(memory)
    print(f"Saved updates for {project_name}")

    # 4. Load and verify
    loaded_memory = await memory_adapter.load_memory(project_name)
    assert loaded_memory is not None

    # Verify artifact1 status updated
    loaded_artifact1 = loaded_memory.get_artifact_by_path(Path("README.md"))
    assert loaded_artifact1 is not None
    assert loaded_artifact1.status == DocumentStatus.COMPLETED
    assert loaded_artifact1.id == artifact1.id

    # Verify artifact2 removed
    loaded_artifact2 = loaded_memory.get_artifact_by_path(
        Path("scripts/generate_main.py")
    )
    assert loaded_artifact2 is None
    assert not loaded_memory.artifacts.get(Bucket.GENERATE_SUPPORTING_ELEMENTS)

    # Verify artifact3 added
    loaded_artifact3 = loaded_memory.get_artifact_by_path(Path("deploy.sh"))
    assert loaded_artifact3 is not None
    assert loaded_artifact3.name == "Deployment Script"
    assert loaded_artifact3.bucket == Bucket.DEPLOY_PIPELINES_AND_AUTOMATION
    assert loaded_artifact3.status == DocumentStatus.PENDING
    assert loaded_artifact3.id == artifact3.id


# Run tests if executed directly (optional, better via test runner)
# if __name__ == "__main__":
#     # Consider using asyncio.run() if needed for top-level execution
#     unittest.main()

================
File: tests/integration/adapters/output/test_sqlite_memory_adapter_active.py
================
"""Integration tests for SQLiteMemoryAdapter focusing on active project logic."""

import pytest
from uuid import uuid4
from pathlib import Path
import time

from sqlmodel import select
from pytest_asyncio import fixture  # Import fixture decorator

from paelladoc.adapters.output.sqlite.sqlite_memory_adapter import SQLiteMemoryAdapter
from paelladoc.domain.models.project import ProjectMemory, ProjectInfo
from paelladoc.adapters.output.sqlite.db_models import ProjectMemoryDB


# Define fixtures directly in this file
@fixture(scope="function")
async def memory_adapter(tmp_path: Path) -> SQLiteMemoryAdapter:
    """Creates a SQLiteMemoryAdapter instance with a temporary database."""
    # Use a unique name for each test function run to avoid conflicts
    db_name = f"test_active_db_{time.time_ns()}.db"
    db_path = tmp_path / db_name
    adapter = SQLiteMemoryAdapter(db_path=db_path)
    # Ensure tables are created *before* the test runs
    await adapter._create_db_and_tables()
    return adapter


@fixture(scope="function")
def create_project_memory():
    """Factory fixture to create ProjectMemory instances."""

    def _create(name: str) -> ProjectMemory:
        return ProjectMemory(
            project_info=ProjectInfo(
                name=name,
                language="en",
                base_path=Path(f"/tmp/{name}"),
                platform_taxonomy="test-platform",
                domain_taxonomy="test-domain",
                size_taxonomy="test-size",
                compliance_taxonomy="test-compliance",
                lifecycle_taxonomy="test-lifecycle",
            ),
            artifacts={},
            platform_taxonomy="test-platform",
            domain_taxonomy="test-domain",
            size_taxonomy="test-size",
            compliance_taxonomy="test-compliance",
            lifecycle_taxonomy="test-lifecycle",
        )

    return _create


# No need to import fixtures directly, Pytest injects them by name
# from src.tests.integration.adapters.output.test_sqlite_memory_adapter import memory_adapter, create_project_memory


@pytest.mark.asyncio
async def test_set_active_project(
    memory_adapter: SQLiteMemoryAdapter, create_project_memory
):
    """Test setting a project as active."""
    project1_name = f"test-project-{uuid4()}"
    project2_name = f"test-project-{uuid4()}"

    memory1 = create_project_memory(project1_name)
    memory2 = create_project_memory(project2_name)

    await memory_adapter.save_memory(memory1)
    await memory_adapter.save_memory(memory2)

    # Activate project 1
    success1 = await memory_adapter.set_active_project(project1_name)
    assert success1 is True

    # Verify project 1 is active
    async with memory_adapter.async_session() as session:
        stmt1 = select(ProjectMemoryDB).where(ProjectMemoryDB.name == project1_name)
        res1 = await session.execute(stmt1)
        db_project1 = res1.scalar_one()
        assert db_project1.is_active is True

        stmt2 = select(ProjectMemoryDB).where(ProjectMemoryDB.name == project2_name)
        res2 = await session.execute(stmt2)
        db_project2 = res2.scalar_one()
        assert db_project2.is_active is False

    # Activate project 2
    success2 = await memory_adapter.set_active_project(project2_name)
    assert success2 is True

    # Verify project 2 is active and project 1 is inactive in a new session
    async with memory_adapter.async_session() as session:
        stmt1 = select(ProjectMemoryDB).where(ProjectMemoryDB.name == project1_name)
        res1 = await session.execute(stmt1)
        db_project1_new = res1.scalar_one()
        assert db_project1_new.is_active is False

        stmt2 = select(ProjectMemoryDB).where(ProjectMemoryDB.name == project2_name)
        res2 = await session.execute(stmt2)
        db_project2_new = res2.scalar_one()
        assert db_project2_new.is_active is True


@pytest.mark.asyncio
async def test_set_active_nonexistent_project(memory_adapter: SQLiteMemoryAdapter):
    """Test setting a non-existent project as active fails."""
    success = await memory_adapter.set_active_project("nonexistent-project")
    assert success is False


@pytest.mark.asyncio
async def test_get_active_project(
    memory_adapter: SQLiteMemoryAdapter, create_project_memory
):
    """Test getting the active project."""
    project1_name = f"test-project-{uuid4()}"
    project2_name = f"test-project-{uuid4()}"

    memory1 = create_project_memory(project1_name)
    memory2 = create_project_memory(project2_name)

    await memory_adapter.save_memory(memory1)
    await memory_adapter.save_memory(memory2)

    # No active project initially
    active_project = await memory_adapter.get_active_project()
    assert active_project is None

    # Activate project 1
    await memory_adapter.set_active_project(project1_name)
    active_project = await memory_adapter.get_active_project()
    assert active_project is not None
    assert isinstance(active_project, ProjectInfo)
    assert active_project.name == project1_name

    # Activate project 2
    await memory_adapter.set_active_project(project2_name)
    active_project = await memory_adapter.get_active_project()
    assert active_project is not None
    assert active_project.name == project2_name


# The following test is removed because the DB-level unique constraint
# using `unique_where` is not supported by SQLite and the core logic
# (ensuring only one project is active) is already tested by
# `test_set_active_project` which verifies the behavior of the
# `set_active_project` method in the adapter.
# @pytest.mark.asyncio
# async def test_unique_constraint_on_active(memory_adapter: SQLiteMemoryAdapter, create_project_memory):
#     """Test that the unique constraint prevents manually setting multiple active projects."""
#     project1_name = f"test-project-{uuid4()}"
#     project2_name = f"test-project-{uuid4()}"
#
#     memory1 = create_project_memory(project1_name)
#     memory2 = create_project_memory(project2_name)
#
#     await memory_adapter.save_memory(memory1)
#     await memory_adapter.save_memory(memory2)
#
#     # Try to set both projects as active in the same transaction
#     async with memory_adapter.async_session() as session:
#         async with session.begin():
#             # Set first project as active
#             stmt1 = select(ProjectMemoryDB).where(ProjectMemoryDB.name == project1_name)
#             res1 = await session.execute(stmt1)
#             db_project1 = res1.scalar_one()
#             db_project1.is_active = True
#
#             # Try to set second project as active - should fail
#             stmt2 = select(ProjectMemoryDB).where(ProjectMemoryDB.name == project2_name)
#             res2 = await session.execute(stmt2)
#             db_project2 = res2.scalar_one()
#             db_project2.is_active = True
#
#             # This should raise an IntegrityError due to the unique constraint
#             with pytest.raises((Exception, sqlalchemy.exc.IntegrityError)) as excinfo:
#                 await session.commit()
#
#             assert "UNIQUE constraint failed" in str(excinfo.value) or "unique_where" in str(excinfo.value)

================
File: tests/integration/adapters/output/test_chroma_vector_store_adapter.py
================
"""
Integration tests for the ChromaVectorStoreAdapter.
"""

import unittest
import asyncio
import sys
from pathlib import Path
import uuid

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.parent.parent.absolute()
sys.path.insert(0, str(project_root))

# Module to test
from paelladoc.adapters.output.chroma.chroma_vector_store_adapter import (
    ChromaVectorStoreAdapter,
    NotFoundError,
)
from paelladoc.ports.output.vector_store_port import SearchResult  # Import base class

# Import Chroma specific types for assertions if needed
from chromadb.api.models.Collection import Collection


class TestChromaVectorStoreAdapterIntegration(unittest.IsolatedAsyncioTestCase):
    """Integration tests using an in-memory ChromaDB client."""

    def setUp(self):
        """Set up an in-memory Chroma client and a unique collection name."""
        print("\nSetting up test...")
        self.adapter = ChromaVectorStoreAdapter(in_memory=True)
        # Generate a unique collection name for each test to ensure isolation
        self.collection_name = f"test_collection_{uuid.uuid4()}"
        print(f"Using collection name: {self.collection_name}")

    async def asyncTearDown(self):
        """Attempt to clean up the test collection."""
        print(
            f"Tearing down test, attempting to delete collection: {self.collection_name}"
        )
        try:
            # Use the adapter's method to delete
            await self.adapter.delete_collection(self.collection_name)
            print(f"Deleted collection: {self.collection_name}")
        except Exception as e:
            # Log error if deletion fails, but don't fail the test run
            print(
                f"Error during teardown deleting collection {self.collection_name}: {e}"
            )
            # We can also try listing collections to see if it exists
            try:
                collections = self.adapter.client.list_collections()
                collection_names = [col.name for col in collections]
                if self.collection_name in collection_names:
                    print(
                        f"Collection {self.collection_name} still exists after teardown attempt."
                    )
                else:
                    print(
                        f"Collection {self.collection_name} confirmed deleted or never existed."
                    )
            except Exception as list_e:
                print(f"Error listing collections during teardown check: {list_e}")

    # --- Test Cases --- #

    async def test_get_or_create_collection_creates_new(self):
        """Test that a new collection is created if it doesn't exist."""
        print(f"Running: {self._testMethodName}")
        collection = await self.adapter.get_or_create_collection(self.collection_name)
        self.assertIsInstance(collection, Collection)
        self.assertEqual(collection.name, self.collection_name)

        # Verify it exists in the client
        collections = self.adapter.client.list_collections()
        collection_names = [col.name for col in collections]
        self.assertIn(self.collection_name, collection_names)

    async def test_get_or_create_collection_retrieves_existing(self):
        """Test that an existing collection is retrieved."""
        print(f"Running: {self._testMethodName}")
        # Create it first
        collection1 = await self.adapter.get_or_create_collection(self.collection_name)
        self.assertIsNotNone(collection1)

        # Get it again
        collection2 = await self.adapter.get_or_create_collection(self.collection_name)
        self.assertIsInstance(collection2, Collection)
        self.assertEqual(collection2.name, self.collection_name)
        # Check they are likely the same underlying collection (same ID)
        self.assertEqual(collection1.id, collection2.id)

    async def test_add_documents(self):
        """Test adding documents to a collection."""
        print(f"Running: {self._testMethodName}")
        docs_to_add = ["doc one text", "doc two text"]
        metadatas = [{"source": "test1"}, {"source": "test2"}]
        ids = ["id1", "id2"]

        returned_ids = await self.adapter.add_documents(
            self.collection_name, docs_to_add, metadatas, ids
        )
        self.assertEqual(returned_ids, ids)

        # Verify documents were added using the underlying client API
        collection = await self.adapter.get_or_create_collection(self.collection_name)
        results = collection.get(ids=ids, include=["metadatas", "documents"])

        self.assertIsNotNone(results)
        self.assertListEqual(results["ids"], ids)
        self.assertListEqual(results["documents"], docs_to_add)
        self.assertListEqual(results["metadatas"], metadatas)
        self.assertEqual(collection.count(), 2)

    async def test_add_documents_without_ids(self):
        """Test adding documents letting Chroma generate IDs."""
        print(f"Running: {self._testMethodName}")
        docs_to_add = ["auto id doc 1", "auto id doc 2"]
        metadatas = [{"type": "auto"}, {"type": "auto"}]

        returned_ids = await self.adapter.add_documents(
            self.collection_name, docs_to_add, metadatas
        )

        self.assertEqual(len(returned_ids), 2)
        self.assertIsInstance(returned_ids[0], str)
        self.assertIsInstance(returned_ids[1], str)

        # Verify using the returned IDs
        collection = await self.adapter.get_or_create_collection(self.collection_name)
        results = collection.get(ids=returned_ids, include=["metadatas", "documents"])

        self.assertIsNotNone(results)
        self.assertCountEqual(
            results["ids"], returned_ids
        )  # Order might not be guaranteed?
        self.assertCountEqual(results["documents"], docs_to_add)
        self.assertCountEqual(results["metadatas"], metadatas)
        self.assertEqual(collection.count(), 2)

    async def test_delete_collection(self):
        """Test deleting a collection."""
        print(f"Running: {self._testMethodName}")
        # Create it first
        await self.adapter.get_or_create_collection(self.collection_name)
        # Verify it exists
        collections_before = self.adapter.client.list_collections()
        self.assertIn(self.collection_name, [c.name for c in collections_before])

        # Delete it using the adapter
        await self.adapter.delete_collection(self.collection_name)

        # Verify it's gone
        collections_after = self.adapter.client.list_collections()
        self.assertNotIn(self.collection_name, [c.name for c in collections_after])

        # Attempting to get it should now raise NotFoundError or ValueError (depending on Chroma version)
        with self.assertRaises((NotFoundError, ValueError)):
            self.adapter.client.get_collection(name=self.collection_name)

    async def _add_sample_search_data(self):
        """Helper to add some consistent data for search tests."""
        docs = [
            "This is the first document about apples.",
            "This document discusses oranges and citrus.",
            "A third document, focusing on bananas.",
            "Another apple document for testing similarity.",
        ]
        metadatas = [
            {"source": "doc1", "type": "fruit", "year": 2023},
            {"source": "doc2", "type": "fruit", "year": 2024},
            {"source": "doc3", "type": "fruit", "year": 2023},
            {"source": "doc4", "type": "fruit", "year": 2024},
        ]
        ids = ["s_id1", "s_id2", "s_id3", "s_id4"]
        await self.adapter.add_documents(self.collection_name, docs, metadatas, ids)
        print(f"Added sample search data to collection: {self.collection_name}")
        # Short delay to allow potential indexing if needed (though likely not for in-memory)
        await asyncio.sleep(0.1)

    async def test_search_simple(self):
        """Test basic similarity search."""
        print(f"Running: {self._testMethodName}")
        await self._add_sample_search_data()

        query = "Tell me about apples"
        results = await self.adapter.search_similar(
            self.collection_name, [query], n_results=2
        )

        self.assertEqual(len(results), 1)  # One list for the single query
        self.assertEqual(len(results[0]), 2)  # Two results requested

        # Check the content of the results (order might vary based on embedding similarity)
        result_docs = [r.document for r in results[0]]
        self.assertIn("This is the first document about apples.", result_docs)
        self.assertIn("Another apple document for testing similarity.", result_docs)

        # Check metadata and ID are included
        first_result = results[0][0]
        self.assertIsInstance(first_result, SearchResult)
        self.assertIsNotNone(first_result.id)
        self.assertIsNotNone(first_result.metadata)
        self.assertIsNotNone(first_result.distance)

    async def test_search_with_metadata_filter(self):
        """Test search with a 'where' clause for metadata filtering."""
        print(f"Running: {self._testMethodName}")
        await self._add_sample_search_data()

        query = "Tell me about fruit"
        # Filter for documents from year 2023
        where_filter = {"year": 2023}
        results = await self.adapter.search_similar(
            self.collection_name, [query], n_results=3, where=where_filter
        )

        self.assertEqual(len(results), 1)
        # Should only find doc1 and doc3 from year 2023
        self.assertLessEqual(
            len(results[0]), 2
        )  # Might return fewer than n_results if filter is strict

        # Corrected: Access metadata via r.metadata, not r.project_info
        returned_sources = [r.metadata.get("source") for r in results[0] if r.metadata]

        # We expect only doc1 and doc3 from year 2023
        expected_sources = ["doc1", "doc3"]

        self.assertCountEqual(returned_sources, expected_sources)

    async def test_search_no_results(self):
        """Test search for text unrelated to the documents."""
        print(f"Running: {self._testMethodName}")
        await self._add_sample_search_data()

        query = "Information about programming languages"
        results = await self.adapter.search_similar(
            self.collection_name, [query], n_results=1
        )

        self.assertEqual(len(results), 1)
        # Depending on the embedding model, might still return *something* even if very dissimilar.
        # A more robust test might check the distance if available.
        # For now, let's assume it might return the closest, even if irrelevant, or empty.
        # If it returns results, ensure they are SearchResult instances
        if results[0]:
            self.assertIsInstance(results[0][0], SearchResult)
        else:
            self.assertEqual(len(results[0]), 0)  # Or assert empty list

    async def test_search_in_nonexistent_collection(self):
        """Test search returns empty list if collection doesn't exist."""
        print(f"Running: {self._testMethodName}")
        query = "anything"
        results = await self.adapter.search_similar(
            "nonexistent_collection_for_search", [query], n_results=1
        )

        self.assertEqual(len(results), 1)  # Still returns a list for the query
        self.assertEqual(len(results[0]), 0)  # But the inner list is empty


# if __name__ == "__main__":
#     unittest.main()

================
File: tests/e2e/test_cursor_simulation.py
================
"""
End-to-End tests for Paelladoc MCP Server.

This simulates how Cursor would interact with the server.
"""

import unittest
import sys
from pathlib import Path

# Ensure we can import Paelladoc modules
project_root = Path(__file__).parent.parent.parent.absolute()
if str(project_root) not in sys.path:
    sys.path.insert(0, str(project_root))

# Import directly from the domain layer
from paelladoc.domain.core_logic import mcp, ping


class TestCursorE2E(unittest.TestCase):
    """End-to-End tests simulating Cursor interacting with Paelladoc."""

    def test_direct_ping_call(self):
        """Test direct call to the ping function."""
        # Call the ping function directly
        result = ping()

        # Verify the result
        self.assertIsInstance(result, dict, "Ping should return a dict")
        self.assertEqual(result["status"], "ok", "Status should be 'ok'")
        self.assertEqual(result["message"], "pong", "Message should be 'pong'")

    def test_ping_with_parameter(self):
        """Test ping function with a parameter."""
        # Call ping with a test parameter
        result = ping(random_string="test-parameter")

        # Verify the result
        self.assertIsInstance(result, dict, "Ping should return a dict")
        self.assertEqual(result["status"], "ok", "Status should be 'ok'")
        self.assertEqual(result["message"], "pong", "Message should be 'pong'")

    def test_mcp_tool_registration(self):
        """Verify that the ping tool is registered with MCP."""
        # Get tools registered with MCP
        tool_manager = getattr(mcp, "_tool_manager", None)
        self.assertIsNotNone(tool_manager, "MCP should have a tool manager")

        tools = tool_manager.list_tools()

        # Check if the ping tool is registered
        tool_names = [tool.name for tool in tools]
        self.assertIn("ping", tool_names, "Ping tool should be registered with MCP")


if __name__ == "__main__":
    unittest.main()
